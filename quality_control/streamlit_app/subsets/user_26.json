[{"passage_id": "68_406582_8", "passage": "70 In contrast, SNI potentiates the synaptic transmission between parabrachial nucleus-central nucleus of amygdala 71 and PFC, 69 leading to memory deficit and depressive behaviors. It has been proposed that the reduced excitatory synaptic transmission in both hippocampus-and PFC-NAcc pathways, leading to a dysfunction of corticomesolimbic reward circuitry that underlies many of the symptoms of depression. 72 Consistently, optogenetic activation of the PFC-NAcc pathway inhibits neuropathic pain and the affective symptoms produced by SNI. 73 Several lines of evidence show that proinflammatory cytokines, including IL-1b and TNFa, regulate synaptic strength also in a region-dependent manner. Both IL-1b 74 and TNF-a 75 are necessary for induction of LTP at C-fiber synapses in SDH. 76, 77 While the cytokines at pathological concentration inhibit LTP in hippocampus [78] [79] [80] and in frontal cortex. 70 Taken together, peripheral nerve injury and the resultant upregulation of IL-1b may lead to the neuropathic pain, memory deficit, and depression-like behavior via the region-dependent changes in synaptic strength. As neuropathic pain was dissociated with STMD and depression-like behavior in SNI and IL-1b injected rats, we proposed that the changes of synaptic connections in different regions may be variable in a given animals. Further studies are needed for elucidate the mechanisms underlying region-dependent regulation of synaptic strength induced by proinflammatory cytokines.\n\n The upregulation of IL-1b is a common cause for chronic pain, memory deficits, and depressive behavior in neuropathic conditions. Hence, IL-1b may be a target for prevention of neuropathic pain and the accompanied cognitive and emotional disorders.\n\n Authors' Contributions WSG, LJW, LJZ, and XGL conceived of the project, designed the experiments. WSG, XW, CLM, and LJZ carried out all experiments. WSG, XW, and MM analyzed the data and prepared the figures. LJZ and XGL supervised the overall experiment. MM, LJW, LJZ, and XGL revised the manuscript. All authors read and approved the final manuscript.\n\n The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.\n\n The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This work was supported by grants from the National Natural Science Foundation of China (U1201223 and 8137119), Guangdong Province University Outstanding Young Teachers' Training Program (S2013010011889), and from Natural Science Foundation of Guangdong Province, China (Yq2013008).", "qa": [["68_406582_8_1", "How does the upregulation of IL-1b contribute to chronic pain, memory deficits, and depressive behavior in neuropathic conditions?\n", "The upregulation of IL-1b, a proinflammatory cytokine, has been found to be a common cause for chronic pain, memory deficits, and depressive behavior in neuropathic conditions. It is believed that the increased levels of IL-1b lead to changes in synaptic strength in different regions of the brain, such as the hippocampus and frontal cortex, which are involved in the regulation of pain, memory, and mood. These changes in synaptic connections may contribute to the development of neuropathic pain and the accompanying cognitive and emotional disorders."], ["68_406582_8_2", "What is the role of the PFC-NAcc pathway in inhibiting neuropathic pain and affective symptoms?\n", "The PFC-NAcc pathway, which connects the prefrontal cortex (PFC) and the nucleus accumbens (NAcc), has been found to play a role in inhibiting neuropathic pain and affective symptoms. Optogenetic activation of this pathway has been shown to reduce neuropathic pain and the associated emotional symptoms in animal models. This suggests that the PFC-NAcc pathway may be a potential target for the treatment of neuropathic pain and the accompanying affective disorders."], ["68_406582_8_3", "How do proinflammatory cytokines, such as IL-1b and TNF-a, regulate synaptic strength in a region-dependent manner?\n", "Proinflammatory cytokines, including IL-1b and TNF-a, have been found to regulate synaptic strength in a region-dependent manner. For example, IL-1b and TNF-a have been shown to be necessary for the induction of long-term potentiation (LTP) at C-fiber synapses in the spinal dorsal horn (SDH). However, at pathological concentrations, these cytokines can inhibit LTP in the hippocampus and frontal cortex. This suggests that the effects of proinflammatory cytokines on synaptic strength can vary depending on the specific brain region. Further research is needed to fully understand the mechanisms underlying this region-dependent regulation of synaptic strength by proinflammatory cytokines."]]}, {"passage_id": "67_6965586_2", "passage": "1663.51 \u00b1326.83 pg/ml, respectively, P <0.001) and CD (2146.91 \u00b1470.39 pg/ml vs. 1674.55\n\n The Mann-Whitney U test was then used where applicable. Associations between the variables with normal distribution were assessed using the Pearson correlation coefficient, while those between the variables without normal distribution were assessed using the Spearman's rank correlation coefficient. All statistical analyses were conducted using the Statistica 8.0 software (StatSoft Inc., Tulsa, Oklahoma, United States). A P value less than 0.05 was considered statistically significant.\n\n results The study was conducted on 105 patients with IBDs: 50 subjects with CD and 55 with UC, and in controls. The characterisitics of the groups are presented in tAble 1.\n\n Patients with CD were characterized by a lower mean age compared with those with UC (P = 0.008) and controls (P = 0.03). No significant age difference was observed between patients with UC and controls.\n\n In the majority of patients with CD (66%), disease-associated lesions were located both in the small intestine and in the colon. Such complications as enterocutaneous and enteroenteric fistulas and abscesses were present in 62% of patients with exacerbated CD, while subjects in remission showed no active fistulas or abscesses. The majority of patients (60%) did not undergo any CD-associated surgical procedures. In 52% of patients with UC, disease-associated lesions extended to the splenic flexure (L1), while in 35% of the patients, the lesions involved the colon, Abbreviations: BMI -body mass index, CD -Crohn's disease, CRP -C-reactive protein, SD -standard deviation, sTNFR1 and sTNFR2 -soluble tumor necrosis factor membrane receptors 1 and 2, TNF-\u03b1 -tumor necrosis factor-\u03b1, UC -ulcerative colitis, WBC -white blood cells dIscussIon To our knowledge, there is a limited number of studies on the correlations of TNF-\u03b1 with sTNFR1 and sTNFR2. A positive correlation between serum concentrations of those markers was reported in patients with impaired glucose tolerance and diabetes, 25,26 but we have not found any data concerning correlations between those markers in IBDs.\n\n In the present study, sTNFR1 and sTNFR2 levels were higher in patients with CD and UC compared with controls. TNF-\u03b1 levels were also higher in patients with CD and UC, but a significant difference was observed only between patients with CD and controls.\n\n Other investigators also demonstrated the higher values of sTNFR1 and sTNFR2 in patients with CD and UC compared with controls.\n\n Hadziselimovic et al. 10 showed a correlation between urinary sTNFR1 and sTNFR2 concentrations and the activity of CD and UC as well as therapeutic effects in these diseases. 10 Higher urinary sTNFR1/2 levels were observed in patients with active CD and UC compared with subjects in remission, which was correlated with the CDAI and CAI. the levels of sTNFR1 and sTNFR2 were higher in patients with active CD compared with those with nonactive disease and controls. However, in patients in remission, sTNFR1 and sTNFR2 levels were comparable to those in controls. Similar results were reported by Spoettl et al. 9 and Hudson et al. 10 In contrast, Noguchi et al. 27 performed \u00b1319.35 pg/ml, respectively, P <0.001) compared with the subgroups in remission. There were no differences in TNF-\u03b1, sTNFR1, and sTNFR2 levels depending on disease location and duration\u00b8 smoking status, development of exacerbated or recurrent disease in the follow-up period, and the type of therapy either in CD or UC.\n\n Positive correlations were demonstrated between disease activity, expressed by the CDAI and CAI scores, and sTNFR1 and sTNFR2. The correlation coefficients for both receptors were higher in UC compared with CD. For TNF-\u03b1, a positive correlation with disease activity was noted only in CD (tAbles 3-5, FIGure) .\n\n We also assessed correlations between routine inflammatory markers and disease activity. In the CD group, we found statistically significant correlations with platelet count (r = 0.45), CRP (r = 0.69) and fibrinogen (r = 0.44).", "qa": [["67_6965586_2_1", "What are the potential complications associated with Crohn's disease?\n", "In the majority of patients with Crohn's disease (CD), complications such as enterocutaneous and enteroenteric fistulas and abscesses can occur. These complications were present in 62% of patients with exacerbated CD, while subjects in remission showed no active fistulas or abscesses."], ["67_6965586_2_2", "Are there any correlations between TNF-\u03b1 and soluble tumor necrosis factor membrane receptors 1 and 2 (sTNFR1 and sTNFR2) in inflammatory bowel diseases (IBDs)?\n", "There is limited data on the correlations between TNF-\u03b1 and sTNFR1 and sTNFR2 in IBDs. However, in the present study, sTNFR1 and sTNFR2 levels were found to be higher in patients with CD and ulcerative colitis (UC) compared to controls. TNF-\u03b1 levels were also higher in patients with CD and UC, but a significant difference was observed only between patients with CD and controls."], ["67_6965586_2_3", "What routine inflammatory markers are correlated with disease activity in patients with CD?\n", "In patients with CD, statistically significant correlations were found between disease activity and platelet count, C-reactive protein (CRP), and fibrinogen levels. These markers showed positive correlations with disease activity, indicating their potential as indicators of disease severity."]]}, {"passage_id": "74_5985777_1", "passage": "As an induction agent, it produces a profound depletion of lymphocytes and is associated with more frequent and severe adverse effects, such as neutropenia, thrombocytopenia, thyroid disease, autoimmune hemolytic anemia and other autoimmune diseases [16] [17] [18] . It is hoped that alemtuzumab induction could permit patients to be maintained on unconventional strategy with less intensive immunosuppression, such as tacrolimus monotherapy [19] , steroid-free [20] , steroid and calcineurin inhibitor (CNI) free regimen [21] .\n\n Rituximab is a chimeric monoclonal Ab against CD20, which is expressed on the majority of B cells. It was first approved in 1997 for refractory B cell lymphomas and it is increasingly applied for autoimmune diseases. In the realm of kidney transplant, rituximab has been used in combination with plasmapheresis and IVIG to treat antibody-mediated rejection (AMR), and to desensitize patients with preformed antibodies for ABO-and/or HLAincompatible kidney transplant [22, 23] .\n\n Induction Therapy\n\n Antibody selection should be guided by a comprehensive assessment of immunologic risk, patient comorbidities, financial burden, and the maintenance immunosuppressive regimen. Clinical trials comparing different antibody induction in various patient populations and with different maintenance immunosuppression are recently reviewed by the author [2] . The published data remain in line with the 2009 KDIGO guideline [24] . Lymphocytedepleting antibody is recommended for those with high immunologic risk as outlined in the 2009 KDIGO clinical practice guidelines (sensitized patient, presence of donor specific antibody, ABO incompatibility, high HLA mismatches, DGF, cold ischemia time >24 hours, African-American ethnicity, younger recipient age, older donor age), though it increases the risk of infection and malignancy [24] . For low or moderate risk patients, IL-2R Ab induction reduces the incidence of acute rejection and graft loss without much adverse effects, making its balance favorable in these patients [25] [26] [27] . IL-2R Ab induction should also be used in the high risk patients with other comorbidities (history of malignancy, viral infection with HIV, HBV or HCV, hematological disorder of leucopenia or thrombocytopenia and elderly) that may preclude usage of lymphocyte-depleting antibody safely [28] [29] [30] . Many patients with very low risk (nonsensitized, Caucasian, Asian, well HLA matched, living related donor transplant) may be induced with intrave-nous steroids without using any antibody, as long as combined potent immunosuppressives are kept as maintenance. In these patients, benefits with antibody induction may be too small to outweigh its adverse effects and the financial cost [2, 24, 31] . Clinical comparison trials have not demonstrated any graft or patient survival benefit of using T-cell depleting Ab induction in patients with low immunological risk [2, 24] . Rituximab induction is useful in desensitization protocols for ABO and/or HLA incompatible transplants. Alemtuzumab induction might be more successful for adopting less intensive maintenance protocols. However, the long-term safety and efficacy of unconventional strategy remain to be determined.\n\n \n\n Glucocorticoids have been used for preventing and treating graft rejection since the early 1960s. They have multiple actions. In addition to the nonspecific anti-inflamematory actions, glucocorticoids have critical immunosuppressive effect by blocking T-cell and antigen-presenting cell (APC) derived cytokine expression. Glucocorticoids bind to cytoplasmic receptor to form a complex, which translocates into the nucleus and binds to glucocorticoid response elements (GRE) in the promoter regions of cytokine genes. Glucocorticoids also inhibit the translocation of transcription factor AP-1 and NF-\u03baB into the nucleus. Therefore, production of several cytokines (IL-1, 2, 3, 6, TNF-\u03b1, gamma-interferon) are inhibited [32, 33] . Large dose of glucocorticoids can be given in the perioperative period as induction therapy (methylprednisolone 250 to 500 mg IV), which is usually followed by oral prednisone 30 to 60 mg/day. The dose is tapered over 1 to 3 months to a typical maintenance dose of 5 to 10 mg/day.", "qa": [["74_5985777_1_1", "How do different types of antibody induction therapies in kidney transplant patients vary based on immunologic risk factors and comorbidities?\n", "The selection of antibody induction therapies in kidney transplant patients is influenced by factors such as immunologic risk, patient comorbidities, financial considerations, and the planned maintenance immunosuppressive regimen. High-risk patients, as defined by criteria like sensitization, donor-specific antibodies, ABO incompatibility, and others, are recommended lymphocyte-depleting antibodies despite the increased risk of infections and malignancies. In contrast, low or moderate-risk patients may benefit from IL-2R antibody induction due to reduced rejection rates and graft loss without significant adverse effects. Patients with very low risk profiles, such as nonsensitized individuals with well-matched donors, may not require antibody induction and can be managed with intravenous steroids alongside potent maintenance immunosuppressives."], ["74_5985777_1_2", "What are the mechanisms of action of glucocorticoids in preventing graft rejection in kidney transplant patients?\n", "Glucocorticoids have been utilized for graft rejection prevention and treatment since the 1960s due to their diverse actions. Apart from their general anti-inflammatory properties, glucocorticoids exert critical immunosuppressive effects by inhibiting T-cell and antigen-presenting cell (APC) derived cytokine expression. Upon binding to cytoplasmic receptors, glucocorticoids form complexes that translocate into the nucleus and bind to glucocorticoid response elements (GRE) in cytokine gene promoters. Additionally, glucocorticoids impede the nuclear translocation of transcription factors like AP-1 and NF-\u03baB, leading to the inhibition of cytokine production, including IL-1, IL-2, IL-6, TNF-\u03b1, and gamma-interferon. In kidney transplant settings, glucocorticoids are typically administered in high doses perioperatively as induction therapy, followed by a gradual tapering to a maintenance dose over several months."], ["74_5985777_1_3", "How do rituximab and alemtuzumab differ in their mechanisms of action and clinical applications in kidney transplant patients?\n", "Rituximab is a chimeric monoclonal antibody targeting CD20 on B cells, initially approved for refractory B cell lymphomas and increasingly used in autoimmune diseases. In kidney transplant, rituximab is employed in combination with plasmapheresis and IVIG for treating antibody-mediated rejection and desensitizing patients with preformed antibodies for ABO and/or HLA incompatible transplants. On the other hand, alemtuzumab acts as a lymphocyte-depleting induction agent associated with severe adverse effects like neutropenia, thrombocytopenia, and autoimmune diseases. The use of alemtuzumab induction in kidney transplant aims to enable less intensive immunosuppression strategies, such as tacrolimus monotherapy or steroid-free regimens, though the long-term safety and efficacy of such approaches remain to be fully understood."]]}, {"passage_id": "2_46447229_3", "passage": "To this effect hepcidin has been seen to bind, internalize and inactivate ferroportin 1 at duodenal mature enterocytes (51) . Intestinal iron absorption is thus blocked (Fig. 3) . Whatever the mechanism of action of hepcidin, its absence favors intestinal iron absorption and the release of iron stored in the reticuloendothelial system (RES). This is seen in all situations where this hepatic hormone is low (iron-deficient diet, bleeding, hypoxia, types I, II, III hemochromatosis, etc). On the contrary, increased hepcidin (inflammation, infection, exogenic iron overload, liver adenomatosis, etc.) (52) results in decreased intestinal iron absorption and iron retention in RES cells. During inflammation and infection hepatic hepcidin synthesis increases (52) , which translates into decreased intestinal iron absorption (53, 54) , iron retention within macrophages (55) , and anemia (45, 53, 56) .\n\n A number of mutations in the HAMP gene have been found in some patients with JH (57, 58) . A change G\u2192A in the sequence +14 at the 5'-untranslated end (5'-UTR) has been reported in a Portuguese family, which creates a new AUG sequence that inhibits the translation of normal hepcidin mRNA, and probably results in the formation of a new, abnormal, unstable and degradable peptide (59) . Other mutations reported include R56X, which creates a \"stop codon\", the deletion of guanine 93, 175G\u2192C (R59G), which precludes prohepcidin activation into hepcidin by convertases (60) , particularly by furin, and 212G\u2192A (G71D), which alters this peptide's structure and function (61) .\n\n In most patients with JH the disorder is linked to chromosome 1q (57) , but the gene involved has remained unknown until very recently. In 2004, Papanikolaou et al. (62) published the results of a thorough study of chromosome 1q, where they unveiled a locus of previously un- Hepcidin is a peptide expressed by gene HAMP in liver cells in response to infection and iron overload. Hemojuvelin, protein HFE, and transferrin receptor 2 (TfR2) also contribute to increase hepcidin production. This slows the passage of iron through enterocytes (intestinal absorption) and the release of iron from macrophages. In these cells iron stems from the degradation of phagocyted old red blood cells. Hepcidin has been suggested to exert these effects by internalizing ferroportin 1 (FP-1) within cells.\n\n known function, LOC148738, which was associated with JH. The gene involved was initially designated HFE2, and more recently HJV. In this gene, which was made up of four exons separated by three introns, they found numerous mutations, and one of them, G320V, was present in all patients of Greek, Canadian, and French descent with JH (62) (62) . The mechanism of action of hemojuvelin is unknown, but seems to be closely linked to that of hepcidin. It is known not to be a hepcidin receptor (62) , as it is not expressed in organs where hepcidin acts (intestine, spleen) (62) . When mutations exist in the HJV gene, urine hepcidin decreases (62) . In JH urine hepcidin is deeply reduced despite the fact that body iron is strongly elevated. Hemojuvelin is therefore thought to be a hepcidin-modulating protein, so that the former's decreased levels or inactivity results in the latter's reduced presence. Such decreases would be responsible for the increased intestinal iron absorption and iron overload found in patients with JH (62).\n\n Since Most of them were located in exons 3 and 4, particularly within the molecular region corresponding to the von Willebrand-like domain (66) , and many were determinant of transcription termination. These mutations included a deletion of 13 base-pairs (CGGGGCCCCGCCC), which may be expected to result in a nil phenotype. They found two mutations in another patient -220delG, which creates a transcription end signal at 113, and 806-807insA, which leads to molecule truncation at position 331 and the formation of a 310-aminoacid molecule.", "qa": [["2_46447229_3_1", "What is the role of hepcidin in iron absorption and retention?\n", "Hepcidin is a peptide expressed by the HAMP gene in liver cells in response to infection and iron overload. It binds to and inactivates ferroportin 1, a protein involved in the transport of iron. This blocks intestinal iron absorption and promotes the retention of iron in macrophages. In situations where hepcidin levels are low, such as iron-deficient diet or certain types of hemochromatosis, intestinal iron absorption is increased. Conversely, increased hepcidin levels, such as during inflammation or exogenic iron overload, result in decreased intestinal iron absorption and iron retention in macrophages."], ["2_46447229_3_2", "What are some mutations in the HAMP gene associated with Juvenile Hemochromatosis (JH)?\n", "Several mutations in the HAMP gene have been found in patients with JH. These include a change in the sequence +14 at the 5'-untranslated end (5'-UTR), which creates a new AUG sequence that inhibits the translation of normal hepcidin mRNA. Other mutations reported include R56X, which creates a \"stop codon\", the deletion of guanine 93, 175G\u2192C (R59G), which precludes prohepcidin activation into hepcidin, and 212G\u2192A (G71D), which alters the structure and function of the peptide. These mutations can lead to decreased levels or inactivity of hepcidin, resulting in increased intestinal iron absorption and iron overload in patients with JH."], ["2_46447229_3_3", "What is the role of hemojuvelin in the regulation of hepcidin?\n", "Hemojuvelin is a protein that contributes to the increase in hepcidin production. It is closely linked to the mechanism of action of hepcidin but is not a hepcidin receptor. When mutations exist in the HJV gene, urine hepcidin decreases. Hemojuvelin is thought to be a hepcidin-modulating protein, and its decreased levels or inactivity result in reduced hepcidin presence. This decrease in hepcidin levels is responsible for the increased intestinal iron absorption and iron overload found in patients with Juvenile Hemochromatosis (JH)."]]}, {"passage_id": "0_1332430_2", "passage": "[3] [4] [5] Currently, substantiated indications for iNO include the treatment of hypoxic respiratory failure of the newborn (PPHN), 6 -9 and the assessment of pulmonary vascular reactivity in patients with pulmonary hypertension. 10 To date, the U.S. Food and Drug Administration has approved nitric oxide only for the treatment of term and near-term (more than 34 weeks of gestational age) neonates with hypoxic respiratory failure associated with pulmonary hypertension. Inhaled NO clearly is effective for this indication and reduces the severity of subsequent lung disease and the necessity for extracorporeal membrane oxygenation in these infants. Off-label clinical use is widespread, and includes using inhaled NO to treat acute respiratory distress syndrome (ARDS); complications of lung and cardiac transplantation; pulmonary hypertension associated with congenital and acquired heart disease, as well as chronic pulmonary diseases; and to produce desirable direct effects on blood elements, specifically during the treatment of acute chest syndrome in sickle cell disease. 11 Lowson describes several alternatives to inhaled NO, and focuses his review on inhaled prostacyclin (PGI 2 ). Why do we need additional drugs if we have nitric oxide? Expense is only one criterion for drug selection. Efficacy, safety, availability, and ease-of-use are other important considerations.\n\n Efficacy of inhaled NO for its off-label uses has been difficult to demonstrate. Placebo-controlled trials of iNO to treat ARDS have been disappointing, demonstrating only transient improvements in oxygenation and no effect on outcome. 12, 13 While in many patients inhaled NO provides selective pulmonary vasodilation, large multicenter trials examining the effect of inhaled NO therapy on clinical course and outcome of patients with diverse causes of pulmonary hypertension have not been performed.\n\n Physiologically, it seems reasonable that a selective pulmonary vasodilator might be effective in treating ARDS. Reduced pulmonary capillary pressure should decrease the extent of pulmonary edema; should improve lung compliance; and might speed resolution of lung injury. Improved oxygenation should permit a reduction of the inspired oxygen concentration and airway pressure. But these effects may be insufficient to alter outcome. Usually, pulmonary artery pressure is only modestly elevated in ARDS. Even in severe cases, the mean pulmonary artery pressure is usually about 30 mmHg. 14 This degree of pulmonary hypertension is well tolerated, and few patients with ARDS die of their pulmonary hypertension. Rather, the survival of patients with ARDS appears to depend more on the occurrence of sepsis and multiple organ failure than on blood gas tensions or pulmonary artery pressure. [15] [16] [17] This Editorial View accompanies the following article: Lowson SM: Inhaled alternatives to nitric oxide. ANESTHESIOLOGY 2002; 96:1504 -13.\n\n The effect of iNO varies among patients. Approximately one-third of patients fail to demonstrate improved oxygenation or decreased pulmonary artery pressure. 12, 18 The cause of hyporesponsiveness remains under investigation. We cannot predict which patients may benefit and why pulmonary vasodilation does not occur in others.\n\n Consequently, the search for ways of improving the efficacy of iNO and designing effective alternative therapies continues. Combinations of therapies have been developed that aim to improve the matching of ventilation-to-perfusion or increase the biologic activity of inhaled NO. Alternative therapies have been suggested that may provide equivalent pulmonary vasodilation. While such therapies are attractive, whether they will affect clinical outcome is unknown.\n\n Ventilatory techniques that increase alveolar recruitment, such as the use of high-frequency oscillation in neonates, 7 or prone positioning of ARDS patients, 19 may improve the response to inhaled NO. Recruiting lung volume, by adding PEEP 20 or by the use of partial liquid ventilation with perfluorocarbons, 21 has been used to augment the response to iNO. The coadministration of vasoconstrictors, such as almitrine and norepinephrine, may enhance pulmonary vasoconstriction and accentuate the improvement in PaO 2 observed during inhaled NO therapy, presumably by improving the matching of ventilation to perfusion. 22, 23 Inhibition of the phosphodiesterase (PDE) enzymes that hydrolyze cGMP can also increase the efficacy and duration of action of iNO. 24, 25 Even if efficacy were improved, however, iNO therapy still has several drawbacks. It is expensive, cumbersome devices are necessary to administer the drug safely, and continuous administration is required. Especially for chronic treatment of pulmonary hypertension, therapies that are inexpensive, available in convenient forms (such as a tablet or simple multidose inhaler), and allow for intermittent dosing would be advantageous.\n\n Does inhaled prostacyclin fulfill these goals?", "qa": [["0_1332430_2_1", "What are some off-label uses of inhaled nitric oxide (iNO) in clinical practice?\n", "Off-label uses of iNO include treating acute respiratory distress syndrome (ARDS), complications of lung and cardiac transplantation, pulmonary hypertension associated with congenital and acquired heart disease, chronic pulmonary diseases, and acute chest syndrome in sickle cell disease."], ["0_1332430_2_2", "What are some factors that contribute to the difficulty in demonstrating the efficacy of iNO for its off-label uses?\n", "Placebo-controlled trials of iNO for off-label uses have been disappointing, showing only transient improvements in oxygenation and no effect on outcome. Additionally, large multicenter trials examining the effect of iNO therapy on clinical course and outcome of patients with diverse causes of pulmonary hypertension have not been performed."], ["0_1332430_2_3", "What are some alternative therapies or techniques that have been suggested to improve the efficacy of iNO?\n", "Some alternative therapies or techniques that have been suggested to improve the efficacy of iNO include ventilatory techniques that increase alveolar recruitment, such as high-frequency oscillation in neonates or prone positioning of ARDS patients, recruiting lung volume by adding positive end-expiratory pressure (PEEP) or using partial liquid ventilation with perfluorocarbons, coadministration of vasoconstrictors to improve the matching of ventilation to perfusion, and inhibition of phosphodiesterase (PDE) enzymes to increase the efficacy and duration of action of iNO."]]}, {"passage_id": "71_40363584_0", "passage": "Intestinal malrotation is more commonly diagnosed in the neonatal period, while adult presentations are reported with an incidence of 0.2% [1] . Such patients pose a unique challenge for the surgeon acutely both from a diagnostic and treatment standpoint. Herein, we report and review a case of intestinal malrotation with midgut volvulus in an adult. This report was approved by our Institutional Review Board.\n\n A 53-year-old man presented to the hospital with a 4-5 hour history of sudden onset, sharp right upper quadrant (RUQ) pain that began postprandially. His pain was associated with nausea, vomiting and obstipation. He also reported significant abdominal distension. At the time of presentation, he endorsed generalized abdominal discomfort.\n\n He is otherwise healthy with a past medical history significant only for benign prostatic hypertrophy and no prior abdominal surgeries. He endorsed mild nonspecific intermittent abdominal pain in the past without clear cause. He denies any recent trauma, weight loss, travel history, changes in appetite or bowel habits. This is his first presentation for such severe abdominal pain.\n\n On physical exam, he was afebrile and hemodynamically stable. His abdominal exam revealed a distended abdomen with significant voluntary guarding. His abdomen was diffusely tender with the point of maximal tenderness in the right and left upper quadrants. His hematological investigations revealed an elevated white blood cell count of 11.6 \u00d7 10 9 /L with a normal lactate of 0.8 mmol/L. His liver enzymes were unremarkable. Computerized tomography scan of the abdomen demonstrated evidence of small bowel malrotation with volvulus causing closed-loop obstruction. The radiologist reported on mucosal hypo-enhacement concerning for small bowel ischemia (Fig. 1 ).\n\n There was no evidence of pneumatosis intestinalis, or free air in the abdomen. His initial management consisted of intravenous (IV) fluids, IV antibiotics and a nasogastric tube. He was consented for an emergency exploratory laparotomy.\n\n Upon entry into the abdomen, a 20 cm segment of small bowel appeared ischemic, but without obvious gangrene. There were numerous adhesions causing closed-loop internal hernias at multiple locations along the small bowel. A mass like object was seen in the RUQ, which initially appeared to be colon. We proceeded with lysis of adhesions and it became evident that this mass was small bowel. In addition, with further dissection, the small bowel was found to segregate in toto to the right abdomen, consistent with malrotation. The midportion of the small bowel had rotated along its mesentery demonstrating midgut volvulus. Subsequently, we performed a counter-clockwise detorsion of the small bowel, internal hernia reduction and take down of Ladd's band in the RUQ. Following adhesiolysis and detorsion, the cecum and appendix were seen in the left lower quadrant, and an appendectomy was also performed. The affected small bowel segment was re-examined and deemed viable, avoiding a resection. The patient's abdomen was closed primarily and he was transferred to the surgical floor.\n\n The patient's postoperative course was marked by prolonged ileus requiring 24 hours of total parental nutrition. The patient was discharged on the eighth postoperative day.\n\n Malrotation occurs when normal embryonic rotation and fixation of small bowel fail to occur during the 10-12th week of gestation [2] [3] [4] . Consequently, a narrow mesenteric base and Ladd's band (adhesion running from cecum to the right lateral abdominal wall) are formed predisposing patients to obstruction. Thus, volvulus most commonly presents as a surgical emergency during the neonatal period or first month of life, and less commonly in older children and adults [1, 2] .\n\n Intestinal malrotation in adults tend to have a variable clinical presentation ranging from acute bowel obstruction to insidious, nonspecific symptoms, which often delays diagnosis compared to pediatric patients [2, 5] .\n\n Most adults present with chronic symptoms which may be present for greater than 6 months [3] . Patients may complain of intermittent abdominal pain, bloating and vomiting, frequently in the postprandial period. Conversely,~10-15% of adults with malrotation present with acute volvulus complaining of severe abdominal pain, nausea, vomiting, hematemesis or hematochezia, with or without hemodynamic instability [5, 6] . In this current report, the patient reported mild abdominal pain and denied other chronic symptoms, and presented acutely with obstruction secondary to volvulus.\n\n The treatment for malrotation depends on the severity of the patient's presentation. Patients experiencing chronic symptoms without acute volvulus are treated with an elective Ladd's procedure. Acute volvulus requires emergency laparotomy after appropriate hemodynamic resuscitation. The Ladd's procedure, originally described by pediatric general surgeon William Ladd in 1936, remains the mainstay in surgical management. The procedure involves five steps and has been described both via an open or laparoscopic approach: assessment for volvulus with counter-clockwise detorsion if present, Ladd's band division, inter-mesenteric band division (fibrous bands between bowel loops other than cecum and duodenum), appendectomy due to its aberrant location (and prevention of future confusion) and finally placement of bowel in the corrected anatomic position. If frank gangrene is evident, the involved bowel is resected; if viability is equivocal, relook laparotomy is recommended within 24-48 hours [2] .\n\n In patients presenting with incidental malrotation, the decision to proceed with operative intervention versus conservative management is controversial [7] . Due to the rarity of the condition, only small case series are available and largely in the pediatric literature. However, most authors advocate for surgical intervention due to the lack of reliable predictors for the development of midgut volvulus.\n\n The outcome of adults undergoing operative intervention for malrotation depends on the severity of presentation [5, 7] . The mortality rate ranges from 0 to 25%, with acute volvulus having the highest mortality. Patients may also experience significant morbidity postoperatively (up to 60%) [1, 5] . Prolonged ileus is anticipated following Ladd's procedure, as was evident in our patient. Patients are also at a higher lifetime risk of developing small bowel obstruction [2] . Recurrent volvulus occurs in 1.8-8% of cases, and clinicians should be aware of its possibility [8] .\n\n In conclusion, we present a rare case of malrotation with midgut volvulus in an adult. Given his acute presentation, operative intervention was the natural trajectory in his management. However, this entity can be challenging to diagnose in the adult population and a high index of suspicion would facilitate a timely diagnosis.", "qa": [["71_40363584_0_1", "What are the common clinical presentations of intestinal malrotation in adults compared to pediatric patients?", "In adults, intestinal malrotation can have a variable clinical presentation, ranging from acute bowel obstruction to insidious, nonspecific symptoms. Chronic symptoms such as intermittent abdominal pain, bloating, and vomiting are more common in adults, often lasting for more than 6 months. However, approximately 10-15% of adults with malrotation present with acute volvulus, experiencing severe abdominal pain, nausea, vomiting, and sometimes hematemesis or hematochezia."], ["71_40363584_0_2", "What is the mainstay of surgical management for intestinal malrotation?", "The mainstay of surgical management for intestinal malrotation is the Ladd's procedure. This procedure, originally described by pediatric general surgeon William Ladd, involves five steps: assessment for volvulus with counter-clockwise detorsion if present, division of Ladd's band, division of inter-mesenteric bands, appendectomy, and placement of the bowel in the corrected anatomical position. If gangrene is present, the involved bowel is resected, and if viability is uncertain, relook laparotomy is recommended within 24-48 hours."], ["71_40363584_0_3", "What are the potential outcomes and complications associated with operative intervention for intestinal malrotation in adults?", "The outcomes of adults undergoing operative intervention for intestinal malrotation depend on the severity of presentation. The mortality rate ranges from 0 to 25%, with acute volvulus having the highest mortality. Significant morbidity, up to 60%, can occur postoperatively. Prolonged ileus is expected following the Ladd's procedure. Patients also have a higher lifetime risk of developing small bowel obstruction, and recurrent volvulus can occur in 1.8-8% of cases. Clinicians should be aware of these possibilities and closely monitor patients postoperatively."]]}, {"passage_id": "54_13022521_2", "passage": "20 Animals underwent the same liver ischemia-reperfusion protocol as described above in Materials and Methods (see \"Hepatic ischemia and reperfusion,\" second paragraph).\n\n \n\n A calculated number of experimental cases with the primary endpoint of glutamate-pyruvate transaminase changes were performed beforehand. Accordingly, sample size calculation was calculated expecting a change of difference to be detected (in means) of 1,000, the expected SD was 700. An estimated sample size of six mice was calculated to be sufficient for an alpha = 0.05 and a power of 0.8 (two-sided sample size estimations, SigmaPlot [San Jose, CA]). For the calculations of the overall mouse number to be applied for the ethical/animal review boards, the number per group was counted to be maximum eight per condition/experiment taking into account to compensate for potential animal loss as a consequence of possible complications during surgical procedures and/or during induction of anesthesia. The number of mice per experiment is described in the legends of the figures (according to different legends of the figures). The variations in the number of cases for the different analyses result from the fact that (1) not under all conditions an optimal match of age and weight of the animals was achievable for six or higher (per group, especially for the KO). Also, (2) complications occurred during surgery and the total number might have reduced, respectively, as it was impossible to replace the lost mice instantly by new mice on the same day due to animal facility regulation. Moreover, (3) we used many repeats but it was not possible to equally increase all n by sharing the blood and organs, as it needed to be selected for the different aspects (histology, liver weight, bleeding out for cell functional test, CT, etc.). Data were all normally distributed as assessed by the Kolmogorov-Smirnov test.\n\n Data were tested by paired t test for detection of inbetween differences and are presented as means and SD. P values less than 0.05 were considered to be statistically significant. All statistical analyses were performed by IBM SPSS 20 program (IBM, Armonk, NY).\n\n \n\n Hyperoxic Treatment and Liver Damage. After 45 min of ischemia, hyperoxic treatment (60 vs. 21% oxygen in inspired air) during a 24-h period of reperfusion resulted in a significant (P < 0.01) increase in liver enzyme activities of glutamate-pyruvate transaminase and glutamate-oxalacetate transaminase by 45 and 48%, respectively ( fig. 1 ). An increase was also observed in mice exposed to 30% oxygen though not reaching the level of two-sided significance ( P = 0.059; see table 1, Supplemental Digital Content, http:// links.lww.com/ALN/B93). Histologic Damage Score after IRI. The comparison of histologic damage score of mice livers following ischemia and reperfusion showed significantly more than 30% higher damage scores in the 60% oxygen group as compared with 21% of oxygen group ( fig. 2) . In vivo micro-CT indicated a significant increase in the volume of nonviable liver tissue by a third after exposition to 60% oxygen compared with exposition to 21% oxygen (n = 5 per group; see fig. 1 and table 2, Supplemental Digital Content 1, http://links.lww. com/ALN/B93). Weights of Liver Tissue after IRI. The weights of the left liver lobes were significantly higher in the 60% oxygen group following ischemia and reperfusion ( fig. 3) .\n\n Levels of ROS after IRI. The ROS (H 2 O 2 ) production by granulocytes was significantly higher by almost 50% in the hyperoxia group (60% oxygen) as compared with normoxia group (21% oxygen) after ischemia and reperfusion. The serum levels of ROS (in \u03bcmol/l) were significantly higher and doubled in the 60% oxygen-treated group (figs. 4 and 5). Sham-operated Animals. Sham-operated animals showed no significant differences between normoxic and hyperoxic treatment for the liver enzymes, histology, liver weights, and ROS, respectively (data not shown). (fig. 6 ). No significant differences between 21 and 60% oxygenation were seen also for the histological scores and left liver lobe weights in the granulocyte-depleted mice (figs. 2 and 3). Effects of Kupffer Cell Depletion. Pretreatment of mice with GdCl 3 resulted in depletion of Kupffer cells.", "qa": [["54_13022521_2_1", "How does hyperoxic treatment during reperfusion affect liver enzyme activities and histologic damage score?\n", "Hyperoxic treatment during reperfusion leads to a significant increase in liver enzyme activities of glutamate-pyruvate transaminase and glutamate-oxalacetate transaminase by 45% and 48% respectively. Additionally, there is more than a 30% increase in histologic damage score in the liver following ischemia and reperfusion in the hyperoxic treatment group compared to the normoxic group."], ["54_13022521_2_2", "What are the effects of hyperoxic treatment on the volume of nonviable liver tissue and left liver lobe weights after ischemia and reperfusion?\n", "Hyperoxic treatment results in a significant increase in the volume of nonviable liver tissue by a third compared to normoxic treatment. Additionally, the left liver lobe weights are significantly higher in the hyperoxic treatment group following ischemia and reperfusion."], ["54_13022521_2_3", "How does hyperoxic treatment during reperfusion impact the production of reactive oxygen species (ROS) after ischemia and reperfusion?\n", "Hyperoxic treatment leads to a significant increase in ROS production by granulocytes, specifically H2O2 production, which is nearly 50% higher in the hyperoxic treatment group compared to the normoxic treatment group. The serum levels of ROS are also significantly higher and doubled in the hyperoxic treatment group."]]}, {"passage_id": "87_80257459_0", "passage": "This study demonstrates that the strongest pre-test predictors of PE in our population are prior history of DVT/PE and elevated troponin. None of the parameters that often generate a request for CTPA, including vital signs or presence of chest pain, were associated with the presence of PE in our study population. In addition, this study confirms the previously known high sensitivity and negative predictive value of the D-dimer excluding PE. A hospital readmission may be due to a new condition, an exacerbation of an underlying chronic condition, a complication associated with previous healthcare received, or premature discharge to a setting where the patient's post-discharge care needs were unable to be met. 2 This study aims to evaluate the pattern of General Medical readmissions at a regional secondary hospital in Queensland and to identify risk factors for readmission.\n\n \n\n Patients readmitted under General Medicine at the Toowoomba Base Hospital between 1 January 2015 and 31 December 2015 were retrospectively identified from the hospital patient administration database. A 'readmission' was defined as a hospital admission within 30 days of a prior index hospital admission under the same medical team. Demographic, clinical, diagnostic and discharge information were obtained from medical records.\n\n Results: 191 patients were identified with 246 readmission episodes, representing 11% of all admissions under General Medicine. Mean age was 65.8 years (range 18-95 years). 52.6% were female. Aboriginal and Torres Strait Islanders accounted for 8.5% of readmitted patients. Majority of patients lived locally (80.2%) with family (53.8%) in a stand-alone house (69%). Only 9.7% of patients resided in supported living accommodation. 63.4% of patients have more than four co-morbidities. Polypharmacy was common with 56.9 and 26.0% of patients taking 4-9 and 10 or more medications respectively. Most patients were only readmitted once (84.2%) with 5.9% of patients readmitted more than twice. Readmission occurred within an average of 9.02 days from a previous hospital discharge. Once readmitted, the acute episode length of stay averaged 7.42 days, whilst patients remained in hospital for an average of 9.24 days in total. 50.2% of patients were readmitted with the same diagnosis. Majority of patients were readmitted with either one (49.7%) or two diagnoses (30.7%). The commonest readmission diagnosis was infection (29.0%), followed by exacerbation of an underlying chronic condition (15.1%) and acute kidney injury (9.7%). The most frequent type of infection and chronic disease exacerbation were pneumonia and chronic obstructive pulmonary disease (COPD) respectively. Majority of patients were discharged back to their original residence (72.8%), whilst 4.9% were discharged to a new residential aged care facility. 13 patients died during their hospital admission.\n\n The readmission rate experienced in this centre is lower compared to previous published standards, though it could be improved further. Common risk factors identified include multiple co-morbidities and polypharmacy. Respiratory conditions accounted for a significant proportion of readmissions, involving either pneumonia or exacerbation of underlying COPD. Future efforts should be directed towards increasing community support for patients and creating effective home transition programmes especially for patients with chronic diseases to reduce readmissions. Background: The identification and management of atrial fibrillation (AF) is of utmost importance in secondary prevention of stroke. At our institution, Coronary Care Unit monitored telemetry (CCU-T) has been used within the Stroke Unit. Given the limited availability of these units, since July 2016, Stroke Unit monitored telemetry (SSU-T) has been made available. SSU-T has replaced CCU-T for all acute stroke patients except in those at high risk of malignant arrhythmia. We sought to study the pattern of telemetry use and rates of AF detection in the 6 months pre-and post-implementation of SSU-T.\n\n Methods: All patients with the discharge diagnosis of stroke or transient ischaemic attack from January to December 2016 were identified from the stroke unit database. Patients transferred for mechanical thrombectomy, patients not receiving care in the stroke unit and patients who died during their admission were excluded from analysis.\n\n Individual medical charts were reviewed for whether telemetry was requested, the presence or absence of the telemetry record and the detection of AF.\n\n Results: 154 patients had CCU-T between January-June 2016 and 194 had SSU-T between July-December 2016. Both groups were comparable with regard to age, sex and cardiovascular risk factors. When it was clinically indicated 79% and 85% of patients had telemetry during the CCU-T and SSU-T periods respectively (P = 0.31). Mean duration of monitoring increased from 1.0 (95% CI 1.0-1.1) days to 1.6 (95% CI 1.5-1.7) days in the SSU-T period (P < 0.01). Period of incidence of AF detection between the CCU-T and SSU-T periods were comparable, 4.51% vs 5.17% (P = 0.83) respectively. On univariate analysis, tPA administration and length of stay <24 h were associated with telemetry completion. Only tPA remains significant (P = 0.015) on multivariate logistic regression analyses as an independent factor associated with completion of telemetry.\n\n Implementation of SSU-T increased the average duration of telemetry by 12 h. There was a small absolute increase in AF detection but this was not statistically significant. This may be due to the small sample size of the study but other technical factors relating to monitoring and interpretations of the alarms warrant further investigation. Factors associated with patients not receiving telemetry as stated in the treatment plan also require further study.", "qa": [["87_80257459_0_1", "What are the common risk factors for readmission in patients under General Medicine?\n", "The common risk factors for readmission in patients under General Medicine include multiple co-morbidities and polypharmacy. These factors increase the likelihood of complications or exacerbations of underlying chronic conditions, leading to the need for readmission."], ["87_80257459_0_2", "What were the most frequent diagnoses for readmission in patients under General Medicine?\n", "The most frequent diagnoses for readmission in patients under General Medicine were infection, exacerbation of an underlying chronic condition (such as chronic obstructive pulmonary disease), and acute kidney injury. Infections, particularly pneumonia, accounted for a significant proportion of readmissions."], ["87_80257459_0_3", "How did the implementation of Stroke Unit monitored telemetry (SSU-T) affect the rates of atrial fibrillation (AF) detection in stroke patients?\n", "The implementation of SSU-T increased the average duration of telemetry monitoring by 12 hours. Although there was a small absolute increase in AF detection, it was not statistically significant. Further investigation is needed to determine if other technical factors or interpretations of alarms played a role in the detection of AF."]]}, {"passage_id": "81_935659_0", "passage": "In addition, functional T cells are required for the development of arthritis in these models,\"8 and it is possible to show genetic restriction at the T cell level of antigen recognition, both in the intact animals and at a clonal level in vitro.- '4 Thus in rats there is a strain dependent correlation between Col 11 antibodies and development of arthritis,'~\" and the arthritis can be transferred by in jection of immunoglobulin concentrate or purified Col 11 antibodies from immunised arthritic rats to healthy recipients. 417 Collagen induced arthritis in rats and mice has been shown to be mediated by complement-fixing Col II antibodies. 4 '\" These findings suggest that such antibodies maiy play a primary part in the initiation and pathogenesis of some forms of arthritis in experimental animals.\n\n Antibodies as well as T cell reactivity to Col II have been described in patients with RA. ')2-6 These findings imply that autoimmunity to Col II may have a role in disease initiation and pathogenesis of RA, as can be shown in animal models.\n\n A number of groups have identified native or denatured, or both. Col II antibodies in patients with RA and have speculated on their potential pathogenetic role. The role of autoimmunity to Col II remains to be clarified in human disease, however, as several factors have emerged which cause its primary role in disease pathogenesis to be questioned. For example, it has become apparent that these antibodies are not confined to RA but can be detected in other rheumatic diseases,2224 and in the case of antibodies to denatured Col II, in a substantial proportion of normal individuals.-\"\n\n The present study was designed to assess the incidence of antibodies to both native and denatured human Col 11 in RA, other forms of arthritis, other autoimmune connective tissue diseases, and certain chronic inflammatory conditions. Such a study. including a wider range of diseases than those usually chosen, should help clarify any role of autoimmunity to Col II in the pathogenesis of RA.\n\n The patient group comprised 76 There was no correlation between antibody activity and the presence or titre of RF. Of the four patients who showed high antibody activity to native Col II, two were negative for RF and the other two had RF titres of 1/40 and 1/160. In addition, some sera with high RF titre did not have native Col II (A) antibody activity. Similar findings were noted in patients with denatured Col II antibody. There wias also no correlation between ESR or disease duration with antibody activity, and patients who were antibody positive did not differ from others with respect to drug treatment (data not showni). These findings are consistent with those reported by others. 2-except for the incidence of the atntibodies, which was slightly lower in the present study. 21 23 > This discrepancy can be expliained by the fact that the criterion used to determine al positive sample in the present study was more stringent (3SD) than that used by most others (2SD). Selection of the lower cut off point produced a higher incidence of positives, but this resulted in some positive sera in the normal control group. The upper limit was therefore retained at the 3SD level.\n\n Interestingly, low antibody activities to both native and denatured Col II were detected in patients with AS, and the mean antibody levels were significantly higher (p<0-01) in this group than in the normal control group. These values were only just above the cut off values, and the biological significance of this reactivity remains to be determined.\n\n Antibody activity was not detectable in patients with RS, PSA, and gout. Only a small number of serum samples were studied, however, and it may be premature to conclude that such antibodies aire always absent from this group of patients. No antibodies were found in any of the 28 patients with OA.\n\n (B)", "qa": [["81_935659_0_1", "What is the potential role of autoimmunity to Col II in the pathogenesis of rheumatoid arthritis (RA)?\n", "Autoimmunity to Col II may have a role in the initiation and pathogenesis of rheumatoid arthritis (RA). Antibodies as well as T cell reactivity to Col II have been found in patients with RA, suggesting that autoimmunity to Col II may be involved in disease initiation and pathogenesis. Animal models have shown that collagen-induced arthritis can be mediated by complement-fixing Col II antibodies. However, the primary role of autoimmunity to Col II in human disease remains to be clarified, as these antibodies have also been detected in other rheumatic diseases and even in a substantial proportion of normal individuals."], ["81_935659_0_2", "Are there any correlations between antibody activity to Col II and other factors such as rheumatoid factor (RF), erythrocyte sedimentation rate (ESR), disease duration, or drug treatment in patients with rheumatoid arthritis (RA)?\n", "There is no correlation between antibody activity to Col II and the presence or titre of RF in patients with RA. Similarly, there is no correlation between antibody activity and ESR or disease duration. Patients who are positive for Col II antibodies do not differ from others in terms of drug treatment. These findings are consistent with previous reports, except for the slightly lower incidence of the antibodies in the present study. The discrepancy in incidence may be due to the more stringent criterion used to determine a positive sample in the present study. It is worth noting that low antibody activities to Col II were also detected in patients with ankylosing spondylitis (AS), but the biological significance of this reactivity is yet to be determined."], ["81_935659_0_3", "Are antibodies to Col II detectable in patients with other forms of arthritis, such as reactive arthritis (RS), psoriatic arthritis (PSA), gout, or osteoarthritis (OA)?\n", "Antibody activity to Col II was not detectable in patients with reactive arthritis (RS), psoriatic arthritis (PSA), or gout. However, it is important to note that only a small number of serum samples were studied, so it may be premature to conclude that such antibodies are always absent from these groups of patients. Interestingly, no antibodies to Col II were found in any of the patients with osteoarthritis (OA)."]]}, {"passage_id": "60_8952825_6", "passage": "RhoA/ROCK signaling has also been implicated in the regulation of microtubule stability (Arimura and Kaibuchi 2007) . Future analysis of RhoA-knockout mice will reveal the physiological function of RhoA and its effectors during axon growth.\n\n Taken together, whereas many signaling pathways have been implicated in the breakage of neuronal polarity, in many instances, their physiological relevance has not been shown. Most of the in vitro studies used dominantnegative and constitutively active mutants, which can exhibit nonspecific or nonphysiological effects resulting from perturbation of multiple signaling pathways (Wang and Zheng 2007) . Therefore, the future challenge is to reveal which of the signaling pathways are regulating neuronal symmetry breakage in vivo.\n\n Although asymmetry in neurons arises through changes in the cytoskeleton, it is not clear how these changes are induced in the first place. What is the initial symmetry breaking event? How does a cell with undifferentiated neurites transform into a polarized cell bearing a single axon and several dendrites? It is assumed that positive and negative feedback loops regulate the breakage of neuronal symmetry in the absence of extrinsic cues (Goslin and Banker 1989; Andersen and Bi 2000) . Mathematical modeling showed that a positive feedback alone may be sufficient to induce symmetry breakage (Altschuler et al. 2008 ). An increased local concentration of signaling molecules, such as Cdc42, Rac, or Par proteins, in specific regions of the plasma membrane, may trigger the recruitment of other signaling molecules, which then together direct the symmetry breakage (Butty et al. 2002; Shi et al. 2003; Gassama-Diagne et al. 2006) .\n\n The positive feedback loop model has been experimentally tested in budding yeast (Wedlich-Soldner et al. 2003; Altschuler et al. 2008) . It has been shown that spontaneous polarization of Cdc42 at the cortical plasma membrane serves as an initial signal for symmetry breaking that may activate further downstream signals regulating the establishment and maintenance of cell polarity. Furthermore, in neurons, local Ras and PI3K positive feedback, localized activation of PI3K, or selective axonal transport of Par complex are some examples of positive feedback loops that may underlie symmetry breakage and axon formation (Nishimura et al. 2004; Shi et al. 2004; Toriyama et al. 2006; Fivaz et al. 2008) .\n\n It is likely that positive and negative regulatory circuits influence microtubule stability and actin dynamics in the future axon and therefore manifest polarity after an initial break of symmetry. Another possibility is that the position of the centrosome, as the major microtubule organizing center, dictates the site of axon formation (Higginbotham and Gleeson 2007; Bornens 2008) . Consistent with this idea, after the last round of cell division, the centrosome of hippocampal neurons moves to the opposite pole of the last cleavage furrow. Axon formation then takes place from this site at later stages (de Anda et al. 2005) . Moreover, experiments in cerebellar granule neurons showed that the centrosome localizes to the site where the initial axon is formed and that it later relocates to the place from where the second axon emerges (Zmuda and Rivas 1998) .\n\n However, other studies question the instructive role of the centrosome in axon formation. For example, Drosophila neurons devoid of functional centrosomes form and elongate their axons normally (Basto et al. 2006) . Moreover, zebrafish retinal ganglion cells do not localize their centrosome to the site of axon formation (Zolessi et al. 2006) . Future experiments will help to elucidate whether centrosome positioning gives the first spatial cue to break the symmetry, or simply is an epiphenomenon (Etienne-Manneville and Hall 2003; Siegrist and Doe 2007) .\n\n After discussing the intrinsic mechanisms regulating neuronal symmetry breaking, we now focus on extrinsic cues and their role in symmetry breaking, in particular, in vivo.\n\n Hippocampal neurons polarize in vitro when separated from extracellular cues (Craig and Banker 1994) , arguing that the initial signal for breaking the neuronal symmetry is an intrinsic property of these neurons. However, studies have shown that extracellular cues can modulate polarization.", "qa": [["60_8952825_6_1", "What are some examples of positive feedback loops that may underlie symmetry breakage and axon formation in neurons?", "Some examples of positive feedback loops that may underlie symmetry breakage and axon formation in neurons include localized activation of PI3K, selective axonal transport of Par complex, and local Ras and PI3K positive feedback."], ["60_8952825_6_2", "How does the position of the centrosome influence axon formation in neurons?", "The position of the centrosome, as the major microtubule organizing center, may dictate the site of axon formation in neurons. After the last round of cell division, the centrosome of hippocampal neurons moves to the opposite pole of the last cleavage furrow, and axon formation takes place from this site at later stages. However, there are studies that question the instructive role of the centrosome in axon formation, suggesting that it may simply be an epiphenomenon."], ["60_8952825_6_3", "Can extracellular cues modulate the polarization of hippocampal neurons?", "Yes, studies have shown that extracellular cues can modulate the polarization of hippocampal neurons. While these neurons can polarize in vitro when separated from extracellular cues, the presence of extracellular cues can influence the process of symmetry breaking in neurons."]]}, {"passage_id": "18_21957679_3", "passage": "This implies that although additional factors may contribute to the release of calcium after liver transplantation, PTH remains the principal homoeostatic mechanism for serum calcium. Multiple cytokines are released immediately after graft reperfusion, which could influence bone metabolism [17, 18] . Interleukins 1 and 6 (IL-1, IL-6) and tumour necrosis factor-\u03b1 (TNF-\u03b1) rapidly appear in the graft's venous effluent after engraftment ; TNF-\u03b1 may act as a graft survival factor [19] . IL-1 is the most potent known stimulus to bone resorption [20] , and may act both immediately and over the longer term through its effects on multinucleated giant cell formation [21] . IL-6 regulates osteoclast development, and stimulates bone resorption in co-operation with IL-1 in vivo [22] . TNF-\u03b1 can increase the number of osteoclasts and may also regulate the differentiation of osteoclasts [23] . These cytokines may therefore contribute to the release of calcium from bone after transplantation. In addition, cyclosporin, administered both intra-and postoperatively might be expected to increase bone resorption in this setting, as it does in others [24] . It is quite possible that these factors would be sufficient, in combination with relatively modest amounts of PTH, to maintain normocalcaemia in the face of increased demand post transplant. Immobilization is unlikely to contribute significantly to bone loss since patients are mobilized in the first 2 to 3 days after transplantation.\n\n The bone loss, high fracture incidence and histomorphometric evidence of increased bone turnover, which have all been reported during the first few months after liver transplantation, support the contention that early in the post-operative period there is a significant increase in bone resorption. Despite this, no significant changes in the serum BSALP and plasma TRAP, markers of bone formation and resorption respectively, were demonstrated. This may reflect their limitations as biochemical markers rather than the absence of an increase in bone turnover, since there is cross-reactivity between bone and liver isoenzymes of alkaline phosphatase in patients with liver disease and plasma TRAP concentrations may not be a sensitive index of bone resorption [25] . In addition, changes of up to 1.5 times the standard deviation of each measurement may have been missed due to the small sample size.\n\n There is evidence from histomorphometric studies that bone turnover is generally normal or reduced in patients with chronic liver disease before transplantation, and that bone loss early after transplantation is predominantly due to increased bone turnover [26] . The results of the present study suggest that bone resorption can be effectively prevented in this vulnerable group of patients by the administration of pamidronate as much as 30 days before transplantation. Further studies are required to establish definitively a protective effect of such treatment against fracture.", "qa": [["18_21957679_3_1", "How do cytokines such as interleukins 1 and 6 (IL-1, IL-6) and tumour necrosis factor-\u03b1 (TNF-\u03b1) influence bone metabolism after liver transplantation, and what potential impact do they have on calcium release from bone?", "Cytokines play a significant role in influencing bone metabolism after liver transplantation. Interleukins 1 and 6 (IL-1, IL-6), and tumour necrosis factor-\u03b1 (TNF-\u03b1) can stimulate bone resorption and regulate osteoclast development and activity. IL-1 is known to be a potent stimulus to bone resorption, while IL-6 regulates osteoclast development and stimulates bone resorption in cooperation with IL-1. TNF-\u03b1 can also increase the number of osteoclasts and regulate their differentiation, potentially contributing to the release of calcium from bone after transplantation. These cytokines, in addition to factors like cyclosporine administration, may combine to maintain normocalcaemia post-transplant."], ["18_21957679_3_2", "What is the impact of early bone loss and increased bone turnover after liver transplantation on bone metabolism, and what potential factors contribute to this phenomenon?", "Early after liver transplantation, there is a significant increase in bone resorption, leading to bone loss, a high incidence of fractures, and histomorphometric evidence of increased bone turnover. Despite this, the serum bone-specific alkaline phosphatase (BSALP) and plasma tartrate-resistant acid phosphatase (TRAP), markers of bone formation and resorption, did not show significant changes. This may reflect their limitations as biochemical markers rather than the absence of an increase in bone turnover. Additionally, factors such as cytokines released after graft reperfusion, cyclosporine administration, and potential limitations of biochemical markers contribute to this phenomenon."], ["18_21957679_3_3", "How might the administration of pamidronate before liver transplantation impact bone resorption, and what further research is necessary to fully understand its potential protective effect?", "The administration of pamidronate as much as 30 days before liver transplantation can effectively prevent bone resorption in patients vulnerable to bone loss post-transplantation. This suggests a potential protective effect of pamidronate against fracture in this patient group. However, further research is required to definitively establish the protective effect of pamidronate, including its specific impact on bone turnover and fracture incidence in the post-transplant period."]]}, {"passage_id": "50_72472207_3", "passage": "The possible reason for this can be that injectables are taken every three months. Only one woman mentioned the condom as a family planning method. No one mentioned about male sterilization while two people mentioned Norplant. Those who are involved in promoting family planning services should, thus, focus on all methods of family planning whether or not the methods are available in their localities. What is surprising is that condom is being promoted for the purpose of HIV prevention but the contraceptive effect of condom is not largely known in this area.\n\n Only seven hundred and nineteen women (53.7%) thought that family planning is important. Six hundred and seven (45.3%) didn't know about family planning and thus couldn't judge its importance. Sustainable efforts are needed to bring about behavioral change towards using family planning methods. Three hundred and nine women (23.1%) had ever used contraceptives and 165 (12.3%) are currently using family planning methods. Other studies conducted in the past in North Gondar Zone (which also included Dembia District) (12) and Dabat District (14) reported current prevalence rates of 8.6% and 2.4%, respectively. In this regard the coverage found in Dembia District is higher. The reasons could be that either family planning service is increasing in the zone or this high coverage can be attributed to the plain topography of Dembia District which makes it possible to reach to most peasant associations by any available means of transportation. On the contrary, a study conducted in 1999 in Gondar town and the surrounding peasant associations (13) had reported current contraceptive prevalence rate of 28.6 % (35.5 % in Gondar town & 11.0 % in the rural peasant associations found around Gondar Town), which is higher than the coverage found in this study. The reason could be that the people who are living in Gondar town are better educated and have better access to different family planning methods. The coverage in the surrounding peasant associations was also higher compared to the results of this study. One possible explanation for this is that people in the surrounding areas of Gondar have the opportunity to use the family planning services in Gondar town and the health institutions around Gondar. According to the 1995 (E.C) report of the Ministry of Health, the national and regional family planning coverage was 21.46% and 22.16%, respectively (3) . Considering the plain topography of the district, the coverage found in Dembia District could be considered low, which requires designing better strategies for family planning delivery. The current contraceptive prevalence rate in Kolla Diba town was 22.5% while in the rural areas as a whole, only 41 (5.2%) are currently using contraceptives. Of the rural areas the family planning coverage in Chillo and Meskele Kirstos was 7.5 & 6.5%, respectively. There was no single user of family planning methods in Mangie Peasant Association, which was the remotest of the areas selected for the study. The coverage difference between Chillo and Meskelekirstos is low, being only 1%. This narrow difference could be the result of the presence of CBRHAs in Meskelekirstos.\n\n Only three methods were found to be used by the clients. These are injectables (64.2%), pills (34.5) and IUCD (1.2%). The wide use of injectables is in agreement with a study conducted in Gondar town and the surrounding areas (13) . The fact that only three methods were used by the clients shows that family planning service providers depend only on the family planning methods which are available in health institutions found in Dembia District. The district needs to provide a wide range of family planning services like Norplant. Additionally, after providing appropriate information, clients could have been referred to Gondar town, which is only 35 kms far from Kolla Diba, for the application of other methods like tubal ligation and vasectomy. This can improve the quality of family planning services in the area (19, 20) .\n\n The desire to have many children was mentioned by a large proportion of women as a reason for not ever using and not desiring to take contraceptives in the future. Other studies conducted in the past also revealed this fact (12, 13, 14) . It is a known tradition in Ethiopia that having many children is considered as an asset. This tradition should stop somewhere since this has an economic and social impact (21) . Side effects caused by the methods were also mentioned by a significant number of women as a reason for discontinuing or not having the desire to take contraceptives in the future. It is true that each contraceptive has its own side effects.", "qa": [["50_72472207_3_1", "What are some reasons mentioned by women for not using contraceptives?\n", "Some reasons mentioned by women for not using contraceptives include the desire to have many children, as it is considered an asset in Ethiopian culture, and concerns about the side effects caused by the methods."], ["50_72472207_3_2", "What are the current prevalence rates of family planning methods in Dembia District compared to other areas?\n", "In Dembia District, the current prevalence rates of family planning methods are 12.3%, with injectables being the most widely used method. Other studies conducted in the past in North Gondar Zone reported prevalence rates of 8.6% and 2.4% in different districts, indicating that the coverage in Dembia District is relatively higher."], ["50_72472207_3_3", "What are some suggestions for improving the quality of family planning services in Dembia District?\n", "To improve the quality of family planning services in Dembia District, it is suggested to provide a wider range of family planning methods, such as Norplant, and refer clients to nearby areas like Gondar town for other methods like tubal ligation and vasectomy. This can be done after providing appropriate information to the clients."]]}, {"passage_id": "57_18621841_2", "passage": "A brain CT scan was performed and showed the previous lesions (a couple revealing worsening with a slight ring contrast enhancement and perilesional edema, others revealing improvement) and some new lesions (Additional file 1: Figure S4) . A magnetic resonance imaging (MRI) of the dorsal spinal cord revealed myelitis: multiples areas of high signal intensity on the T2-weighted images, located between D3 and D9, and probably a posterolateral lesion at D7-D8 (Fig. 1 ). At this moment (i.e., 3 months after HAART initiation), the effect of therapy on the CD4 + T cells and viral load was already evident; CD4 + T cell count was 128 cells/\u03bcL and the plasma viral load of 121 copies/mL. The cerebrospinal fluid (CSF) evaluation showed: 5 cells/\u03bcL; 58 mg/ dL protein; 51 mg/dL glucose; negative results for Gram and acid-fast stains, microbiological cultures, cryptococcal antigen, Venereal Disease Research Laboratory (VDRL) test and polymerase chain reactions (PCR) to identify other common agents (JC virus, BK virus, cytomegalovirus, herpes simplex virus, human herpesviruses 6, varicella-zoster virus, and enterovirus). A CSF PCR for T. gondii was not performed due to technical limitations. The CSF PCR for Epstein-Barr virus was positive, but a normal CSF lymphocyte phenotype assay and a whole-body positron emission tomography (PET) scan revealed neither hyper-metabolic cerebral nor medullar lesions, which ruled out CNS lymphoma. Serological HTLV-I/II antibody assay was negative. The clinical and laboratory information suggested a potential case of CNS-IRIS. The differential diagnosis between IRIS and a progression of toxoplasmosis infection is difficult, but if the last hypothesis was true, most probably a marked worsening of all the previous lesions would be present in the brain CT scan. Furthermore, the patient stated that he had correctly took his antitoxoplasma medication.\n\n Because there are no consensual recommendations for the treatment of toxoplasmosis-associated CNS-IRIS, anti-toxoplasma therapy with full doses was restarted by precaution, HAART and prophylaxis for opportunistic infections were maintained (Fig. 1) and the patient was kept under close surveillance. Corticosteroids were not administered.\n\n One month after hospitalization, the clinical status of the patient was stable and the results of a new brain and medullar MRI were similar to the previous one. A stereotactic brain/medulla biopsy was planned, however it was readily postpone due to its inherent technical risks as soon as the patient revealed signals of clinical improvement. A MRI was repeated 4 months later showing a significant improvement of the brain lesions and a complete resolution of the medullar lesions. The patient presented fully sensory recovery about 9 months after HAART onset, only maintaining the sequelar left hemiparesis. At that time-point, CD4\n\n + T cell count was 244 cells/\u03bcL and Figure S3 and S4, respectively. c) Suppressive therapy: long-term, low-dose anti-toxoplasma therapy to prevent further recurrent episodes the HIV viral load was <20 copies/mL. The patient maintained in addition to HAART suppressive treatment for toxoplasmosis for 12 months.\n\n A detailed analysis of T cell subsets evolution was performed in the present CNS-IRIS case. Taking into consideration that this investigation encompasses a single case of a rare condition, the analysis was complemented with a group of HIV-infected patients that did not develop IRIS and were followed in the same hospital, all with <100 CD4 + T cells/\u03bcL at baseline and a rapid decline of the HIV plasma viral load after HAART initiation, similar to what was observed in the reported case.\n\n No differences were observed on the absolute numbers of memory CD4 + or CD8 + T cells (CD45RA \u2212 CD45RO + ) between the CNS-IRIS case and controls at any time-point. The case had a significant lower percentage of memory CD4 + T cells at baseline, but this difference was inverted one month later (Fig. 2) . In both CNS-IRIS case and controls, there was an increase in the percentage of memory CD4 + T cells from baseline to 1 month, but this increase was higher in the CNS-IRIS case (1.4 fold-change) compared to the controls (1.1 mean fold-change). A higher percentage of memory CD4 + T cells in the CNS-IRIS patient was still present at symptoms onset and IRIS diagnosis (i.e. 2 and 3 months after HAART initiation).", "qa": [["57_18621841_2_1", "What were the main diagnostic tests and markers used to assess the patient's condition, and how did they contribute to reaching a potential diagnosis of CNS-IRIS versus toxoplasmosis progression?", "The diagnostic tests included a brain CT scan, an MRI of the dorsal spinal cord, evaluation of CD4 + T cell count and viral load, cerebrospinal fluid (CSF) analysis, including cell count, protein, glucose, and testing for various pathogens via PCR and serological assays. These tests aided in the assessment of the patient's condition, distinguishing between potential diagnoses of CNS-IRIS and toxoplasmosis progression. The brain CT and MRI revealed existing and new lesions, while CSF evaluation and serological assays provided information on the presence of specific pathogens. These tests played a crucial role in differentiating between the two potential diagnoses."], ["57_18621841_2_2", "What treatment strategies were employed for the patient's condition, and why were some specific interventions chosen over others?", "The patient's treatment involved restarting anti-toxoplasma therapy with full doses as a precaution, maintaining highly active antiretroviral therapy (HAART) and prophylaxis for opportunistic infections, as well as close surveillance. Notably, corticosteroids were not administered. The decision to restart anti-toxoplasma therapy with full doses was based on the lack of consensual recommendations for the treatment of toxoplasmosis-associated CNS-IRIS. Additionally, the choice to maintain HAART and prophylaxis for opportunistic infections aimed to manage the patient's HIV infection and prevent further complications."], ["57_18621841_2_3", "How did the patient's T cell subset analysis contribute to the understanding of the CNS-IRIS case and its comparison to HIV-infected patients who did not develop IRIS?", "The T cell subset analysis provided valuable insights into the CNS-IRIS case and its comparison to HIV-infected patients who did not develop IRIS. The analysis revealed differences in the percentage of memory CD4 + T cells between the CNS-IRIS case and controls at various time-points, particularly at baseline, one month after HAART initiation, and at symptoms onset and IRIS diagnosis. These findings contributed to a better understanding of the immune response in the CNS-IRIS case, especially in comparison to HIV-infected patients who did not develop IRIS, despite similar baseline characteristics in terms of CD4 + T cell count and HIV plasma viral load."]]}, {"passage_id": "53_5176893_0", "passage": "Increasing interest is now being paid to a correct diagnosis in the early stages of the different types of degenerative diseases of the central nervous system of infancy and childhood. Among the diseases which affect the white matter of the nervous system a subgroup is recognized as the heredo-degenerative type or the leucodystrophies. In 1956 Poser and van Bogaert (1956) stated that it was almost imposslble to make a clinical diagnosis of the leucodystrophies unless there was a well-documented family history. However, by the demonstration of a characteristic sulphatide pattern of urinary sediment lipids by paper chromatography the metachromatic type of leucodystrophy (sulphatidosis) can now be diagnosed during life (Hagberg and Svennerholm, 1960; Hagberg, Sourander, and Svennerholm 1961) . It can also be diagnosed through the demonstration of brown metachromatic granular deposits in peripheral nerve biopsy specimens (Thieffry and Lyon, 1959; Hagberg, Sourander, and Thoren, 1962) . In the other important type of leucodystrophy, the globoid cell type of Krabbe's disease, no characteristic clinical laboratory tests have hitherto been described. However, diagnostic help can be obtained from analysis of the proteins of the cerebrospinal fluid. This will be shown in six well-documented cases of Krabbe's disease. CASE HISTORIES CASE 1 E.M., born in 1959, was the second girl of two. Her elder sister had died aged 10 months of a disorder with a nearly identical clinical picture. The patient developed normally during her first few months of life. However, she was restless and cried more than usual from 7 weeks of age. From about 4 months of age she showed irritability, and regression of mental and motor development and general stiffness. One month later she lay in the opisthotonus position with extended and crossed legs, and variable muscular hypertonus, bilateral ankle clonus, and degenerative changes of the visual fields were seen. When 6 months old she was blind with optic atrophy, had no contact with her surroundings, was hyperaesthetic and cried violently, had clonic fits, and was in a state of extensor rigidity. The serum lipids were: cholesterol 126 mg./100 ml., phospholipids 169 mg./100 ml., triglycerides 79 mg./100 ml. She died aged 20j months old. CASE 2 A.W., a girl born in 1960, was the first child of a healthy family and appeared normal until 1 month old. She then started to be irritable and screamed frequently. Regression of mental and motor development, general stiffness, and fever of unknown origin, were noted about two months later. When 5 months old she was severely retarded, lay in the opisthotonus position with extended and crossed legs, and showed muscular hypertonus of variable intensity. The serum lipids on two different occasions were: cholesterol 118, 141 mg./100 ml.; phospholipids 162, 167 mg./100 ml.; lecithin 129,133 mg./ 100 ml.; sphingomyelin 37, 38 mg./100 ml. triglycerides 83, 68 mg./100 ml. Cholecystography was normal. She died aged 71 months. CASE 3 K.J., a girl born in 1961, was the third child of a family where an elder brother had died at the age of 13 months with the same clinical picture. This girl was healthy and normal until 4j months old, when she started to cry day and night. When she cried, clonic fits were observed in the arms and legs. She rapidly regressed in development and soon showed a state of variable hypertonicity. She also had bouts of fever without any signs of infection. When 6 months old she was markedly hyperaesthetic and lay in a constant opisthotonus position, with extensor rigidity. She was probably blind and was hypersensitive to sounds. The serum lipids were: cholesterol 202 mg./100 ml., phospholipids 222 mg./100 ml., lecithin 161 mg/100 ml., sphingomyelin 56 mg./100 mg., triglycerides 71 mg./100 ml. Cholecystography was normal. She died aged 16j months.\n\n CASE 4 E.S., a girl born in 1958, was the eldest sister of three, case 5 being the youngest one. Her mental and motor development was quite normal until 4 months of age when she stagnated and then lost already acquired mental and motor skills.", "qa": [["53_5176893_0_1", "What are the diagnostic methods for identifying the metachromatic type of leucodystrophy (sulphatidosis)?\n", "The metachromatic type of leucodystrophy (sulphatidosis) can be diagnosed through the demonstration of a characteristic sulphatide pattern of urinary sediment lipids by paper chromatography. It can also be diagnosed through the demonstration of brown metachromatic granular deposits in peripheral nerve biopsy specimens."], ["53_5176893_0_2", "How can Krabbe's disease be diagnosed?\n", "Currently, there are no characteristic clinical laboratory tests for diagnosing Krabbe's disease, specifically the globoid cell type. However, diagnostic help can be obtained from the analysis of the proteins in the cerebrospinal fluid."], ["53_5176893_0_3", "What are some common symptoms and characteristics of the degenerative diseases of the central nervous system in infancy and childhood?\n", "Common symptoms and characteristics of degenerative diseases of the central nervous system in infancy and childhood include regression of mental and motor development, general stiffness, irritability, visual field degeneration, optic atrophy, clonic fits, extensor rigidity, and hyperaesthesia."]]}, {"passage_id": "60_2685300_0", "passage": "Ergonomic interventions to improve working conditions and to prevent the occurrence of occupational diseases have been widely used in industrial production lines [1] . In case of productive cells characterized by tasks that require some type of physical burden and/or the adoption of cognitive interventions to reduce the level or duration of exposure of workers, such as: redesign of jobs, suitable furniture and tools, ergonomic guidelines, rest breaks and rotation function [2] . The latter, considered organizational measures are generally adopted in tasks whose exposure level cannot be lowered due to the characteristics of the job or through physical measures [3] .\n\n Job rotation is one of the most practiced organizational interventions, such as cost reduction and/or prevention and health promotion for workers. Cost reduction is achieved by training workers to perform a greater number of functions for flexible allocation of worker activity. The prevention and health promotion for workers occurs through switching between different tasks with different levels of exposure and biomechanical applications, which in theory reduce the cumulative and or average exposure that should in turn promote the reduction of musculoskeletal and cognitive overloads [2, 4] .\n\n Thus, the job rotation has been adopted in repetitive, static, or monotonous activities, aiming to relieve the effects of muscle and cognitive overload, monotony, absenteeism, and stress [5, 6] .\n\n Regarding the assessment of the effectiveness of the job rotation and the prevention of musculoskeletal complaints strategy, studies show conflicting results. Hinnen et al. evaluated supermarket workers by a cross-sectional study and found a 40% reduction in complaints of neck pain and a 20% reduction in complaints of pain in the shoulder for those who carried out labor [7] . Another study compared groups who performed or did not perform job rotation, and also found significant reductions in physical burden on workers of a garbage collection department who underwent rotation between the driving task and collecting trash [8] . However, while performing in a longitudinal study, it was observed that in the long run, the job rotation increased overload on other body regions, and workers began to report more back pain, especially in the group who had just completed the driving task [9] . Probably, it is because the groups continued exposure to the same risk factors, even when switching the task. Similar results were obtained by Frazer et al. [10] , who evaluated two tasks of material handling, with low and high overhead level. The risk of lower back pain increased as a greater amount of time was used to perform the task with higher overhead, due to the cumulative and peak force [10] .\n\n The different study methods and criteria used for deployment of job rotation may partly explain the heterogeneity of results. This, plus the absence of clinical, controlled, randomized studies evaluating the effectiveness of the job rotation and the prevention of musculoskeletal complaints strategy, complicates the clinical decision making of professionals in the health and safety of the worker.\n\n Importantly, positive results for the worker's health will only be achieved if the planning of the job rotation meets some important criteria such as number of workers and tasks involved, exposure level, requested body region, frequency of movements, duration of exposure, and duration of rest break, among others [11] . To assist in this planning, some methods and algorithms have been developed using the variables mentioned before [11] [12] [13] [14] ; however, the proposed rotation as reported in the literature, has not been evaluated in controlled clinical studies or randomized, and the effect of reducing absenteeism caused by musculoskeletal disorders in the workplace is not yet proven.\n\n All these limitations point to the need for implementing a well design job rotation program along with a welldesigned intervention study in order to fully and robustly evaluate the theory of job rotation and its application. Hence, we aimed to develop such a well-designed job rotation program that has specific criteria to reduce the cumulative and average biomechanical exposure and then to evaluate the effect of this newly developed job rotation program in the prevention of musculoskeletal disorders in industrial textile workers.\n\n \n\n This is a randomized cluster controlled trial, prospectively registered, and with blinded assessment will be used to investigate the effectiveness of job rotation to prevent musculoskeletal disorders in industrial workers. The procedures and consent form were approved by the Research Ethics Committee of Cidade de S\u00e3o Paulo University (protocol no. 18170313.5.0000.0064) and were prospectively registered at ClinicalTrials.gov -NCT01979731. The study is being funded by the National Counsel of Technological and Scientific Development (CNPq), Brazil (473651/2013-0).\n\n Study participants will be production line workers, recruited from the textile industry of a large company in Bahia, Brazil. The productive sectors of this industry will be classified according to the level of exposure to risk factors for musculoskeletal pain and disorders.", "qa": [["60_2685300_0_1", "What are some ergonomic interventions that can be implemented to improve working conditions and prevent occupational diseases?\n", "Ergonomic interventions that can be implemented to improve working conditions and prevent occupational diseases include the redesign of jobs, the use of suitable furniture and tools, the implementation of ergonomic guidelines, the provision of rest breaks, and the adoption of job rotation. These interventions aim to reduce the level or duration of exposure of workers to physical burden and cognitive interventions, thereby reducing the risk of musculoskeletal and cognitive overloads."], ["60_2685300_0_2", "How does job rotation contribute to the prevention of musculoskeletal and cognitive overloads in workers?\n", "Job rotation is a commonly practiced organizational intervention that aims to prevent musculoskeletal and cognitive overloads in workers. It involves switching between different tasks with different levels of exposure and biomechanical applications. By rotating workers between tasks, the cumulative and average exposure to specific risk factors can be reduced, leading to a decrease in musculoskeletal and cognitive overloads. Job rotation also helps to relieve the effects of muscle and cognitive overload, monotony, absenteeism, and stress."], ["60_2685300_0_3", "What are some factors that need to be considered when planning a job rotation program to ensure its effectiveness in preventing musculoskeletal disorders?\n", "When planning a job rotation program to prevent musculoskeletal disorders, several important criteria need to be considered. These include the number of workers and tasks involved, the exposure level to risk factors, the requested body region, the frequency of movements, the duration of exposure, and the duration of rest breaks, among others. These factors play a crucial role in determining the effectiveness of the job rotation program in reducing the cumulative and average biomechanical exposure and preventing musculoskeletal disorders in workers."]]}, {"passage_id": "56_51613870_2", "passage": "Recipients and donors characteristics are presented in Table 1 .\n\n The median age of recipients was 50 years (IQR, 42-52 years). The majority of the subjects were male (n \u00bc 11, 68%) and of Caucasian origin (n \u00bc 15, 79%). KT recipients were followed up for a median of 2.4 years (IQR, 1.2-4.6 years). At the time of the analysis, According to local protocol, in the early postoperative period, 11 KT recipients received an immunosuppressive treatment including low-dose Cys, EVL, and steroid. From 2015, eight patients received TAC, MPA, and steroid. Table 2 summarizes the dose adjustments of all the immunosuppressive drugs within five years following transplantation.\n\n The median age of HIV \u00c0 KT recipients was 52 years (IQR, 43-60 years); older recipients (age !65 years) had a median age of 68 years (IQR, 66-70 years). Whereas there were no significant differences in ages between HIV-infected and the entire uninfected population (P \u00bc 0.065), age difference was significant between older HIV \u00c0 and HIV \u00fe patients (P 0.0001).\n\n Preferred drugs included raltegravir and dolutegravir for the INSTI class, maraviroc for CCR5 receptor antagonist, lamivudine for NRTI, and rilpivirine for NNRTI. These antiviral drugs were largely used in our Center as they offered the advantage of having no drug interactions and minimal toxicity. Indeed, 94.7% of patients were on raltegravir or dolutegravir, 66.6% on lamivudine, and 42.1% on maraviroc or rilpivirine, at the end of follow-up (Table 3) .\n\n On the other hand, some drugs were avoided when possible: these included boosted regimens and some NNRTIs (risk of pharmacological interactions), tenofovir disoproxil fumarate (risk of renal toxicity), and abacavir (apparent risk of cardiovascular disease).\n\n Patient and graft survival. Patient survival among all HIV \u00fe recipients at one, three, and five years was 94.4%, 94.4%, and 70.8%, respectively. One-, three-, and five-year graft survival rates were 84%, 72%, and 55.6%, respectively. Graft survival censored for patient death at one, three, and five years was 94.7%, 81.4%, and 81.7%, respectively (Table 4) .\n\n Delayed graft function occurred in 17% (n \u00bc 3) of KT recipients. Three patients died with a functioning graft from cardiovascular causes. Four patients had graft failure due to AMR (n \u00bc 2) and allograft nephrectomy (n \u00bc 2). Only two patients with persistent and untreated HCV replication had a poor outcome: one resumed hemodialysis one year after transplantation owing to complications of severe infectious disease (suppurative bacterial pyelonephritis caused by atypical mycobacteria), and one died of a sudden acute myocardial infarction four years from transplantation. Patient, graft, and death-censored graft survival among all HIV \u00c0 recipients and HIV \u00c0 recipients aged !65 years at one, three, and five years are illustrated in Table 4 and Figure 1 . Compared with all HIV \u00c0 recipients, HIV \u00fe had similar one-year (94.4% vs. 98.9%, P \u00bc 0.12) and threeyear (94.4% vs. 95.6%, P\u00bc 0.6) patient survival and \u00fe and HIV \u00c0 patients who underwent kidney transplantation at our Center during the same time frame. Uninfected kidney transplant recipients were stratified by age in two groups: older recipients (aged ! 65 years) and all recipients. HIV \u00fe kidney transplant recipients had a significantly inferior five-year patient survival rates compared to HIV \u00c0 kidney-transplant recipients (P \u00bc 0.03) and a similar rate (P \u00bc 0.79) compared to older recipients. Five-year graft survival rates of HIV \u00fe recipients were between those reported for older kidney transplant recipients and for all kidney transplant recipients; there were no significant differences between HIV \u00fe and all HIV \u00c0 recipients (P \u00bc 0.14) and HIV \u00fe and older HIV \u00c0 recipients (P \u00bc 0.44).", "qa": [["56_51613870_2_1", "What are the preferred antiviral drugs used in kidney transplant recipients with HIV?\n", "The preferred antiviral drugs used in kidney transplant recipients with HIV include raltegravir and dolutegravir for the INSTI class, maraviroc for CCR5 receptor antagonist, lamivudine for NRTI, and rilpivirine for NNRTI. These drugs are chosen because they have minimal drug interactions and toxicity."], ["56_51613870_2_2", "What are the patient and graft survival rates among kidney transplant recipients with HIV?\n", "The patient survival rates among all HIV-positive kidney transplant recipients at one, three, and five years were 94.4%, 94.4%, and 70.8%, respectively. The graft survival rates at one, three, and five years were 84%, 72%, and 55.6%, respectively. When censored for patient death, the graft survival rates at one, three, and five years were 94.7%, 81.4%, and 81.7%, respectively."], ["56_51613870_2_3", "How does the patient survival of kidney transplant recipients with HIV compare to uninfected kidney transplant recipients?\n", "The patient survival rates of kidney transplant recipients with HIV were similar to uninfected kidney transplant recipients at one and three years. However, at five years, the patient survival rates of HIV-positive recipients were significantly inferior compared to uninfected recipients. There were no significant differences in graft survival rates between HIV-positive and uninfected recipients."]]}, {"passage_id": "75_206313323_0", "passage": "introduction Neoadjuvant (preoperative or primary) systemic chemotherapy has been the long-standing treatment of choice for patients with locally advanced breast cancer [1] [2] [3] [4] [5] . In recent years, it has also become a frequently used option in patients with operable early breast cancer (EBC) and clear indication for adjuvant chemotherapy [6, 7] . Although overall no survival advantage for the neoadjuvant treatment approach as compared with adjuvant treatment has been shown in this setting [8, 9] , advantages of neoadjuvant systemic chemotherapy such as the higher rate of breast-conserving surgery [8, [10] [11] [12] and the possibility of in vivo chemosensitivity testing validate its use in EBC.\n\n Neoadjuvant systemic chemotherapy substantially reduces the size of primary tumor and lymph node metastases in >80% of cases, improving operability and increasing the probability of breast-conserving surgery [13] . It also provides the earliest possible systemic treatment against preexisting micrometastases and might be associated with better survival in a subgroup of young women after extended follow-up [14] . Furthermore, neoadjuvant treatment permits a rapid assessment of response of the primary tumor to a particular chemotherapy regimen. This assessment allows the opportunity to cross over to a regimen with non-cross-resistant drugs for an individual patient if there is minimal or no response to the first-line regimen [15] . In separate meta-analyses, neoadjuvant and adjuvant therapy have been shown to produce apparently equivalent survival and disease progression rates [16] , with neoadjuvant therapy associated with higher rates of breastconserving surgery and pathological complete response (pCR) [17] .\n\n The most common approaches are to start neoadjuvant treatment with anthracycline-taxane-based combination regimens or with an anthracycline-based regimen sequentially followed by a taxane either docetaxel every 3 weeks (q3w) or paclitaxel weekly (2009 guidelines of the AGO for diagnosis and treatment of breast cancer version are available at www.agoonline.com). Tumors are well known to be non-cross-resistant with taxanes and anthracycline-based regimens. In patients with advanced breast cancer in whom treatment with anthracycline-based regimens failed, the use of taxanes resulted in overall response rates of 18% to >50% [8] . In two separate phase III trials, neoadjuvant doxorubicin plus cyclophosphamide followed by docetaxel (AC-D) has produced promising results [8, 18] . In the National Surgical Adjuvant Breast and Bowel Project (NSABP) B-27 study, AC-D given before surgery achieved a better overall response rate of 91% (versus 86% with AC alone, P < 0.001) and a higher pCR rate of 26% (versus 14% for AC alone, P < 0.001) [8] . Neoadjuvant AC-D also provided a superior pCR rate of 14% when compared with 7% for dose-dense doxorubicin plus docetaxel in the German Preoperative Adriamycin Docetaxel Study Group (GEPARDUO) trial (odds ratio = 2.2, P < 0.001) [18] .\n\n The achievement of pCR is associated with improved survival [2] . Therefore, pCRs are widely accepted as surrogate markers for survival. However, it must be noted that high pCR rates are mainly observed in patients with hormone receptor (HR)-negative breast cancer either triple negative [estrogen receptor (ER), progesterone receptor (PgR), and human epidermal growth factor receptor 2 (HER2) negative] or HR negative and HER2 positive. pCR rates in HR-positive breast cancer in general are low, resulting in questions about the benefit of chemotherapy in those patients [19] . Therefore, it is reasonable to evaluate if the introduction of new compounds into sequential standard regimens may increase pCR rates and thereby improve the long-term outcome. Pemetrexed (Eli Lilly and Company, Indianapolis, IN, USA), a folate antimetabolite, inhibits thymidylate synthase, dihydrofolate reductase, and glycinamide ribonucleotide formyl transferase. In pretreated advanced breast cancer with and without vitamin supplementation, single-agent pemetrexed achieved response rates in the range of 8%-28% [20] [21] [22] [23] [24] ; in locally advanced breast cancer with vitamin supplementation, the response rate was 31% [25] .", "qa": [["75_206313323_0_1", "What are the advantages of neoadjuvant systemic chemotherapy in the treatment of breast cancer?\n", "Neoadjuvant systemic chemotherapy in the treatment of breast cancer offers several advantages. It substantially reduces the size of the primary tumor and lymph node metastases, making surgery more feasible. It also provides the earliest possible systemic treatment against preexisting micrometastases and may be associated with better survival in certain patient groups. Additionally, neoadjuvant treatment allows for a rapid assessment of the tumor's response to chemotherapy, providing the opportunity to switch to a different regimen if there is minimal or no response to the initial treatment. Neoadjuvant therapy is also associated with higher rates of breast-conserving surgery and pathological complete response."], ["75_206313323_0_2", "What are the common approaches to neoadjuvant treatment in breast cancer?\n", "The most common approaches to neoadjuvant treatment in breast cancer involve starting with anthracycline-taxane-based combination regimens or an anthracycline-based regimen followed by a taxane. These regimens are chosen because tumors are known to be non-cross-resistant with taxanes and anthracycline-based drugs. In patients with advanced breast cancer who have failed treatment with anthracycline-based regimens, the use of taxanes has shown overall response rates ranging from 18% to over 50%. Studies have shown promising results with neoadjuvant doxorubicin plus cyclophosphamide followed by docetaxel, achieving higher overall response rates and pathological complete response rates compared to other regimens."], ["75_206313323_0_3", "What is the significance of achieving pathological complete response (pCR) in breast cancer treatment?\n", "Achieving pathological complete response (pCR) in breast cancer treatment is associated with improved survival. pCR refers to the absence of any residual invasive cancer in the breast and lymph nodes after neoadjuvant treatment. It is considered a surrogate marker for survival and is widely accepted as an important outcome measure in clinical trials. However, it is important to note that high pCR rates are mainly observed in patients with hormone receptor (HR)-negative breast cancer, particularly those who are triple negative (negative for estrogen receptor, progesterone receptor, and human epidermal growth factor receptor 2). In HR-positive breast cancer, pCR rates are generally low, raising questions about the benefit of chemotherapy in those patients."]]}, {"passage_id": "0_10495864_4", "passage": "We continue to describe the cohort as euthymic to remain consistent with the bulk of the literature; however, the term interepisode should be considered. Whether poor sleep quality in this population is best categorized as prodromal or residual, treatment targeting the sleep quality outside of targeting depressive symptoms may reduce the substantial impairment and disruption in social and occupational functioning.\n\n We found some consistencies between factors associated with poor sleep quality and psychological and environmental factors that are important predictors of sleep quality in the general population. In the HC group, social undermining predicted poor sleep, whereas in the BD group, association was found between poor sleep and neuroticism and stressful life events. Neuroticism is measured in the widely used five-factor model of personality developed by Costa and McCrae, which also measures extraversion (E), openness (O), agreeableness (A), and conscientiousness (C) (Costa and McCrae 1992) . Neuroticism (N) measures the tendency to experience negative affects including fear, sadness, embarrassment, anger, and disgust. Individuals with BD have been shown to have elevated N and O, and low A, C, and E (Barnett et al. 2011) . N marks a trait that has been associated with many psychiatric illnesses in addition to BD, including major depressive disorder, anxiety disorders, and personality disorders (Barnett et al. 2011; Costa et al. 2005; Middeldorp et al. 2011; Bagby et al. 2008; Bienvenu et al. 2004 ). Personality traits have high heritability and may predispose individuals to psychiatric illness; however, there is some indication that personality traits may be affected by psychiatric illness as well (Costa et al. 2005; Christensen and Kessing 2006) . We believe that the association between poor sleep quality and neuroticism reported here is a general phenomenon which may be enhanced and exaggerated in BD patients, perhaps due to genetic predisposition, but is not unique to BD or to psychiatrically ill populations. We found that poor sleep quality was associated with stressful events in euthymic BD and social stress in the HC group, which has also been found in general population samples (Fernandez-Mendoza et al. 2010b; LeBlanc et al. 2007 ). This is particularly important for BD because social stress has been shown to be greater in BD patients even when euthymic and was shown to be bi-directionally related to disturbed sleep . Affect reactivity in response to stress has also been shown to be greater in euthymic BD than in unipolar depressive disorder or controls (Knowles et al. 2007 ). Negative mood and sleep have also been shown to have a bi-directional relationship in a euthymic bipolar population; that is, negative mood in the evening may disturb sleep, and disturbed sleep may cause negative mood in the morning . Close attention to the role of stress and mood reactivity is warranted in future studies of sleep quality in euthymic BD.\n\n A history of rapid cycling was related to poor sleep quality in the euthymic BD group. Rapid cycling may be associated with sleep quality due to frequent cycling and short euthymic periods between episodes, and similar to stress reactivity, rapid cycling can be conceptualized as both a cause of poor sleep quality and a result of underlying poor sleep quality. In fact, rapid cycling may describe a distinct phenotype with biologically different properties driving both the mood and sleep disturbance. For example, rapid cycling has been linked to panic disorder in familial and genetic studies (MacKinnon and Zamoiski 2006; MacKinnon et al. 2003a, b) and has also been linked to low-activity risk alleles of the catechol-Omethyl transferase gene (Papolos et al. 1998; Kirov et al. 1998; Joyce et al. 1995) as well as abnormalities in the hypothalamic-pituitary-thyroid axis (Chakrabarti 2011) . In addition, the hypothalamic-pituitary-adrenal (HPA) axis is an important modulator of sleep and the immune system and is abnormally activated in chronic insomnia (Vgontzas and Chrousos 2002; Vgontzas et al. 2004 Vgontzas et al. , 2007 Basta et al. 2007 ). One pilot study has shown abnormally high cortisol and high cortisol response to the dexamethasone suppression test in five patients with rapid cycling, regardless of mood state (Watson et al. 2005) . Underlying HPA axis dysfunction at baseline or in response to stress may link sleep quality, rapid cycling, and stress reactivity in BD.", "qa": [["0_10495864_4_1", "How does neuroticism relate to sleep quality in individuals with bipolar disorder?\n", "Neuroticism, which measures the tendency to experience negative affects, including fear, sadness, embarrassment, anger, and disgust, has been found to be associated with poor sleep quality in individuals with bipolar disorder. Neuroticism is a trait that is commonly seen in psychiatric illnesses, including bipolar disorder, major depressive disorder, anxiety disorders, and personality disorders. It is believed that neuroticism may be a general phenomenon that is enhanced and exaggerated in individuals with bipolar disorder, possibly due to genetic predisposition. However, poor sleep quality and neuroticism are not unique to bipolar disorder or psychiatrically ill populations."], ["0_10495864_4_2", "What factors are associated with poor sleep quality in individuals with bipolar disorder?\n", "In individuals with bipolar disorder, factors such as social undermining, neuroticism, and stressful life events have been found to be associated with poor sleep quality. Social undermining predicts poor sleep in individuals without bipolar disorder, while neuroticism and stressful life events are specifically associated with poor sleep in individuals with bipolar disorder. It is important to note that these factors are also important predictors of sleep quality in the general population. Social stress has been shown to be greater in individuals with bipolar disorder, even when they are euthymic (not experiencing symptoms), and is bi-directionally related to disturbed sleep. Affect reactivity in response to stress has also been found to be greater in individuals with bipolar disorder compared to those with unipolar depressive disorder or no psychiatric illness."], ["0_10495864_4_3", "How does rapid cycling relate to sleep quality in individuals with bipolar disorder?\n", "Rapid cycling, which refers to frequent mood episodes and short periods of euthymia (no symptoms) between episodes, is related to poor sleep quality in individuals with bipolar disorder. Rapid cycling may be associated with sleep quality due to the cycling nature of the disorder and the short periods of stability between episodes. It is possible that rapid cycling is both a cause and a result of underlying poor sleep quality. Rapid cycling has been linked to panic disorder and abnormalities in the hypothalamic-pituitary-thyroid axis. Additionally, the hypothalamic-pituitary-adrenal (HPA) axis, which is involved in regulating sleep and the immune system, is abnormally activated in chronic insomnia. Abnormalities in the HPA axis may link sleep quality, rapid cycling, and stress reactivity in bipolar disorder."]]}, {"passage_id": "49_47014335_0", "passage": "Due to its superficial location and alignment, clavicle is one of the bone that is most frequently fractured in the upper body due to direct or indirect trauma. As of the presence of numerous muscular and ligamentous attachment along with weight of upper limb, the clavicle is able to carry significant force.\n\n Fracture of clavicle is most common due to its subcutaneous location. It accounts for 3-5% of all fractures in adults and 10-15% of all fractures in paediatric age group. 1,2 Roughly a Quarter of every clavicle fractures seems to be at the distal end. 2 Neer has classified these lateral end fractures into three types ( Fig. 1 ) according to their relation to the coracoclavicular ligaments 3 and Rockwood in 1982, subclassified Type II fractures as Type IIA and Type IIB fractures. 4 Neer observed that the type II fractures carries a higher risk of non union (as high as 25-50%) for conservatively managed fractures. 3, 4 As trapezius displaces the proximal fragment superiorly and the weight of the arm draws the distal fragment inferiorly results in major displacement which leads to higher incidence of non union. 5 Among this, 15% non union is symptomatic and painful, which have made many to suggest early surgical management of this fractures. 6 The delayed conservative management results in bone resorption, prominent deformity and an altered surgical field that further complicates any subsequent surgical intervention. 6 Delay in surgical intervention results in elevated complication rate. Surgical management ranges from joint spanning to articulation sparing implants, distal clavicle excision, osteosynthesis by hook plate or a locking compression plate fixation, poor fixation still remains a challenge and no definitive solution has been identified, so none is widely accepted as a Gold standard, each has its own sets of advantages and disadvantages. [7] [8] [9] [10] [11] In our study, we evaluated the fractures which were treated by superior anterior locking plate (Fig. 2) . We measured the following: a) union rates, b) complications, c) functional outcome by Oxford shoulder score and QuickDASH scoring, d) earliest time for return to work to find out its advantages and applicability as compared to previous mentioned modalities of fixation.\n\n In this retrospective analysis of distal clavicular fractures treated at our setting, we found among 14 male patients, there were 12 unilateral unstable fractures (eight right, four left) and two bilateral fractures (case 1 -Figs. 3-7) treated between time period of June 2012 to February 2015. The mean age of patients was 43.5 years (24-55 years). The mode of injuryten due to road traffic accidents and four due to direct fall on shoulder. None of them had significant associated injuries. On initial visit of the patient, we obtained two roentgenic views (AP & Zanca) of the distal clavicle. All patients were taken up for surgery within nine days of injury. Necessary consent and approvals for surgery were obtained. We had an inclusion & exclusion criteria for our study.\n\n All skeletally mature individuals with Type II fractures within 10 days of injury, Absence of pre-existing subacromial pathology. \n\n \n\n Surgical procedures were taken under general anaesthesia after prophylactic antibiotics, with patient in supine position on the operating table. The operated surgical field was elevated using a sand bag under the shoulder for better exploration. Under strict aseptic precaution, patient parts were painted and drapped. We used the standard anterosuperior approach to the clavicle. Both the fracture site and the acromion were exposed completely. The initial reduction of fragments was maintained with K-wires. The appropriate plate size was assessed and fixation performed. In few cases, we encountered inappropriate anatomical reduction where we had to use 2 mm mini fragment screws to lag the fracture fragments together. The lag screws were countersunk in order to avoid hardware prominence and conflicts with positioning of plate.\n\n Due to instability of fracture fragment, in one case, SS (Stainless steel) wiring was done in the form of cerclage for additional stability. The locking compression plate design allowed multiple 2.7 mm locking screws polyaxially in the distal fracture fragment. Both the locking and non locking screws with lag and lock principle was used in the proximal fracture fragment. The torn coracoclavicular ligament was identified and if so, necessary suturing was done. No suture anchors were employed for coracoclavicular augmentation in our fixation procedure. The patient with bilateral clavicular fracture underwent surgical fixtation in the same sitting. Final plate as well as screw position was confirmed with the help of C-arm intensification.", "qa": [["49_47014335_0_1", "How does the anatomical location and structure of the clavicle contribute to its susceptibility to fractures?\n", "The clavicle's superficial location and alignment, along with its numerous muscular and ligamentous attachments and the weight of the upper limb, make it prone to fractures from direct or indirect trauma. Its subcutaneous position and ability to withstand significant force due to its attachments and load-bearing function explain why it is one of the most commonly fractured bones in the upper body."], ["49_47014335_0_2", "What factors influence the risk of non-union in distal clavicle fractures, and how do they impact treatment decisions?\n", "Factors such as the displacement of fracture fragments by the trapezius muscle and the weight of the arm, leading to major displacement, contribute to a higher risk of non-union in distal clavicle fractures. Neer's observation that type II fractures have a higher non-union risk, especially when managed conservatively, underscores the importance of considering early surgical intervention to prevent complications like bone resorption, deformity, and altered surgical fields that can complicate subsequent treatments."], ["49_47014335_0_3", "What are the challenges and considerations associated with surgical management options for distal clavicle fractures, and why is there no widely accepted gold standard treatment?\n", "Surgical management options for distal clavicle fractures range from joint spanning to various implant fixation methods, each with its own advantages and disadvantages. Despite advancements in techniques like superior anterior locking plate fixation, poor fixation outcomes remain a challenge. The lack of a definitive gold standard treatment is due to the varied nature of fractures, individual patient factors, and the complexity of achieving optimal fixation while minimizing complications, leading to a diverse range of surgical approaches without a universally accepted best practice."]]}, {"passage_id": "30_6923067_1", "passage": "Moreover, it is increasingly recognized that glucose-lowering agents may have divergent metabolic and cardiovascular effects (whether acting via their main target or an 'off target' effect), so that glucose-lowering per se is not a guarantee of long-term benefit in terms of outcome. Indeed, cardiovascular 'safety' has been highlighted in the last 3 years by concerns initially raised by Nissen and Wolski 11 regarding the thiazolidinedione (TZD) agent rosiglitazone. The ensuing debate has reshaped the therapeutic landscape, prompting both renewed scrutiny of older agents (including SUs) and closer attention to the long-term effects of other more recently introduced classes [DPP4 inhibitors and glucagon-like peptide-1 (GLP1) agonists].\n\n The key factors guiding drug choice are therefore:\n\n (i) adverse effects associated with main pharmacological action (e.g. hypoglycaemia, weight gain); (ii) agent-specific less predictable (or idiosyncratic) adverse effects [e.g. oedema, fluid retention, fractures (TZDs); pancreatitis (GLP-1 agonists); risk of infection (DPP-4 inhibitors)]; (iii) host factors (e.g. duration of disease/pre-existing congestive heart failure/renal impairment); (iv) cost; (v) years of patient exposure (as a crude indicator of safety).\n\n The latter two are of course closely associated (Table 1) .\n\n It is important to recall that the effects of any particular agent in combination with metformin may not be the same as when that agent is used as monotherapy-although this is often assumed to be the case in clinical practice.\n\n As these agents (SUs) were the mainstays of pharmacological glucose lowering in the decades prior to the publication of UKPDS, they are very familiar to prescribers. Given that they are widely available in generic form, SUs are also low in cost to health services.\n\n The onset of action of SUs to lower blood glucose is more rapid than with other agents. As they increase the amount of insulin secreted by pancreatic b-cells at any given ambient glucose concentration, there is a dose-dependent risk of weight gain and hypoglycaemia. 12 In terms of quantifying these risks, participants treated with glibenclamide monotherapy in A Diabetes Outcome Progression Trial (ADOPT) gained 1.6 kg-all of which occurred over the first year. 13 Severe hypoglycaemia (requiring ambulance and/or hospital treatment) occurs in approximately one in every 100 people treated with a SU each year vs. one in every 2000 with metformin and one in every 10 with insulin. 14 The effect of SUs on glycaemia appeared in ADOPT to be less sustained over time than with either metformin or TZDs (all as monotherapy), 13 a phenomenon attributed to 'b-cell exhaustion'.\n\n Second-generation SUs (e.g. glibencamide, gliclazide and glimepiride) are associated with very low rates of other adverse events, but evidence for their cardiovascular safety is surprisingly sparse, particularly as their mechanism of action depends on opening potassium-ATP (K ATP ) channels and some agents (e.g. glibenclamide) bind to both b-cell (SUR1) and cardiac (SUR2A) subtypes of the adjacent SU receptor. 15 The only comparison between SUs and placebo which attempted to assess rates of cardiovascular disease was the University Group Diabetes Programme, published in 1970, which reported 26 cardiovascular deaths in participants randomized to tolbutamide vs. 10 allocated to placebo (n = 205). 16 However, this finding cannot be relied on as evidence in that significantly more patients allocated to tolbutamide had cardiovascular disease at baseline. 17 The UKPDS comparisons between individual intensive treatments and conventional treatment can be taken as partial reassurance on this account in that the confidence intervals (CIs) for relative risk of diabetes-related death (although wide) spanned unity for both chlorpropamide and glibenclamide vs. a mainly dietary approach. 18 Indeed, in ADOPT, there was a trend for fewer serious cardiovascular events with SUs than with metformin-although the trial was not powered to study cardiovascular endpoints.", "qa": [["30_6923067_1_1", "What are some adverse effects associated with the use of second-generation SUs for glucose lowering?\n", "Some adverse effects associated with the use of second-generation SUs for glucose lowering include weight gain and hypoglycemia. The risk of weight gain and hypoglycemia is dose-dependent, meaning that higher doses of SUs increase the risk. Severe hypoglycemia, requiring ambulance and/or hospital treatment, occurs in approximately one in every 100 people treated with a SU each year. "], ["30_6923067_1_2", "What is the mechanism of action of second-generation SUs for glucose lowering?\n", "Second-generation SUs, such as glibencamide, gliclazide, and glimepiride, work by opening potassium-ATP (K ATP ) channels. Some agents, like glibenclamide, bind to both b-cell (SUR1) and cardiac (SUR2A) subtypes of the SU receptor. However, evidence for the cardiovascular safety of second-generation SUs is limited, despite their mechanism of action involving cardiac receptors."], ["30_6923067_1_3", "How do second-generation SUs compare to other glucose-lowering agents in terms of cardiovascular safety?\n", "The evidence for the cardiovascular safety of second-generation SUs is surprisingly sparse. There have been limited studies comparing SUs to placebo or other glucose-lowering agents in terms of rates of cardiovascular disease. The UKPDS comparisons between individual intensive treatments and conventional treatment provide partial reassurance, as the confidence intervals for relative risk of diabetes-related death spanned unity for SUs compared to a mainly dietary approach. However, more research is needed to fully understand the cardiovascular safety of second-generation SUs."]]}]