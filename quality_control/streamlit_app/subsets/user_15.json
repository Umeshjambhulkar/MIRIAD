[{"passage_id": "68_406582_8", "passage": "70 In contrast, SNI potentiates the synaptic transmission between parabrachial nucleus-central nucleus of amygdala 71 and PFC, 69 leading to memory deficit and depressive behaviors. It has been proposed that the reduced excitatory synaptic transmission in both hippocampus-and PFC-NAcc pathways, leading to a dysfunction of corticomesolimbic reward circuitry that underlies many of the symptoms of depression. 72 Consistently, optogenetic activation of the PFC-NAcc pathway inhibits neuropathic pain and the affective symptoms produced by SNI. 73 Several lines of evidence show that proinflammatory cytokines, including IL-1b and TNFa, regulate synaptic strength also in a region-dependent manner. Both IL-1b 74 and TNF-a 75 are necessary for induction of LTP at C-fiber synapses in SDH. 76, 77 While the cytokines at pathological concentration inhibit LTP in hippocampus [78] [79] [80] and in frontal cortex. 70 Taken together, peripheral nerve injury and the resultant upregulation of IL-1b may lead to the neuropathic pain, memory deficit, and depression-like behavior via the region-dependent changes in synaptic strength. As neuropathic pain was dissociated with STMD and depression-like behavior in SNI and IL-1b injected rats, we proposed that the changes of synaptic connections in different regions may be variable in a given animals. Further studies are needed for elucidate the mechanisms underlying region-dependent regulation of synaptic strength induced by proinflammatory cytokines.\n\n The upregulation of IL-1b is a common cause for chronic pain, memory deficits, and depressive behavior in neuropathic conditions. Hence, IL-1b may be a target for prevention of neuropathic pain and the accompanied cognitive and emotional disorders.\n\n Authors' Contributions WSG, LJW, LJZ, and XGL conceived of the project, designed the experiments. WSG, XW, CLM, and LJZ carried out all experiments. WSG, XW, and MM analyzed the data and prepared the figures. LJZ and XGL supervised the overall experiment. MM, LJW, LJZ, and XGL revised the manuscript. All authors read and approved the final manuscript.\n\n The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.\n\n The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This work was supported by grants from the National Natural Science Foundation of China (U1201223 and 8137119), Guangdong Province University Outstanding Young Teachers' Training Program (S2013010011889), and from Natural Science Foundation of Guangdong Province, China (Yq2013008).", "qa": [["68_406582_8_1", "How does the upregulation of IL-1b contribute to chronic pain, memory deficits, and depressive behavior in neuropathic conditions?\n", "The upregulation of IL-1b, a proinflammatory cytokine, has been found to be a common cause for chronic pain, memory deficits, and depressive behavior in neuropathic conditions. It is believed that the increased levels of IL-1b lead to changes in synaptic strength in different regions of the brain, such as the hippocampus and frontal cortex, which are involved in the regulation of pain, memory, and mood. These changes in synaptic connections may contribute to the development of neuropathic pain and the accompanying cognitive and emotional disorders."], ["68_406582_8_2", "What is the role of the PFC-NAcc pathway in inhibiting neuropathic pain and affective symptoms?\n", "The PFC-NAcc pathway, which connects the prefrontal cortex (PFC) and the nucleus accumbens (NAcc), has been found to play a role in inhibiting neuropathic pain and affective symptoms. Optogenetic activation of this pathway has been shown to reduce neuropathic pain and the associated emotional symptoms in animal models. This suggests that the PFC-NAcc pathway may be a potential target for the treatment of neuropathic pain and the accompanying affective disorders."], ["68_406582_8_3", "How do proinflammatory cytokines, such as IL-1b and TNF-a, regulate synaptic strength in a region-dependent manner?\n", "Proinflammatory cytokines, including IL-1b and TNF-a, have been found to regulate synaptic strength in a region-dependent manner. For example, IL-1b and TNF-a have been shown to be necessary for the induction of long-term potentiation (LTP) at C-fiber synapses in the spinal dorsal horn (SDH). However, at pathological concentrations, these cytokines can inhibit LTP in the hippocampus and frontal cortex. This suggests that the effects of proinflammatory cytokines on synaptic strength can vary depending on the specific brain region. Further research is needed to fully understand the mechanisms underlying this region-dependent regulation of synaptic strength by proinflammatory cytokines."]]}, {"passage_id": "67_6965586_2", "passage": "1663.51 \u00b1326.83 pg/ml, respectively, P <0.001) and CD (2146.91 \u00b1470.39 pg/ml vs. 1674.55\n\n The Mann-Whitney U test was then used where applicable. Associations between the variables with normal distribution were assessed using the Pearson correlation coefficient, while those between the variables without normal distribution were assessed using the Spearman's rank correlation coefficient. All statistical analyses were conducted using the Statistica 8.0 software (StatSoft Inc., Tulsa, Oklahoma, United States). A P value less than 0.05 was considered statistically significant.\n\n results The study was conducted on 105 patients with IBDs: 50 subjects with CD and 55 with UC, and in controls. The characterisitics of the groups are presented in tAble 1.\n\n Patients with CD were characterized by a lower mean age compared with those with UC (P = 0.008) and controls (P = 0.03). No significant age difference was observed between patients with UC and controls.\n\n In the majority of patients with CD (66%), disease-associated lesions were located both in the small intestine and in the colon. Such complications as enterocutaneous and enteroenteric fistulas and abscesses were present in 62% of patients with exacerbated CD, while subjects in remission showed no active fistulas or abscesses. The majority of patients (60%) did not undergo any CD-associated surgical procedures. In 52% of patients with UC, disease-associated lesions extended to the splenic flexure (L1), while in 35% of the patients, the lesions involved the colon, Abbreviations: BMI -body mass index, CD -Crohn's disease, CRP -C-reactive protein, SD -standard deviation, sTNFR1 and sTNFR2 -soluble tumor necrosis factor membrane receptors 1 and 2, TNF-\u03b1 -tumor necrosis factor-\u03b1, UC -ulcerative colitis, WBC -white blood cells dIscussIon To our knowledge, there is a limited number of studies on the correlations of TNF-\u03b1 with sTNFR1 and sTNFR2. A positive correlation between serum concentrations of those markers was reported in patients with impaired glucose tolerance and diabetes, 25,26 but we have not found any data concerning correlations between those markers in IBDs.\n\n In the present study, sTNFR1 and sTNFR2 levels were higher in patients with CD and UC compared with controls. TNF-\u03b1 levels were also higher in patients with CD and UC, but a significant difference was observed only between patients with CD and controls.\n\n Other investigators also demonstrated the higher values of sTNFR1 and sTNFR2 in patients with CD and UC compared with controls.\n\n Hadziselimovic et al. 10 showed a correlation between urinary sTNFR1 and sTNFR2 concentrations and the activity of CD and UC as well as therapeutic effects in these diseases. 10 Higher urinary sTNFR1/2 levels were observed in patients with active CD and UC compared with subjects in remission, which was correlated with the CDAI and CAI. the levels of sTNFR1 and sTNFR2 were higher in patients with active CD compared with those with nonactive disease and controls. However, in patients in remission, sTNFR1 and sTNFR2 levels were comparable to those in controls. Similar results were reported by Spoettl et al. 9 and Hudson et al. 10 In contrast, Noguchi et al. 27 performed \u00b1319.35 pg/ml, respectively, P <0.001) compared with the subgroups in remission. There were no differences in TNF-\u03b1, sTNFR1, and sTNFR2 levels depending on disease location and duration\u00b8 smoking status, development of exacerbated or recurrent disease in the follow-up period, and the type of therapy either in CD or UC.\n\n Positive correlations were demonstrated between disease activity, expressed by the CDAI and CAI scores, and sTNFR1 and sTNFR2. The correlation coefficients for both receptors were higher in UC compared with CD. For TNF-\u03b1, a positive correlation with disease activity was noted only in CD (tAbles 3-5, FIGure) .\n\n We also assessed correlations between routine inflammatory markers and disease activity. In the CD group, we found statistically significant correlations with platelet count (r = 0.45), CRP (r = 0.69) and fibrinogen (r = 0.44).", "qa": [["67_6965586_2_1", "What are the potential complications associated with Crohn's disease?\n", "In the majority of patients with Crohn's disease (CD), complications such as enterocutaneous and enteroenteric fistulas and abscesses can occur. These complications were present in 62% of patients with exacerbated CD, while subjects in remission showed no active fistulas or abscesses."], ["67_6965586_2_2", "Are there any correlations between TNF-\u03b1 and soluble tumor necrosis factor membrane receptors 1 and 2 (sTNFR1 and sTNFR2) in inflammatory bowel diseases (IBDs)?\n", "There is limited data on the correlations between TNF-\u03b1 and sTNFR1 and sTNFR2 in IBDs. However, in the present study, sTNFR1 and sTNFR2 levels were found to be higher in patients with CD and ulcerative colitis (UC) compared to controls. TNF-\u03b1 levels were also higher in patients with CD and UC, but a significant difference was observed only between patients with CD and controls."], ["67_6965586_2_3", "What routine inflammatory markers are correlated with disease activity in patients with CD?\n", "In patients with CD, statistically significant correlations were found between disease activity and platelet count, C-reactive protein (CRP), and fibrinogen levels. These markers showed positive correlations with disease activity, indicating their potential as indicators of disease severity."]]}, {"passage_id": "74_5985777_1", "passage": "As an induction agent, it produces a profound depletion of lymphocytes and is associated with more frequent and severe adverse effects, such as neutropenia, thrombocytopenia, thyroid disease, autoimmune hemolytic anemia and other autoimmune diseases [16] [17] [18] . It is hoped that alemtuzumab induction could permit patients to be maintained on unconventional strategy with less intensive immunosuppression, such as tacrolimus monotherapy [19] , steroid-free [20] , steroid and calcineurin inhibitor (CNI) free regimen [21] .\n\n Rituximab is a chimeric monoclonal Ab against CD20, which is expressed on the majority of B cells. It was first approved in 1997 for refractory B cell lymphomas and it is increasingly applied for autoimmune diseases. In the realm of kidney transplant, rituximab has been used in combination with plasmapheresis and IVIG to treat antibody-mediated rejection (AMR), and to desensitize patients with preformed antibodies for ABO-and/or HLAincompatible kidney transplant [22, 23] .\n\n Induction Therapy\n\n Antibody selection should be guided by a comprehensive assessment of immunologic risk, patient comorbidities, financial burden, and the maintenance immunosuppressive regimen. Clinical trials comparing different antibody induction in various patient populations and with different maintenance immunosuppression are recently reviewed by the author [2] . The published data remain in line with the 2009 KDIGO guideline [24] . Lymphocytedepleting antibody is recommended for those with high immunologic risk as outlined in the 2009 KDIGO clinical practice guidelines (sensitized patient, presence of donor specific antibody, ABO incompatibility, high HLA mismatches, DGF, cold ischemia time >24 hours, African-American ethnicity, younger recipient age, older donor age), though it increases the risk of infection and malignancy [24] . For low or moderate risk patients, IL-2R Ab induction reduces the incidence of acute rejection and graft loss without much adverse effects, making its balance favorable in these patients [25] [26] [27] . IL-2R Ab induction should also be used in the high risk patients with other comorbidities (history of malignancy, viral infection with HIV, HBV or HCV, hematological disorder of leucopenia or thrombocytopenia and elderly) that may preclude usage of lymphocyte-depleting antibody safely [28] [29] [30] . Many patients with very low risk (nonsensitized, Caucasian, Asian, well HLA matched, living related donor transplant) may be induced with intrave-nous steroids without using any antibody, as long as combined potent immunosuppressives are kept as maintenance. In these patients, benefits with antibody induction may be too small to outweigh its adverse effects and the financial cost [2, 24, 31] . Clinical comparison trials have not demonstrated any graft or patient survival benefit of using T-cell depleting Ab induction in patients with low immunological risk [2, 24] . Rituximab induction is useful in desensitization protocols for ABO and/or HLA incompatible transplants. Alemtuzumab induction might be more successful for adopting less intensive maintenance protocols. However, the long-term safety and efficacy of unconventional strategy remain to be determined.\n\n \n\n Glucocorticoids have been used for preventing and treating graft rejection since the early 1960s. They have multiple actions. In addition to the nonspecific anti-inflamematory actions, glucocorticoids have critical immunosuppressive effect by blocking T-cell and antigen-presenting cell (APC) derived cytokine expression. Glucocorticoids bind to cytoplasmic receptor to form a complex, which translocates into the nucleus and binds to glucocorticoid response elements (GRE) in the promoter regions of cytokine genes. Glucocorticoids also inhibit the translocation of transcription factor AP-1 and NF-\u03baB into the nucleus. Therefore, production of several cytokines (IL-1, 2, 3, 6, TNF-\u03b1, gamma-interferon) are inhibited [32, 33] . Large dose of glucocorticoids can be given in the perioperative period as induction therapy (methylprednisolone 250 to 500 mg IV), which is usually followed by oral prednisone 30 to 60 mg/day. The dose is tapered over 1 to 3 months to a typical maintenance dose of 5 to 10 mg/day.", "qa": [["74_5985777_1_1", "How do different types of antibody induction therapies in kidney transplant patients vary based on immunologic risk factors and comorbidities?\n", "The selection of antibody induction therapies in kidney transplant patients is influenced by factors such as immunologic risk, patient comorbidities, financial considerations, and the planned maintenance immunosuppressive regimen. High-risk patients, as defined by criteria like sensitization, donor-specific antibodies, ABO incompatibility, and others, are recommended lymphocyte-depleting antibodies despite the increased risk of infections and malignancies. In contrast, low or moderate-risk patients may benefit from IL-2R antibody induction due to reduced rejection rates and graft loss without significant adverse effects. Patients with very low risk profiles, such as nonsensitized individuals with well-matched donors, may not require antibody induction and can be managed with intravenous steroids alongside potent maintenance immunosuppressives."], ["74_5985777_1_2", "What are the mechanisms of action of glucocorticoids in preventing graft rejection in kidney transplant patients?\n", "Glucocorticoids have been utilized for graft rejection prevention and treatment since the 1960s due to their diverse actions. Apart from their general anti-inflammatory properties, glucocorticoids exert critical immunosuppressive effects by inhibiting T-cell and antigen-presenting cell (APC) derived cytokine expression. Upon binding to cytoplasmic receptors, glucocorticoids form complexes that translocate into the nucleus and bind to glucocorticoid response elements (GRE) in cytokine gene promoters. Additionally, glucocorticoids impede the nuclear translocation of transcription factors like AP-1 and NF-\u03baB, leading to the inhibition of cytokine production, including IL-1, IL-2, IL-6, TNF-\u03b1, and gamma-interferon. In kidney transplant settings, glucocorticoids are typically administered in high doses perioperatively as induction therapy, followed by a gradual tapering to a maintenance dose over several months."], ["74_5985777_1_3", "How do rituximab and alemtuzumab differ in their mechanisms of action and clinical applications in kidney transplant patients?\n", "Rituximab is a chimeric monoclonal antibody targeting CD20 on B cells, initially approved for refractory B cell lymphomas and increasingly used in autoimmune diseases. In kidney transplant, rituximab is employed in combination with plasmapheresis and IVIG for treating antibody-mediated rejection and desensitizing patients with preformed antibodies for ABO and/or HLA incompatible transplants. On the other hand, alemtuzumab acts as a lymphocyte-depleting induction agent associated with severe adverse effects like neutropenia, thrombocytopenia, and autoimmune diseases. The use of alemtuzumab induction in kidney transplant aims to enable less intensive immunosuppression strategies, such as tacrolimus monotherapy or steroid-free regimens, though the long-term safety and efficacy of such approaches remain to be fully understood."]]}, {"passage_id": "2_46447229_3", "passage": "To this effect hepcidin has been seen to bind, internalize and inactivate ferroportin 1 at duodenal mature enterocytes (51) . Intestinal iron absorption is thus blocked (Fig. 3) . Whatever the mechanism of action of hepcidin, its absence favors intestinal iron absorption and the release of iron stored in the reticuloendothelial system (RES). This is seen in all situations where this hepatic hormone is low (iron-deficient diet, bleeding, hypoxia, types I, II, III hemochromatosis, etc). On the contrary, increased hepcidin (inflammation, infection, exogenic iron overload, liver adenomatosis, etc.) (52) results in decreased intestinal iron absorption and iron retention in RES cells. During inflammation and infection hepatic hepcidin synthesis increases (52) , which translates into decreased intestinal iron absorption (53, 54) , iron retention within macrophages (55) , and anemia (45, 53, 56) .\n\n A number of mutations in the HAMP gene have been found in some patients with JH (57, 58) . A change G\u2192A in the sequence +14 at the 5'-untranslated end (5'-UTR) has been reported in a Portuguese family, which creates a new AUG sequence that inhibits the translation of normal hepcidin mRNA, and probably results in the formation of a new, abnormal, unstable and degradable peptide (59) . Other mutations reported include R56X, which creates a \"stop codon\", the deletion of guanine 93, 175G\u2192C (R59G), which precludes prohepcidin activation into hepcidin by convertases (60) , particularly by furin, and 212G\u2192A (G71D), which alters this peptide's structure and function (61) .\n\n In most patients with JH the disorder is linked to chromosome 1q (57) , but the gene involved has remained unknown until very recently. In 2004, Papanikolaou et al. (62) published the results of a thorough study of chromosome 1q, where they unveiled a locus of previously un- Hepcidin is a peptide expressed by gene HAMP in liver cells in response to infection and iron overload. Hemojuvelin, protein HFE, and transferrin receptor 2 (TfR2) also contribute to increase hepcidin production. This slows the passage of iron through enterocytes (intestinal absorption) and the release of iron from macrophages. In these cells iron stems from the degradation of phagocyted old red blood cells. Hepcidin has been suggested to exert these effects by internalizing ferroportin 1 (FP-1) within cells.\n\n known function, LOC148738, which was associated with JH. The gene involved was initially designated HFE2, and more recently HJV. In this gene, which was made up of four exons separated by three introns, they found numerous mutations, and one of them, G320V, was present in all patients of Greek, Canadian, and French descent with JH (62) (62) . The mechanism of action of hemojuvelin is unknown, but seems to be closely linked to that of hepcidin. It is known not to be a hepcidin receptor (62) , as it is not expressed in organs where hepcidin acts (intestine, spleen) (62) . When mutations exist in the HJV gene, urine hepcidin decreases (62) . In JH urine hepcidin is deeply reduced despite the fact that body iron is strongly elevated. Hemojuvelin is therefore thought to be a hepcidin-modulating protein, so that the former's decreased levels or inactivity results in the latter's reduced presence. Such decreases would be responsible for the increased intestinal iron absorption and iron overload found in patients with JH (62).\n\n Since Most of them were located in exons 3 and 4, particularly within the molecular region corresponding to the von Willebrand-like domain (66) , and many were determinant of transcription termination. These mutations included a deletion of 13 base-pairs (CGGGGCCCCGCCC), which may be expected to result in a nil phenotype. They found two mutations in another patient -220delG, which creates a transcription end signal at 113, and 806-807insA, which leads to molecule truncation at position 331 and the formation of a 310-aminoacid molecule.", "qa": [["2_46447229_3_1", "What is the role of hepcidin in iron absorption and retention?\n", "Hepcidin is a peptide expressed by the HAMP gene in liver cells in response to infection and iron overload. It binds to and inactivates ferroportin 1, a protein involved in the transport of iron. This blocks intestinal iron absorption and promotes the retention of iron in macrophages. In situations where hepcidin levels are low, such as iron-deficient diet or certain types of hemochromatosis, intestinal iron absorption is increased. Conversely, increased hepcidin levels, such as during inflammation or exogenic iron overload, result in decreased intestinal iron absorption and iron retention in macrophages."], ["2_46447229_3_2", "What are some mutations in the HAMP gene associated with Juvenile Hemochromatosis (JH)?\n", "Several mutations in the HAMP gene have been found in patients with JH. These include a change in the sequence +14 at the 5'-untranslated end (5'-UTR), which creates a new AUG sequence that inhibits the translation of normal hepcidin mRNA. Other mutations reported include R56X, which creates a \"stop codon\", the deletion of guanine 93, 175G\u2192C (R59G), which precludes prohepcidin activation into hepcidin, and 212G\u2192A (G71D), which alters the structure and function of the peptide. These mutations can lead to decreased levels or inactivity of hepcidin, resulting in increased intestinal iron absorption and iron overload in patients with JH."], ["2_46447229_3_3", "What is the role of hemojuvelin in the regulation of hepcidin?\n", "Hemojuvelin is a protein that contributes to the increase in hepcidin production. It is closely linked to the mechanism of action of hepcidin but is not a hepcidin receptor. When mutations exist in the HJV gene, urine hepcidin decreases. Hemojuvelin is thought to be a hepcidin-modulating protein, and its decreased levels or inactivity result in reduced hepcidin presence. This decrease in hepcidin levels is responsible for the increased intestinal iron absorption and iron overload found in patients with Juvenile Hemochromatosis (JH)."]]}, {"passage_id": "0_1332430_2", "passage": "[3] [4] [5] Currently, substantiated indications for iNO include the treatment of hypoxic respiratory failure of the newborn (PPHN), 6 -9 and the assessment of pulmonary vascular reactivity in patients with pulmonary hypertension. 10 To date, the U.S. Food and Drug Administration has approved nitric oxide only for the treatment of term and near-term (more than 34 weeks of gestational age) neonates with hypoxic respiratory failure associated with pulmonary hypertension. Inhaled NO clearly is effective for this indication and reduces the severity of subsequent lung disease and the necessity for extracorporeal membrane oxygenation in these infants. Off-label clinical use is widespread, and includes using inhaled NO to treat acute respiratory distress syndrome (ARDS); complications of lung and cardiac transplantation; pulmonary hypertension associated with congenital and acquired heart disease, as well as chronic pulmonary diseases; and to produce desirable direct effects on blood elements, specifically during the treatment of acute chest syndrome in sickle cell disease. 11 Lowson describes several alternatives to inhaled NO, and focuses his review on inhaled prostacyclin (PGI 2 ). Why do we need additional drugs if we have nitric oxide? Expense is only one criterion for drug selection. Efficacy, safety, availability, and ease-of-use are other important considerations.\n\n Efficacy of inhaled NO for its off-label uses has been difficult to demonstrate. Placebo-controlled trials of iNO to treat ARDS have been disappointing, demonstrating only transient improvements in oxygenation and no effect on outcome. 12, 13 While in many patients inhaled NO provides selective pulmonary vasodilation, large multicenter trials examining the effect of inhaled NO therapy on clinical course and outcome of patients with diverse causes of pulmonary hypertension have not been performed.\n\n Physiologically, it seems reasonable that a selective pulmonary vasodilator might be effective in treating ARDS. Reduced pulmonary capillary pressure should decrease the extent of pulmonary edema; should improve lung compliance; and might speed resolution of lung injury. Improved oxygenation should permit a reduction of the inspired oxygen concentration and airway pressure. But these effects may be insufficient to alter outcome. Usually, pulmonary artery pressure is only modestly elevated in ARDS. Even in severe cases, the mean pulmonary artery pressure is usually about 30 mmHg. 14 This degree of pulmonary hypertension is well tolerated, and few patients with ARDS die of their pulmonary hypertension. Rather, the survival of patients with ARDS appears to depend more on the occurrence of sepsis and multiple organ failure than on blood gas tensions or pulmonary artery pressure. [15] [16] [17] This Editorial View accompanies the following article: Lowson SM: Inhaled alternatives to nitric oxide. ANESTHESIOLOGY 2002; 96:1504 -13.\n\n The effect of iNO varies among patients. Approximately one-third of patients fail to demonstrate improved oxygenation or decreased pulmonary artery pressure. 12, 18 The cause of hyporesponsiveness remains under investigation. We cannot predict which patients may benefit and why pulmonary vasodilation does not occur in others.\n\n Consequently, the search for ways of improving the efficacy of iNO and designing effective alternative therapies continues. Combinations of therapies have been developed that aim to improve the matching of ventilation-to-perfusion or increase the biologic activity of inhaled NO. Alternative therapies have been suggested that may provide equivalent pulmonary vasodilation. While such therapies are attractive, whether they will affect clinical outcome is unknown.\n\n Ventilatory techniques that increase alveolar recruitment, such as the use of high-frequency oscillation in neonates, 7 or prone positioning of ARDS patients, 19 may improve the response to inhaled NO. Recruiting lung volume, by adding PEEP 20 or by the use of partial liquid ventilation with perfluorocarbons, 21 has been used to augment the response to iNO. The coadministration of vasoconstrictors, such as almitrine and norepinephrine, may enhance pulmonary vasoconstriction and accentuate the improvement in PaO 2 observed during inhaled NO therapy, presumably by improving the matching of ventilation to perfusion. 22, 23 Inhibition of the phosphodiesterase (PDE) enzymes that hydrolyze cGMP can also increase the efficacy and duration of action of iNO. 24, 25 Even if efficacy were improved, however, iNO therapy still has several drawbacks. It is expensive, cumbersome devices are necessary to administer the drug safely, and continuous administration is required. Especially for chronic treatment of pulmonary hypertension, therapies that are inexpensive, available in convenient forms (such as a tablet or simple multidose inhaler), and allow for intermittent dosing would be advantageous.\n\n Does inhaled prostacyclin fulfill these goals?", "qa": [["0_1332430_2_1", "What are some off-label uses of inhaled nitric oxide (iNO) in clinical practice?\n", "Off-label uses of iNO include treating acute respiratory distress syndrome (ARDS), complications of lung and cardiac transplantation, pulmonary hypertension associated with congenital and acquired heart disease, chronic pulmonary diseases, and acute chest syndrome in sickle cell disease."], ["0_1332430_2_2", "What are some factors that contribute to the difficulty in demonstrating the efficacy of iNO for its off-label uses?\n", "Placebo-controlled trials of iNO for off-label uses have been disappointing, showing only transient improvements in oxygenation and no effect on outcome. Additionally, large multicenter trials examining the effect of iNO therapy on clinical course and outcome of patients with diverse causes of pulmonary hypertension have not been performed."], ["0_1332430_2_3", "What are some alternative therapies or techniques that have been suggested to improve the efficacy of iNO?\n", "Some alternative therapies or techniques that have been suggested to improve the efficacy of iNO include ventilatory techniques that increase alveolar recruitment, such as high-frequency oscillation in neonates or prone positioning of ARDS patients, recruiting lung volume by adding positive end-expiratory pressure (PEEP) or using partial liquid ventilation with perfluorocarbons, coadministration of vasoconstrictors to improve the matching of ventilation to perfusion, and inhibition of phosphodiesterase (PDE) enzymes to increase the efficacy and duration of action of iNO."]]}, {"passage_id": "5_7459224_0", "passage": "The Center for Disease Control and Prevention has defined nosocomial infection (NI) as a localized or systemic condition resulting from an adverse reaction to the presence of an infectious agent(s) or its toxins, without any evidence that the infection was present or incubating at the time of admission to the Intensive Care Unit (ICU). [1] NIs are more common in patients admitted in the surgical ICU because of immobility, surgical incisions, multiple invasive monitoring lines, urinary catheters, and mechanical ventilation. In Cardiac Surgical Intensive Care Unit (CSICU), in addition to the aforementioned issues, there are intercostal drainage tubes, long duration surgeries, hypothermia, poor nutrition, cardiac cachexia, and patients undergoing open chest management which increases the susceptibility of these patients for infections. NIs in the postoperative period not only increase morbidity and mortality, but also impose a significant economic burden on the health care infrastructure. [2] The other factors which can prolong the stay in CSICU, and predispose the patients for NI are low cardiac output syndrome, acute kidney injury, need for increased inotropic support, need for renal replacement therapy, coagulopathies, neurological injury, and cardiopulmonary bypass (CPB) induced systemic inflammatory response. On the basis of perioperative risk factors many models have been formulated to predict the occurrence of NI after cardiac surgery. [3] The data on the incidence of NI among the CSICUs and the organism profiles together with the antibiotic resistance details in developing setups is scanty, more so from surgical ICUs. Such information is useful for medical audit as well as planning for preventive/ corrective measures. We analyzed the incidence, risk factors, common sites, the pathogens responsible for NI and their antibiotic susceptibility profile in our cohort of cardiac surgical patients in a large CSICU.\n\n \n\n This retrospective study was conducted in the CSICU of Cardiothoracic Vascular Surgery Department of the All India Institute of Medical Sciences, New Delhi. The CSICU has 42 beds including 8 for neonates.\n\n All (neonates, infants, pediatric, and adult) patients admitted to the CSICU after elective cardiovascular surgery from January 2013 to December 2014 were studied. Detailed information was obtained from the records of all patients who developed microbiologically documented NI. In addition for comparative purposes, data on admitted patients who did not develop NI was collected for a 2-month period from November to December 2014. This was done mainly due to the consideration that data on patients admitted during a 2-month period in a high output center like ours would give enough statistical power for any comparison between infected and non-infected groups. Demographic data, intra-and post-operative information, time, and source and number of samples sent for microbiological investigations were collected. Patients with preoperative infection, on ventilator, on antibiotics before surgery, emergency operations, heart transplant recipients, and patients received on extracorporeal membrane oxygenator were excluded. Patients were categorized into valvular (valve repair/replacements), nonvalvular (coronary artery bypass grafting, aortic aneurysms, and dissections), cyanotic (all right to left shunts and univentricular hearts), acyanotic (all left to right shunts), and miscellaneous (all closed heart surgeries including patent ductus arteriosus ligation, coarctation of aorta repair, pericardiectomy, etc.).\n\n NIs: Infections developed within 48 h of shifting the patients from ICU to the wards or patients who were readmitted to the ICU for infection before their hospital discharge. All infections were diagnosed on the basis of clinical suspiciousness, radiological information and were confirmed by biochemical criteria like positive cultures from different body secretions. Major infections such as lower respiratory infections (LRTIs), wound/surgical-site infections (SSIs), blood stream infections (BSIs), and urinary tract infections (UTIs) were defined as follows -LRTIs-infections involving lower respiratory tract including bronchitis, pneumonitis, and pneumonia; SSI-nonhealing wound with or without discharge involving the sternal wound and/or leg or arm (sites of graft harvesting) and culture proven pathogens from the discharge; BSI-culture proven pathogens in blood, sampled from one central, and one peripheral site; UTI -more than 10 5 colony forming bacterial units on culture.\n\n All biological samples were collected and sent to the microbiology laboratory as per the standard procedures. The data on the positive culture reports from different sites, the microorganisms isolated and their susceptibility to commonly used antibiotics were collected from the microbiology reports.\n\n All patients in the postoperative CSICU were treated with antibiotics as per the standard protocol of the unit.", "qa": [["5_7459224_0_1", "What are some risk factors that increase the susceptibility of patients in the Cardiac Surgical Intensive Care Unit (CSICU) to nosocomial infections?\n", "Risk factors that increase the susceptibility of patients in the CSICU to nosocomial infections include immobility, surgical incisions, multiple invasive monitoring lines, urinary catheters, mechanical ventilation, intercostal drainage tubes, long duration surgeries, hypothermia, poor nutrition, cardiac cachexia, and open chest management. Other factors such as low cardiac output syndrome, acute kidney injury, increased inotropic support, renal replacement therapy, coagulopathies, neurological injury, and cardiopulmonary bypass (CPB) induced systemic inflammatory response can also prolong the stay in the CSICU and predispose patients to nosocomial infections."], ["5_7459224_0_2", "What are the common sites of nosocomial infections in the postoperative period in the Cardiac Surgical Intensive Care Unit (CSICU)?\n", "The common sites of nosocomial infections in the postoperative period in the CSICU include lower respiratory tract infections (LRTIs), wound/surgical-site infections (SSIs), bloodstream infections (BSIs), and urinary tract infections (UTIs). LRTIs involve the lower respiratory tract, including bronchitis, pneumonitis, and pneumonia. SSIs are nonhealing wounds with or without discharge, involving the sternal wound and/or sites of graft harvesting, with culture-proven pathogens from the discharge. BSIs are diagnosed when culture-proven pathogens are found in blood samples taken from one central and one peripheral site. UTIs are defined as having more than 10^5 colony-forming bacterial units on culture."], ["5_7459224_0_3", "What measures can be taken to prevent nosocomial infections in the Cardiac Surgical Intensive Care Unit (CSICU)?\n", "To prevent nosocomial infections in the CSICU, measures can be taken such as promoting mobility and early ambulation of patients, proper surgical site care and wound management, strict adherence to infection control practices including hand hygiene, proper insertion and maintenance of invasive monitoring lines and urinary catheters, appropriate use of antibiotics, regular monitoring and management of cardiac output, kidney function, and coagulation parameters, and minimizing the duration of cardiopulmonary bypass. Additionally, maintaining optimal nutrition, preventing hypothermia, and implementing strategies to reduce systemic inflammatory response during open chest management can also help prevent nosocomial infections in the CSICU."]]}, {"passage_id": "58_214806978_3", "passage": "If not, excreta must be disinfected in accordance with the Regulation of Disinfection Technique in Healthcare Settings [26] .\n\n (1) Introduce patients to the ward environment and isolation regulations upon admission.\n\n (2) Regularly assess patients' psychological condition and consult with the psychiatry department if necessary.\n\n (3) During bedside operations, provide patients with appropriate emotional support through gestures such as eye contact, touch, nods, handshakes, and thumbs-up gestures, thereby helping them to build confidence in their ability to overcome the disease [27] .\n\n (4) Help to communicate information to external relatives, provide patients with continuous information support, and encourage them to actively collaborate in their treatment.\n\n (1) Principle: Collaborate with the clinical medical team to assess the feasibility of commencing respiratory rehabilitation program for patients. Respiratory rehabilitation activities should not reduce the patient's blood oxygen saturation and blood pressure [28] .\n\n (2) Instructional format: Videos, brochures, etc.\n\n (3) Rehabilitation contents: Routine changes in body posturing (e.g., semi-recumbent position, lateral position, prone position, etc.) to reduce the work of breathing muscles and save energy; respiratory control techniques (e.g., abdominal breathing and pursed-lip breathing) to expand the lower chest and relieve breathing difficulties; effective sputum excretion techniques (e.g., postural drainage and effective coughing) to promote sputum excretion and maintain an unobstructed airway.\n\n (1) To ensure adequate sleep, instruct patients to rest in bed. Those with sleep disorders should be managed by drugs as per medical orders.\n\n (2) Patients who can get out of the bed should follow the \"Three Steps to Get Up\" guideline (lie in the bed for 30 seconds before getting up, drop both legs for 30 seconds before standing up, and stand for 30 seconds before walking). Instruct patients to perform activities such as sitting, standing, and stepping along the bedside once they are out of bed [29] .\n\n (3) Prevent patients from falling out of bed. Instruct patients to perform muscle training such as clenching their fists, raising their arms, ankle pumping, heel slipping, lifting their legs, and contracting quadriceps and gluteal muscles, according to their tolerance levels. Compression stockings can be utilized to prevent the formation of deep vein thrombosis in lower limbs.\n\n (4) Avoid excessive activities to reduce patients' blood oxygen saturation and blood pressure.\n\n (1) Instruct patients to perform respiratory rehabilitation activities according to their rehabilitation program.\n\n (2) Ensure timely transmission of patients' medical information and instruct them to go to the designated place for centralized isolation of 14 days [30] .\n\n (3) Inform patients of follow-up instructions for weeks 2 and 4 and carefully perform follow-up and review.\n\n (4) Instruct patients to work and rest regularly and ensure they maintain adequate sleep, a balanced diet, and a calm emotional state.\n\n (5) Instruct patients to be vigilant of hand hygiene, maintain distance from family members, wear a mask, have separate meals, and avoid sharing personal items such as tableware and washing materials during home isolation [31] .", "qa": [["58_214806978_3_1", "What are some measures that can be taken to support patients psychologically during their treatment?\n", "Some measures that can be taken to support patients psychologically during their treatment include regularly assessing their psychological condition and consulting with the psychiatry department if necessary. Additionally, providing patients with appropriate emotional support through gestures such as eye contact, touch, nods, handshakes, and thumbs-up gestures can help them build confidence in their ability to overcome the disease."], ["58_214806978_3_2", "What are some components of a respiratory rehabilitation program for patients?\n", "Components of a respiratory rehabilitation program for patients may include routine changes in body posturing to reduce the work of breathing muscles and save energy, respiratory control techniques to expand the lower chest and relieve breathing difficulties, and effective sputum excretion techniques to promote sputum excretion and maintain an unobstructed airway."], ["58_214806978_3_3", "What instructions should be given to patients during their home isolation period?\n", "During the home isolation period, patients should be instructed to be vigilant about hand hygiene, maintain distance from family members, wear a mask, have separate meals, and avoid sharing personal items such as tableware and washing materials. They should also be instructed to work and rest regularly, maintain adequate sleep, follow a balanced diet, and ensure a calm emotional state. Additionally, patients should be informed of follow-up instructions for weeks 2 and 4 and should carefully undergo follow-up and review."]]}, {"passage_id": "0_116724461_2", "passage": "analyzed more than 100 serum samples from controls and liver cancer patients and identified tryptophan, glutamine, and 2-hydroxybutyric acid as early markers for liver cancer [29] .\n\n Cigarette smoking is the major risk factor for lung cancer, although additional factors have been identified [30] . Chronic obstructive pulmonary disease (COPD) may increase the risk of lung cancer. Metabolomic approaches were applied to distinguishing COPD and lung cancer in serum samples collected from patients with one of these diseases (TNM stages I, II, III, and IV). Higher levels of acetate, citrate, and methanol were found in individuals with COPD compared to those with lung cancer; and N-glycosylated proteins, leucine, lysine, mannose, and choline levels were higher in those with lung cancer [30] .\n\n Blood samples from patients with pancreatic cancer and cachexia were characterized by metabolomic approaches to identify contributing metabolites [31] . Two groups, one with and another without cachexia, were followed longitudinally. Serum levels of IL-6, tumor necrosis factor (TNF)-alpha, and leptons as well as loss of body weight were determined in both groups using GC-MS. Compared to the cachexia-free group, levels of these markers varied day to day and were higher in cachexia patients. Most patients with advanced stage pancreatic cancer develop cachexia with symptoms such as decreased dietary intake, anxiety, and depression.\n\n The incidence and prevalence of prostate cancer is very high in the United States and worldwide. Although prostate-specific antigen (PSA) is used for the diagnosis and prognosis of this cancer, the sensitivity and specificity of this antigen is low. It is difficult to make a clinical decision for treatment if PSA levels are lower than 0.4 microgram per mL. Metabolomic biomarkers with potential for use in diagnosing prostate cancer include sarcosine, proline, kynurenine, uracil, and glycerol-3-phosphate in urine [32] . These metabolites can be measured in longitudinally collected urine samples by LC-MS [33] .\n\n Metabolomics currently involves a variety of challenges. For example, metabolomics data generally are complex and it has been observed that the data matrix frequently contains missing values, making quantitative analysis difficult. Peaks that are present in the chromatogram can be missed by investigators during peak picking. Zhan et al. proposed using the Kernel-based scoring approach to address missing data [24] . The Kernel method is available in the R statistical computing environment.\n\n Metabolomics requires the development of sophisticated and powerful statistical methodologies to make clinical observations easy to follow. These statistical approaches enable comparison of the abundance levels of a metabolite between cases and controls to assess their significance. Metabolomic association networks that use high-dimensional MS data have the potential to improve data analysis and use by investigators.\n\n Although the analytical platforms for metabolomic analysis are robust, sample pretreatment procedures differ among institutions such as clinics and hospitals. This contributes to systematically biased results. For example, when tumor tissues are collected, normal cells may be included in these samples and confound the analysis. Laser microdissection is a potential solution, but the procedure is highly technical, time-consuming, and expensive for epidemiologic studies. Fasting status shows different metabolomic patterns in serum versus urine collected from the same patient. Techniques that show minimum variability of metabolites in samples from patients under fasting or non-fasting conditions are needed. Intra-individual variations have been reported for nutritional status (food frequency) and physical exercise [34] .\n\n In epidemiologic studies that seek to identify race-and ethnicity-specific metabolites, cohorts containing multiple ethnic groups are needed so that findings can be generalized to all populations. Additional challenges include: variability in metabolomic measurements and related implications; the need to create new techniques for analysis, computation, and interpretation; epidemiologists have limited training in metabolomics; the need to integrate metabolomic data with genomic, epigenomic, transcriptomic, and proteomic data; and the availability of highly purified standards.\n\n Opportunities include the fact that biofluids such as urine and blood can be collected noninvasively and are suitable for both epidemiologic and clinical studies. Applications of metabolomics in drug-resistant breast cancer cells were described recently [35] , and similar approaches can be investigated in other cancers.\n\n Understanding the metabolic basis of cancer has the potential to provide the foundation for the development of novel approaches targeting tumor metabolism. Tumors characterized by aerobic glycolysis and/or glucose dependence could be more sensitive than other tumors to agents targeting the tumor vasculature and glucose transport. Tumors characterized by impaired TCA cycle function and/or respiration that is glutamine-dependent could be sensitive to agents targeting glutamine metabolism (such as glutaminase). Tumors that have impaired mitochondrial/electron transport function could be sensitive to agents that target the reductive carboxylation and fatty acid synthesis pathways. Malignancies that are characterized by IDH1, IDH2, FH, or succinate dehydrogenase mutations could affect TET2 function, resulting in hypermethylation phenotypes [36] . Such malignancies could be responsive to hypomethylating agents.\n\n Metabolites are the end products of biological regulatory and metabolomic processes, and they can be measured in biological fluids and tissues. Their levels can be regarded as the response of biological systems to genetic, lifestyle, and environmental changes. Molecular profiling based on metabolomic analysis may facilitate the stratification of patients with cancer into homogeneous biological groups to facilitate the clinical management of these patients. Similar to other omics biomarkers, metabolomic biomarkers should have high sensitivity and specificity. Because metabolites are closely related to a patient's phenotype, their potential for use in disease diagnosis and prognosis is significant. Metabolomics has the potential to become a valuable tool for precision medicine.\n\n MV contributed in developing the outline and preparing the first draft of the manuscript. HNB provided comments and finalized the manuscript.\n\n The authors declare no conflicts of interest.", "qa": [["0_116724461_2_1", "What are some potential metabolomic biomarkers for diagnosing prostate cancer?\n", "Sarcosine, proline, kynurenine, uracil, and glycerol-3-phosphate are potential metabolomic biomarkers for diagnosing prostate cancer. These metabolites can be measured in urine samples using LC-MS."], ["0_116724461_2_2", "What are some challenges in metabolomics data analysis?\n", "Some challenges in metabolomics data analysis include complex data, missing values in the data matrix, and peaks being missed during peak picking. The use of statistical methodologies and metabolomic association networks can help address these challenges."], ["0_116724461_2_3", "How can metabolomics be used in the clinical management of cancer patients?\n", "Metabolomics can help stratify patients with cancer into homogeneous biological groups, which can facilitate their clinical management. Metabolomic biomarkers can be used for disease diagnosis and prognosis, and understanding the metabolic basis of cancer can lead to the development of novel approaches targeting tumor metabolism."]]}, {"passage_id": "50_8679367_5", "passage": "Control of non-CSCs without control of CSCs has been a common result of the familiar sequence: primary treatment, tumor reduction, improvement in symptoms, recurrence locally or in metastases, refractoriness to treatment, mortality.\n\n Treating patients successfully requires control of morbidity by control of the visible non-immortal bulk of tumor, and also control of the cancer stem cell population. Neither of these two tasks is sufficient, and they present different challenges and opportunities. This observation stimulates thinking about applying the WDR. One strategy that leverages the power of the WDR in the context of CSCs would be to administer the CSC-active agent until the short-term risk of unacceptable morbidity is high, and then provide standard chemotherapy to control morbidity. As an immediate consequence, in the adjuvant setting the CSC-active agent would be strongly preferred. Upon tumor recurrence, the choice becomes difficult. Morbidity risk increases, rapidly in some cases and slowly in others. It is plausible that even then the CSC-active agent could be a good choice. An alternative CSC-active agent, if available, could be a better choice; but standard chemotherapy effective on the primary, which has at best diminished efficacy on metastases, would be least favored. Conversely, in the neoadjuvant setting, since the main clinical purpose is to de-bulk the tumor prior to primary surgery, CSC-targeted therapy would be a low priority at best.\n\n The set of regimen choices originally studied by Goldie and Coldman expressed a constraint that the two hypothetical agents A and B could not be given in the same cycle due to toxicity limitations. Thus, for each cycle there was a choice, A or B. This artificial setting nevertheless receives some echo from the study of Schott et al, where the CSC-active agent and non-CSC-active agent were not concurrent, in order to minimize risk of toxicity. Going forward in future trials, it will be desirable to begin by deciding what toxicity constraints should apply when combining a stem-cell-active treatment with a standard treatment, listing a few feasible combination schedules acceptable by these constraints, and designing a trial to compare these schedules. Then the clinical trial can help to answer a biological question as well as a clinical question.\n\n Having found via the WDR that the \"obvious\" way to combine multiple anticancer agents and modalities is often exactly the worst way (Fig. 1) , a failure to design trials informed by tumor dynamics is risky; early clinical failures may kill an entire promising stream of treatment development.\n\n Modeling tumor dynamics with CSCs involves complications tied with aspects of the biology for which we have at best hints up to now. We do not understand if, or how, the CSCs and non-CSCs communicate. How do CSCs \"know\" when to divide asymmetrically and when symmetrically? Does a CSC respond to its immediate microenvironment, or systemic signals affecting all CSCs in concert?\n\n A simplifying assumption made in discussions of CSCs is that their differentiated descendants are indeed permanently not self-renewing. However, there is some evidence that a proliferative non-CSC cancer cell, supposedly with potential for only small numbers of cell divisions, might occasionally obtain the CSC phenotype. If so, then this can radically change the balance in comparing treatment strategies. There is evidence that hypoxic conditions, typically found in the interior of tumor primaries and advanced metastases, can induce cells to regain self-renewing capability.\n\n 14 Some evidence in favor of this possibility appears, for example, in neuroblastomas 52 and breast cancer. 53 This biological possibility complicates the adjuvant setting. If proliferative non-CSC cells can gain CSC-like selfrenewing capability, if the rate is not too low, this could argue against continued use of CSC-active agents. Recruitment into the CSC subpopulation could exceed the ability of the CSCactive agent to reduce the subpopulation.\n\n Another complication involves the recently discovered biology: there appear to be two major populations of CSCs. An intriguing set of observations have recently appeared connecting CSCs with transitions of cancer cell phenotype between epithelial and mesenchymal forms. 54, 55 The epithelialmesenchymal transition (EMT) is a phenomenon identified from embryology, giving rise to the loss of cell-to-cell contact and polarity. 56 Epithelial cells tend to be polar, nonmotile, and relatively sensitive to chemotherapy agents. They generally express E-cadherin. Mesenchymal cells are motile, and generally do not divide; they generally express N-cadherin. An association between CSCs and EMT is widely acknowledged. Experimental induction of EMT in mammary-derived immortalized cells by the embryogenesis-associated transcription factors Snail and Twist generates characteristics of stem cells, such as the CD44 high /CD24 low expression pattern.", "qa": [["50_8679367_5_1", "How does the presence of cancer stem cells (CSCs) impact treatment outcomes?\n", "The presence of cancer stem cells (CSCs) can have a significant impact on treatment outcomes. Treating patients successfully requires control of both the visible non-immortal bulk of tumor and the cancer stem cell population. Failure to control CSCs can lead to tumor recurrence, refractoriness to treatment, and increased mortality. Therefore, it is important to develop strategies that specifically target CSCs in order to improve treatment outcomes."], ["50_8679367_5_2", "What are the challenges and opportunities in controlling the cancer stem cell population?\n", "Controlling the cancer stem cell population presents different challenges and opportunities compared to controlling the visible non-immortal bulk of tumor. One challenge is that CSCs can be resistant to standard chemotherapy, leading to treatment refractoriness. However, targeting CSCs can also provide opportunities for improving treatment outcomes. By administering CSC-active agents in combination with standard chemotherapy, it may be possible to effectively control both the CSC population and the non-CSC tumor bulk, leading to better treatment outcomes."], ["50_8679367_5_3", "How does the biology of cancer stem cells (CSCs) complicate treatment strategies?\n", "The biology of cancer stem cells (CSCs) introduces several complications in treatment strategies. One complication is the potential communication between CSCs and non-CSCs, which is not well understood. Additionally, there is evidence that non-CSC cancer cells can occasionally obtain the CSC phenotype, which can change the balance in comparing treatment strategies. Furthermore, the discovery of two major populations of CSCs and their association with epithelial-mesenchymal transition (EMT) adds complexity to treatment approaches. Understanding these biological complexities is crucial for designing effective treatment strategies."]]}, {"passage_id": "70_27594907_2", "passage": "It is known that 18-62% of sternal fractures are accompanied with cardiac injury. 13 We think that one of the reasons of sudden deaths at early stage is cardiac contusion, as one of the patient died due to cardiac contusion in our series. Therefore, electrocardiography and cardiac enzyme panel was carried out in all the patients suspected to have sternal fracture. The most frequently observed intrathoracic pathologies in thoracic trauma are pneumothorax, hemothorax or both, and the first stage of treatment is tube thoracostomy. 14, 15 In our study, pneumothorax was diagnosed in 5.1% of patients, whereas hemothorax and hemopneumothorax were diagnosed in 3.1% and 3.3% respectively. All of these patients were subjected to tube thoracotomy. There are publications reporting that life-threatening complications such as tension pneumothorax could be prevented by this way. 15 On the other hand, Menger et al. 16 reported the ratio of tube complications as 20%.\n\n We advocate that chest tube should be inserted in all pneumothorax and hemopneumothorax cases by experienced physicians. In our study, 0.8% (n=9) of the patients underwent emergency thoracotomy. The indications were hemorrhagic shock, and hemorrhagic drainage greater than 1500 cc following tube insertion, or bleeding >100 cc/hr within 6-8 hours, or >200 cc/hr within 3-4 hours. The most important factors affecting mortality in thoracic trauma is coexisting injuries in other systems and organs. In a study including 3406 cases, Regel et al. 17 reported that thoracic traumas were most frequently accompanied by extremity fractures and this is followed by brain injuries. In similar studies, thoracic traumas have been reported to be accompanied most frequently by musculoskeletal injuries. 1, 10, 18 In our study, 25.6% of the patients had coexisting systemic injuries, in which the musculoskeletal injuries were the most frequently observed one. In thoracic trauma, physicians in emergency service should be alert for coexisting systemic injuries.\n\n In our study, 60.4% of the cases admitted to the emergency service were treated as outpatients, and 36.8% were hospitalized for treatment. About 1.7% of the patients were referred to a tertiary level centre because of coexisting systemic pathologies and deterioration in their general condition. Emircan et al. 19 studied factors affecting mortality of patients with thoracic trauma, and reported that Trauma Revised Score-Injury Severity Score has been the strongest factor in determining mortality and thus patients with thoracic trauma should be treated as a high risk group and diagnosis and treatment should be aggressive. Although our approach towards the patients with thoracic trauma was aggressive too, early stage mortality was observed in 1.1% of the patients. In some studies, mortality rates have been found to be increased in patients aged 45 and less, and 65 and over. Mortality rates have been found to be significantly higher in penetrating injuries and traffic related injuries to pedestrians when compared to other injuries. Akcam et al. 20 performed emergency bedside thoracotomy in 6 patients with penetrating injury and 3 patients with blunt thoracic trauma, and success was only achieved in 3 patients with penetrating injury. They concluded that emergency bedside thoracotomy is a life-saving procedure in the cases of penetrating thoracic injury. As we had no facility of emergency bedside thoracotomy in emergency services, thoracotomies were compulsorily performed in operating room. We lost minutes, which is important for the patients. Emergency thoracotomy was performed in 9 patients with penetrating injury. Of these, 4 were operated successfully. Unfortunately, 5 patients died due to major vascular and cardiac injuries during", "qa": [["70_27594907_2_1", "What are the most frequently observed intrathoracic pathologies in thoracic trauma?\n", "The most frequently observed intrathoracic pathologies in thoracic trauma are pneumothorax, hemothorax, or both. These conditions can be diagnosed through imaging techniques such as chest X-rays or CT scans. The first stage of treatment for these pathologies is tube thoracostomy, which involves inserting a chest tube to drain air or fluid from the pleural space."], ["70_27594907_2_2", "What are the indications for emergency thoracotomy in patients with thoracic trauma?\n", "Emergency thoracotomy may be indicated in patients with thoracic trauma in cases of hemorrhagic shock, hemorrhagic drainage greater than 1500 cc following tube insertion, or bleeding exceeding certain rates within specific time frames (e.g., >100 cc/hr within 6-8 hours, or >200 cc/hr within 3-4 hours). These indications suggest severe bleeding or hemodynamic instability that requires immediate surgical intervention."], ["70_27594907_2_3", "What factors affect mortality in patients with thoracic trauma?\n", "The most important factors affecting mortality in thoracic trauma are coexisting injuries in other systems and organs. In a study, it was found that thoracic traumas were most frequently accompanied by extremity fractures and brain injuries. Additionally, age can also impact mortality rates, with higher rates observed in patients aged 45 and younger, as well as those aged 65 and older. Mortality rates have also been found to be significantly higher in cases of penetrating injuries and traffic-related injuries to pedestrians compared to other types of injuries."]]}, {"passage_id": "0_36806545_0", "passage": "Aspergillosis is the most common type of invasive fungal infection worldwide, and the most widespread species observed in these infections is Aspergillus fumigatus [1, 2] . A. fumigatus strains cause life-threatening invasive aspergillosis in patients with haematologic tumors, cystic fibrosis patients and immune-compromised patients who have had an organ transplant [3] . Of the airborne fungi, A. fumigatus infection developed in a patient who was staying in the intensive care unit after undergoing liver transplantation. It was also observed that 2 other patients staying in the same unit developed A. fumigatus infection. After the molecular studies, it was determined that Aspergillus spores were firstly transmitted into the air while wound dressings were changed, and then indirectly to other humans as airborne spores [4] . It has been reported that Aspergillus infections have increased by 2-26% in haematopoietic stem cell patients, by 1-15% in organ transplant recipients and by 10-57% in patients with cystic fibrosis, and that the mortality rates in these patients have also increased [5, 6] . In order to reduce the high mortality rates in such patients, early diagnosis and immediate initiation of anti-fungal therapy are extremely important [5] . Species of Aspergillus fumigatus are microscopically characterized with blue-green conidia borne on uniseriate conidiophores with subclavate vesicles, and defined as phenotypic due to their macroscopic properties [7] . However, species of A. fumigatus may differ morphologically and, based on minor phenotypic differences, a variety of new species are defined within this group [5, 8] . Identification of a species only on the basis of morphological characteristics is highly controversial because morphology depends on the growth conditions, and the time required for the identification of anamorphic forms of Aspergillus species is 5 days or more [8] . On the other hand, some studies have indicated that the strains phenotypically identified as A. fumigatus may be genetically different [5, 8] . Recently, with multilocus sequence analysis, various slow-spore-forming A. fumigatus isolates have been identified as a new species within the section Fumigati [7] . Since the species within the genus Aspergillus have very different antibiotic sensitivities, their identification at the species level is extremely important [5, 9] . Therefore, for the diagnosis of A. fumigatus infections, accurate species identification, and classification of the species within the section Fumigati, molecular methods which contribute to obtaining both phenotypic and more sensitive and exact results should be used [2, 10] .\n\n Molecular methods widely used for the identification of fungi isolated from environmental and clinical specimens are PCR-based, and include the analysis of genes such as 18srRNA, mitochondrial DNA, rodlet A (rodA), \u03b2eta-tubulin (\u03b2tub), actin and calmodulin and internal transcribed spacer (ITS) regions [2, 5, 11, 12, 13] . Therefore, this study was aimed to polyphasically classify 20 strains which were isolated from the atmosphere of the inpatient departments where the patients who presented to the Ege University Medical Faculty Hospital Newborn Unit and environment were treated, and the strains identified as A. fumigatus through phenotypic methods by analyzing ITS regions and rodA and \u03b2tub genes of these strains.\n\n Bacterial strains. In this study, the 20 A. fumigatus strains which were isolated in different seasons from the atmosphere of the inpatient departments where the patients who presented to the Ege University Medical Faculty Newborn Unit and environment. The isolation and phenotypic identification of these strains were conducted through macroscopic and microscopic methods. The colony colour and properties of the strains were evaluated after 7 days of incubation on CYA25 (35), MEA (30), CY20S (30), CYA37 (57) and CZ agars, and their microscopic properties were investigated [14] .\n\n Anti-fungal susceptibility testing. Anti-fungal susceptibility tests of 19 A.", "qa": [["0_36806545_0_1", "What are the risk factors for developing invasive aspergillosis?\n", "Risk factors for developing invasive aspergillosis include having hematologic tumors, being a cystic fibrosis patient, and being immune-compromised, such as after an organ transplant."], ["0_36806545_0_2", "How can Aspergillus fumigatus infections be transmitted?\n", "Aspergillus fumigatus infections can be transmitted through the air as airborne spores. In the provided passage, it was observed that Aspergillus spores were transmitted into the air while wound dressings were changed and then indirectly to other humans as airborne spores."], ["0_36806545_0_3", "What are the molecular methods used for the diagnosis and identification of Aspergillus fumigatus infections?\n", "Molecular methods, such as PCR-based analysis of genes like 18srRNA, mitochondrial DNA, rodlet A (rodA), \u03b2eta-tubulin (\u03b2tub), actin, calmodulin, and internal transcribed spacer (ITS) regions, are widely used for the diagnosis and identification of Aspergillus fumigatus infections. These methods provide more sensitive and accurate results compared to phenotypic methods."]]}, {"passage_id": "63_4909729_0", "passage": "Advances in antiretroviral therapy (ART) have resulted in greatly reduced mortality among people living with HIV [1, 2] , such that life expectancy for HIV-positive individuals with access to treatment now approaches that of the general population [3] . International [4] [5] [6] and UK [7] guidelines recommend that patients are involved in decision-making about HIV care and treatment. For this to be achieved, patients require adequate knowledge of their HIV-related health and treatment goals, and are dependent on health care professionals providing and discussing the relevant information.\n\n Reviewing laboratory markers such as CD4 counts and HIV viral load (VL) in conjunction with a patient is now common practice amongst HIV health care providers. Previous studies have suggested that providing health-related knowledge to people with HIV infection may improve adherence and treatment outcomes, as well as benefitting the patient\u00c0health care provider relationship [8] [9] [10] . Furthermore, as increasing evidence shows that a suppressed HIV VL greatly reduces the risk of onward transmission of HIV to sexual partners [11] [12] [13] [14] [15] [16] [17] , people with HIV infection may use their VL results to make decisions on condomless sex. Therefore, in this context, it is critical that individuals have correct self-knowledge of their latest VL result.\n\n Little is known about the accuracy of individuals' knowledge of their own HIV biomarker status, or whether socioeconomic and other factors impact on such knowledge. In particular, there have been no studies assessing the accuracy of self-report of VL status among people living with HIV in the UK. Despite access to free HIV care in the UK via the National Health Service, recent results from the Antiretrovirals, Sexual Transmission Risk and Attitudes (ASTRA) study indicated that socioeconomic disadvantage was associated with a substantially increased probability of virological nonsuppression and virological rebound among people on treatment for HIV infection in the UK [18] . In this study, we used data from ASTRA to investigate the ability of individuals on ART to correctly report their HIV VL level and CD4 count level, by comparing self-report with the clinic-recorded value. We also assessed the association of demographic, socioeconomic, HIV-related and health-related factors with inaccurate reporting of HIV VL level.\n\n \n\n ASTRA is a cross-sectional questionnaire study that recruited people with diagnosed HIV infection from eight HIV out-patient clinics in the UK between February 2011 and December 2012 [19] . The time period allocated for recruitment in each clinic was of sufficient duration that infrequent clinic attendees would be included in those invited to participate. Exclusion criteria were as follows: age < 18 years; unable to understand the study questionnaire (available in English or French) because of language or cognitive difficulties; too ill or distressed to complete the questionnaire. Participants self-completed a confidential paper questionnaire that included items on demographics (gender/sexual orientation; age; ethnicity), socioeconomic factors (UK birth/English fluency; education; employment; housing; financial hardship; supportive network assessed by a modified version of the Duke Functional Social Support questionnaire [18, 20] Participants who reported that they had ever started ART were asked to report the value of their VL from the last time they received their test results, with three options: \"50 copies/mL or less ('undetectable' or 'suppressed)\"; \"more than 50 copies/mL ('detectable' or 'raised')\"; or \"don't know\". All participants were asked to report their last CD4 count result, with five options; \"less than 200\"; \"200-350\"; \"351-500\"; \"more than 500\"; or \"don't know/can't remember\". For all participants, the study recruiter documented, from clinic records, the latest VL and CD4 count laboratory results that were available to the participant at the time the questionnaire was issued. Therefore, information on VL and CD4 levels was available both from the participant self-report and from the clinic record.\n\n Agreement between self-report and clinic record for VL and CD4 was assessed only among participants who reported currently taking ART. Accurate self-report was defined as agreement between participant self-report and clinic record that the latest VL was either \u2264 50 copies/mL or > 50 copies/mL. Inaccurate selfreport was defined as either disagreement between selfreport and clinic record on the level of VL, or a response of \"don't know\" to the question on VL level. Participants who had no clinic-recorded VL, whose clinic-recorded VL was dated after the questionnaire issue date, or who gave no response to the question on VL level were excluded from the analysis. Similarly, accuracy of reporting CD4 count was defined as agreement between participant self-report and clinic record that the latest CD4 count was > 350 or \u2264 350 cells/lL.", "qa": [["63_4909729_0_1", "How has the availability of antiretroviral therapy (ART) impacted the life expectancy of individuals living with HIV?\n", "Advances in antiretroviral therapy (ART) have greatly reduced mortality among people living with HIV, to the point where life expectancy for HIV-positive individuals with access to treatment now approaches that of the general population."], ["63_4909729_0_2", "Why is it important for patients with HIV to have adequate knowledge of their HIV-related health and treatment goals?\n", "In order for patients to be involved in decision-making about HIV care and treatment, they need to have adequate knowledge of their HIV-related health and treatment goals. This knowledge allows them to make informed decisions and actively participate in their own healthcare."], ["63_4909729_0_3", "What factors may impact individuals' ability to accurately report their HIV viral load (VL) levels?\n", "Factors such as socioeconomic disadvantage and other demographic, socioeconomic, HIV-related, and health-related factors may impact individuals' ability to accurately report their HIV viral load (VL) levels. This suggests that there may be disparities in knowledge and understanding of one's biomarker status, particularly among individuals who face socioeconomic challenges."]]}, {"passage_id": "34_25255049_1", "passage": "21 However, a paucity of evidence supports the belief that elderly people do not derive benefit from participating in a clinical trial. It remains a matter of controversy that physician bias could be based on toxicity difference or age alone, but both of these factors are likely to play a role as a barrier to clinical trial participation among the elderly.\n\n \n\n When patients eligible for a clinical trial consider whether to participate, many cite a lack of autonomy over treatment choice as being a reason for foregoing participation. 24 Many patients also want to choose their own treatment and fear that their participation in a clinical trial will mean they will lose that decision-making capacity. Increased survival was less important than improved quality of life for older patients when declining trial enrollment.\n\n 24 These preferences illustrate a cultural or philosophical difference that may exist between age groups, which may be a barrier, yet it is difficult to describe.\n\n Elderly patients also cited other reasons for declining to enroll in a clinical trial, including such concerns as adverse events, friends who oppose participation, or a belief that participation in a clinical trial would provide no value to future generations. 21 Family opposition to enrollment is a more important issue for older patients compared with their younger counterparts.\n\n The role of altruism in trial participation has also been explored, but conclusions about altruism are difficult to make. 25 Moreover, compared with their younger counterparts, elderly patients are less likely to actively seek participation and less informed of the availability of clinical trials, which may be related to the differences seen in literacy rates among the age groups. 24 Other cultural implications from both patients and physicians may also be present and are difficult to assess but do lead to decreased trial enrollment. 24 When patients were surveyed, the reason given for enrolling in a trial was \"I trusted the doctor treating me,\" although it was also given as a reason for those who chose not to enroll in a trial. 25 Patient perceptions of trial efficacy appear to play a role in patient enrollment. One study found that 44% of patients who declined enrollment did so when offered participation in a trial comparing standard treatment with a novel agent. 25 Patients surveyed reported a higher likelihood to enroll in trials that included standard treatment with or without the addition of a novel drug. As many as 20% of surveyed patients reported that they chose to enroll in a trial because they felt that the trial provided the best treatment options. 25 In addition, some patients felt that they must receive at a minimum standard treatment and said they could not afford to be assigned to a placebo group.\n\n Logistical barriers present another obstacle to enrollment in clinical trials among the elderly, making a difficult-to-recruit population even smaller. 26 Participation in a clinical trial may require patients to travel to cancer or academic centers, and elderly persons may have a smaller support network than their younger counterparts; however, in 1 study, this factor was not significantly different between older and younger patients. 21 Financial issues may also play a role in elderly patients choosing not to enroll in clinical trials, although no difference was seen among age groups in this study. \n\n Generally, clinical trials do not limit eligibility based on age alone, but other criteria, including performance status (PS), organ dysfunction, and disease status, may preclude older patients from participating in a study. Previous studies have demonstrated that survival correlates well with PS and comorbidities, and study exclusion criteria often are based on these data. 27, 28 Although these are logical exclusion criteria, they limit elderly enrollment in clinical trials because older patients generally have a higher number of comorbidities than their younger counterparts.\n\n In multiple studies, trial ineligibility was the greatest barrier to clinical trial enrollment among older persons, with both patients and physicians perceiving this barrier as a major obstacle, and up to 60% of elderly patients who did not enroll in a clinical trial stated they failed to do so because of trial unavailability or ineligibility. 21, 29 One study reported that 65% of patients 65 years of age or older were eligible to participate in the trial compared with 78% of younger patients. 21 However, if patients were eligible, trial participation rates did not significantly differ by age (34% for age \u2265 65 years vs 40% for age < 65 years). After considering other factors, overall survival and toxicity rates were similar among the younger and older patients. PS was the most significant determinant of overall 30-day (PS 0-1, 97.5%; PS 2-3, 79%) and 1-year (PS 0-1, 21%; PS 2-3, 9.5%) survival rates (P = .029).", "qa": [["34_25255049_1_1", "What factors contribute to elderly patients declining to participate in clinical trials?\n", "Elderly patients may decline participation in clinical trials due to a variety of factors. These include a desire for autonomy over treatment choice, concerns about adverse events, opposition from friends or family members, and a belief that participating in a trial would not provide value to future generations. Additionally, cultural or philosophical differences may exist between age groups, leading to a barrier in trial enrollment. Lack of awareness about the availability of clinical trials and lower literacy rates among the elderly compared to younger individuals may also contribute to decreased enrollment."], ["34_25255049_1_2", "How do patient perceptions of trial efficacy impact enrollment in clinical trials?\n", "Patient perceptions of trial efficacy play a role in their decision to enroll in clinical trials. One study found that patients were more likely to enroll in trials that included standard treatment, either alone or in combination with a novel drug. Patients may prioritize receiving the best treatment options, and some may feel that they cannot afford to be assigned to a placebo group. Trust in the treating physician was also cited as a reason for both enrolling and declining participation in a trial."], ["34_25255049_1_3", "What are the logistical barriers that impact elderly patients' enrollment in clinical trials?\n", "Logistical barriers present challenges for elderly patients considering clinical trial enrollment. These barriers include the need to travel to specialized centers for cancer or academic research, which may be difficult for elderly individuals who have a smaller support network than younger patients. Financial issues may also play a role, although there was no significant difference observed among age groups in one study. These logistical barriers further limit the already difficult-to-recruit elderly population from participating in clinical trials."]]}, {"passage_id": "7_7608904_1", "passage": "e left paraduodenal fossa results from failure of mesenteric fusion with the parietal peritoneum and malrotation of the midgut resulting in development of a potential space [3] . LPDH occurs when small intestine prolapses posteroinferiorly into this fossa (of Landzert), which is bounded by the fourth part of the duodenum, the posterior peritoneum, the inferior mesenteric vein, and left branches of the middle colic artery [7, 8] .\n\n is may result in small bowel incarceration, obstruction, and subsequent ischaemia.\n\n Patients most commonly present in the 4th to 5th decades or life [2] , and there is a 3 : 1 male preponderance [4] . Presentation is variable depending on the severity of the hernia sequelae and the presence of obstruction [2] . Approximately 50% of patients recall previous recurring abdominal pain of nonspeci c nature [9] . As such, the entity poses a diagnostic challenge, with majority of cases identi ed only at operation [10] . Patients with LPDH have a 50% lifetime risk of hernia incarceration with 20-50% mortality for acute presentations; hence, operative management is recommended regardless of symptoms [5] .\n\n e most e ective preoperative diagnostic tool is the computed tomography (CT) scan [11] .\n\n is may reveal a cluster of small bowels at the ligament of Treitz with or without associated ndings of small intestine obstruction [2] . An additional compression on the posterior stomach and distal duodenum results in inferior displacement of transverse colon and shifting of the mesenteric truck to the right [12] . However, instances of acute abdomen warrant omission of preoperative imaging and immediate exploratory laparotomy.\n\n Treatment methods reported in the literature include laparoscopy or laparotomy and repair. However, exploratory laparotomy is more often reported, especially in the setting of an acute complication such as strangulation, perforation, 2 Case Reports in Surgery or large distension from obstruction [13] . In addition, laparotomy may be more appropriate in circumstances where laparoscopy may not be possible or dangerous such as signi cant adhesions, haemodynamic instability, and contraindication to pneumoperitoneum [13] . Nonetheless, the laparoscopic approach to management of this condition has become increasingly prevalent in the literature [14] . Regardless of the approach, basic principles of hernia repair are adopted, namely, reduction of hernia contents and repair of hernia defect [9] . Excision of the hernia sac has been described but is not mandatory given the potential for injury to the colic vessels [14] . Correct identi cation and preservation of the vascular structures that constitute the hernia neck is essential [12] . Rarely, widening of the hernia neck is required to reduce the contents and may involve incision of constricting peritoneal fold inferiorly and even division of the inferior mesenteric vein in more di cult cases [15] . A literature search of PubMed back to 1980 discovered only approximately 50 reported cases of intestinal obstruction secondary to a LPDH. However, to our knowledge, this is the rst reported case of LPDH exacerbated by laparoscopic pneumoperitoneum and intraoperative patient positioning. is case is particularly interesting for a few reasons. Firstly, the patient demographic is not typical of this disease. Our patient was a young female, while reported cases more commonly describe males in the 4th to 5th decades of life. Secondly, the patient described no symptoms at any time prior to the day of her appendectomy. Although the patient clearly possessed the congenital anomaly of a left paraduodenal fossa, the application of pneumoperitoneum, placement in the Trendelenburg position, and manipulation of small intestine for the purpose of facilitating laparoscopic appendectomy appear to have precipitated the LPDH and subsequent incarceration. In this instance, the diagnosis was not made prior to laparotomy as in most cases presenting with an acute abdomen. Although diagnostic laparoscopy was performed upon return to theatre, gross small bowel distension from obstruction prevented further progression via the laparoscopic approach.\n\n Written informed consent was obtained from the patient for publication of this case report and accompanying images.\n\n e authors declare that there are no con icts of interest regarding the publication of this article.\n\n Mathew A. Kozman was the operating surgeon for the appendectomy and assistant surgeon for the laparotomy. He wrote the manuscript and conceived and designed the study. Mathew A. Kozman and Oliver M. Fisher were responsible for drafting and revising the article content and for the nal approval of the manuscript prior to submission.", "qa": [["7_7608904_1_1", "What are the risk factors and symptoms associated with left paraduodenal hernia (LPDH)?", "LPDH is more commonly observed in patients in their 4th to 5th decades of life, with a higher prevalence in males. The condition can present with various symptoms, including abdominal pain, obstruction, and ischemia. About 50% of patients may recall previous episodes of abdominal pain, and LPDH is often diagnosed only at the time of operation."], ["7_7608904_1_2", "What is the recommended management approach for left paraduodenal hernia (LPDH)?", "In cases of LPDH, operative management is recommended regardless of symptoms, as there is a significant risk of hernia incarceration with potential mortality for acute presentations. Treatment options include laparoscopy or laparotomy and repair. Exploratory laparotomy is more commonly reported, especially in the presence of acute complications like strangulation, perforation, or large distension from obstruction. However, the laparoscopic approach has also been increasingly used in managing LPDH."], ["7_7608904_1_3", "How is left paraduodenal hernia (LPDH) diagnosed and what imaging modalities are commonly used?", "The most effective preoperative diagnostic tool for LPDH is a computed tomography (CT) scan. CT scans can reveal a cluster of small bowels at the ligament of Treitz, with or without associated findings of small intestine obstruction. In addition, the compression on the posterior stomach and distal duodenum can result in inferior displacement of the transverse colon and shifting of the mesenteric trunk to the right. However, in cases of acute abdomen, immediate exploratory laparotomy may be necessary without preoperative imaging."]]}, {"passage_id": "6_41270485_1", "passage": "Its distribution was spreading as the number of habitats available for the intermediate hosts was extended by water development projects such as dams, irrigation schemes, river navigation improvements and impedance of drainage associated with the growing construction of roads.\n\n The World Health Organisation, formed after the Second World War in 1948, found schistosomiasis control was little changed from that prevailing at the end of the First World War. Moreover, it promised to get worse when ambitious plans for water resource developments were eventually executed to increase the agricultural and industrial productivity of developing countries. These developments, besides extending the number and distribution of potential snail habitats, resulted in mass population movements and changed agricultural practices, all favouring the spread of schistosomiasis. At the time, few anticipated the human population explosion in the second half of the century which would greatly increase the number of people at risk. Accelerating migration from the countryside to the town threatened to overwhelm existing, aging water and sanitation systems, and to increase urban schistosomiasis in the periurban slums.\n\n Compared with malaria, which WHO confidently (if, with hindsight, misguidedly) expected to eradicate quite easily because of the range of effective drugs and insecticides available, the schistosomiasis situation was very bleak. The main tools were toxic antimonial drugs and an ineffective molluscicide. WHO orchestrated a research programme by national, international and commercial organisations to remedy the lack of molluscicides and drugs.\n\n The first fruits of the WHO programme were molluscicides which became available in the mid 1950s and one of which, niclosamide, is still in use and effective today. For the next 25 years, community control of schistosomiasis was spearheaded by molluscicides in Africa, south west Asia and, to a lesser extent, the New World. Careful biological studies suggested an alternative approach to the control of Oriental schistosomiasis: Oncomelania populations are less resilient than Biomphalaria and Bulinus spp. to disturbance. Habitat modification was recommended as a potential control measure because it also improved rice production and this could, potentially, fund the elimination of snail habitats (Pesigan et al. 1958) .\n\n Initial hopes of eradicating aquatic snails with one or two molluscicide applications, especially from irrigation schemes, were soon dashed. Field studies on snail population dynamics were undertaken to improve the timing and efficacy of molluscicidal applications (Dazo et al. 1966 ) and were followed by field studies on transmission of the schistosomes. The seasonality of transmission was revealed in many, though not all, areas. Carefully timed mollusciciding was then able to minimise, if not stop, transmission in many places well into the 1980s (Sturrock et al. 1974) . However, the effect on the human worm burden was slow (Jordan 1985) . Interest in molluscicides began to wane when new drugs finally appeared and their decline was hastened by the sharp rise in the price of pesticides after the oil crisis in the 1970s, and by a growing (though largely unjustified) fear of adverse environmental effects. In some places they were completely abandoned.\n\n Development of drugs took much longer than that of molluscicides. By the early 1970s, metrifonate was established for use against S. haematobium, although the need for three fortnightly treatments caused logistic problems for community use. Fears for its safety prevented the widespread acceptance of hycanthone, the first single-dose drug effective against S. mansoni and, possibly, S. haematobium, but a similar drug, oxaminiquine, was released for community use against S. mansoni in the 1970s. The real turning point was the arrival in the early 1980s of praziquantel, a safe, effective, single-dose drug active for all schistosomes and many other human and veterinary helminths, besides (WHO 1985) . It promised to revolutionise community chemotherapy against schistosomiasis, although it was initially too expensive for widespread use. Its price has at last dropped radically and it is now the major weapon for community control of schistosomiasis.\n\n New drugs revived hopes of eradicating schistosomes directly by treating people. Single-dose treatment programmes certainly had dramatic results, but transmission persisted and reinfection invariably occurred. Experience showed that repeated treatments were essential to maintain initial gains. As long as praziquantel remained expensive, eradication seemed impossible: morbidity control became a more realistic objective. The most successful control programmes have been those that included some method of curbing transmission, including mollusciciding, even at a reduced level (Webbe & El Hak 1990) .", "qa": [["6_41270485_1_1", "How did water development projects contribute to the spread of schistosomiasis?\n", "Water development projects such as dams, irrigation schemes, river navigation improvements, and impedance of drainage extended the number of habitats available for the intermediate hosts of schistosomiasis. These projects resulted in mass population movements, changed agricultural practices, and increased the distribution of potential snail habitats, all of which favored the spread of schistosomiasis."], ["6_41270485_1_2", "What were the main tools used for schistosomiasis control and how effective were they compared to those for malaria?\n", "The main tools for schistosomiasis control were toxic antimonial drugs and molluscicides. However, these tools were not as effective as the range of drugs and insecticides available for malaria control. While effective molluscicides became available in the mid-1950s, the control of schistosomiasis relied heavily on molluscicides for community control in Africa, southwest Asia, and the New World. The decline in the use of molluscicides was hastened by the rise in pesticide prices and concerns about adverse environmental effects."], ["6_41270485_1_3", "How did the development of drugs impact the control of schistosomiasis?\n", "The development of drugs for schistosomiasis took longer than that of molluscicides. By the early 1970s, metrifonate and oxaminiquine were established for use against specific species of schistosomes. However, the real turning point came with the arrival of praziquantel in the early 1980s. Praziquantel was a safe, effective, single-dose drug active against all schistosome species. It revolutionized community chemotherapy against schistosomiasis, although its initial high cost limited widespread use. As the price of praziquantel dropped, it became the major weapon for community control of schistosomiasis."]]}, {"passage_id": "49_14507405_0", "passage": "The principal goal of connectomics is the comprehensive mapping and analysis of brain connectivity, across all scales, from the micro-scale of individual synaptic connections between neurons to the macro-scale of brain regions and interregional pathways [1] . The nascent field of macro-connectomics, at first glance, shares little but a name with its microscopic cousin. Instead of building maps of neural circuits that are detailed enough to include every axonal connection, macro-connectomics attempts to map brain connections at the largest scale. In doing so, it bridges two influential ideas in systems neuroscience [2] .\n\n Functional specialisation considers large regions of the brain's grey matter as individual units that become engaged in different functional contexts; and functional integration considers how such brain regions interact and influence one-another to produce coherent experiences and behaviour (e.g. [3, 4] ). It is this systems-level understanding of neural processing that has most to benefit from macro-connectomics, which aims to provide systematic approaches both for identifying functional sub-units, and for mapping the connections between them.\n\n Invasive techniques for localising brain regions and tracing anatomical connections have existed for many decades. Tracers are injected into a candidate brain region, taken up inside cells and transported along the axon. Post-mortem histological staining then reveals the distribution of the labelled axons and their connections with distant cells. Tracer techniques are exquisitely precise and accurate. Using different tracers, experimenters may specifically map connections travelling in different pathways or emerging from different cell types or layers. Using viral tracers, monosynaptic or multi-synaptic connections may be selectively labelled. Recent advances have been directed at detailed and accurate quantification of the density of regional brain connections [5, 6] .\n\n By comparison, presently available techniques for measuring brain connections noninvasively are based on a process of inference -their estimation is indirect; they can be difficult to interpret quantitatively; and they continue to be error-prone. However, their noninvasive nature and ease of measurement permit us to address scientific questions that cannot be answered by any other means. In particular, brain connections can be measured in living human subjects, and measurements can be made simultaneously across the entire brain, thus permitting the creation of a comprehensive whole-brain connection map, the connectome. Hence, areal connections may be compared in humans across individuals and across many cortical and sub-cortical sites, allowing detailed studies of connectional organisation and individual differences. Furthermore, these techniques enable direct investigation of the common rationale that underlies the study of brain circuitry at any scale -the assumed importance of connectional architecture for functional processing and thence behaviour. Using in vivo techniques, this dependence may be tested directly, by comparing structural connectivity to measurements of regional activations and interregional correlations (functional connectivity). Furthermore, variations in anatomical or functional connectivity across the population may be related to variations in behavioural abilities [7] .\n\n In this review, we survey the current state-of-the-art in human connectomics, including a comparison of techniques for mapping brain connectivity, the use of connectivity data to discern functionally specialized regions, the relation of structural to functional connections, and the use of network analysis measures to quantitatively characterize the architecture of the human connectome.\n\n There are two common approaches for mapping inter-regional connections in-vivo. They both use Magnetic Resonance Imaging (MRI), but rely on very different principles. Diffusion tractography aims to infer the tracks of axon bundles millimetre-by-millimetre as they traverse the brain's white matter. By contrast, resting state functional MRI measures spontaneous fluctuations in the blood-oxygenation-level-dependent (BOLD) signal in grey matter regions and estimates statistical dependencies between these BOLD time series, usually expressed as cross-correlations.\n\n Central to all diffusion tractography studies is the anisotropic diffusion of water in and around axons. Whilst freely diffusing molecules will diffuse equally in all directions, the presence of semi-permeable boundaries in tissue may hinder diffusion along some orientations but not others. In brain white matter, axonal membranes and myelin sheaths hinder diffusion perpendicular to the axon [8] , leaving diffusion fastest along the axon. This orientational dependence of diffusion is termed anisotropy. Using diffusion-weighted MRI, we can measure this anisotropy and the peak diffusion orientations in each imaging voxel [9] [10] [11] [12] (Fig. 1) . However, such voxels cover cubes of tissue around 1.5\u00d71.5\u00d71.5mm 3 , each potentially containing tens of thousands of axons. The diffusion orientations we measure therefore relate to the orientations of bundles of coherently oriented axons passing through the voxel.", "qa": [["49_14507405_0_1", "What is the difference between connectomics at the micro-scale and macro-scale, and how does macro-connectomics contribute to our understanding of neural processing?\n", "Connectomics at the micro-scale focuses on mapping individual synaptic connections between neurons, while macro-connectomics aims to map brain connections at the largest scale. Macro-connectomics bridges the ideas of functional specialization and functional integration in systems neuroscience. Functional specialization considers large brain regions as individual units engaged in different functions, while functional integration explores how these regions interact and influence each other to produce coherent experiences and behavior. Macro-connectomics provides systematic approaches for identifying functional sub-units and mapping the connections between them, contributing to a systems-level understanding of neural processing."], ["49_14507405_0_2", "What are the advantages and limitations of invasive techniques and noninvasive techniques for measuring brain connections?\n", "Invasive techniques involve injecting tracers into a brain region, which are taken up by cells and transported along axons. Post-mortem histological staining reveals the distribution of labeled axons and their connections. Invasive techniques are precise and accurate, allowing for detailed mapping of connections. On the other hand, noninvasive techniques for measuring brain connections, such as Magnetic Resonance Imaging (MRI), are based on inference and estimation. While noninvasive techniques are difficult to interpret quantitatively and can be error-prone, they offer the advantage of measuring brain connections in living human subjects across the entire brain. This allows for the creation of comprehensive whole-brain connection maps (connectomes) and the investigation of the relationship between structural and functional connectivity."], ["49_14507405_0_3", "What are the two common approaches for mapping inter-regional connections in-vivo using Magnetic Resonance Imaging (MRI), and how do they differ in their principles?\n", "The two common approaches for mapping inter-regional connections in-vivo using MRI are diffusion tractography and resting state functional MRI. Diffusion tractography aims to infer the tracks of axon bundles in the brain's white matter by measuring the anisotropic diffusion of water. Axonal membranes and myelin sheaths hinder diffusion perpendicular to the axon, resulting in anisotropic diffusion. Diffusion-weighted MRI measures this anisotropy and the peak diffusion orientations in each imaging voxel. Resting state functional MRI, on the other hand, measures spontaneous fluctuations in the blood-oxygenation-level-dependent (BOLD) signal in grey matter regions and estimates statistical dependencies between these BOLD time series, usually expressed as cross-correlations. These two approaches differ in their focus on white matter (diffusion tractography) and grey matter (resting state functional MRI) and the principles they rely on for mapping brain connections."]]}, {"passage_id": "30_52914150_1", "passage": "Before instrumentation, all specimens were prescanned at an isotropic resolution of 20 lm using a microCT scanner (lCT 50; Scanco Medical AG, Bru\u00a8ttisellen, Switzerland) at 45 kV and 88 lA. Reconstruction of the slice images was performed automatically using lCT software version 6.1 (Scanco Medical AG) and then manually exported in a Digital Imaging and Communications in Medicine (DICOM) format and saved on a hard disk.\n\n Irrigation was standardized for all three groups. With each file use, the canal was irrigated with 2 ml of distilled water using a 27-gauge open-ended flat-tipped safety needle (Suyun Medical Materials, Jiangsu, China), recapitulated with a no. 10 K-file (Dentsply Sirona) and irrigated again before the next instrument. The needle was inserted as deep as possible without binding. The determination of working length (WL) was accomplished by inserting a no. 10 K-file until just visible at the apical foramen using 2\u00c2 magnification loupes. This step verified the patency and created a glide path. A 6:1 contra-angle handpiece (VDW) driven by a VDW Silver (VDW) endodontic motor was used in the rotary mode only. Each group was prepared according to the manufacturers' guidelines by one operator (Z.H.) and each file was used only in one resin block.\n\n The procedures used in the three groups were as follows. In group PTN, for each specimen, the WL of the root canal was determined by a no. 10 K-file. PTN X1 (17/04) and X2 (25/06) files were used in the WL at a speed of 300 rotations per minute (rpm) and a torque of 2 Ncm. In group HFCM, after determining the WL with a no. 10 K-file, the 25/08 file was used to prepare the coronal third of the canal at a speed of 500 rpm and a torque of 2.5 Ncm, followed by the use of 20/06 and 25/06 files in the WL. In group HFEDM, after the WL was determined by a no. 10 K-file, the 25/08 file of the HFCM system was used to prepare the canal orifice at a speed of 400 rpm and a torque of 2.5 Ncm. Then, 10/05 and 25/08 files were used in the WL.\n\n After preparation, all specimens were decontaminated in an ultrasonic cleaner with distilled water for 15 min and dried at 40\n\n C for 24 h in a desiccator as described above. The specimens were rescanned using the same microCT scanner and post-instrumentation DICOM images were acquired.\n\n The DICOM format images of each specimen were converted with the Materialise Interactive Medical Image Control System Mimics software (Medical version 17.0; Materialise, Leuven, Belgium). The software calculates accurate three-dimensional (3D) models from stacked information in DICOM format images using a special algorithm. The 'mask' (Figure 1 ), which was a software-generated image from the DICOM images of each canal, was equally divided digitally into three parts, namely the apical, middle and coronal sections.\n\n The volume (mm 3 ) of each mask obtained from each reconstructed 3D object was regarded as the volume of the corresponding canal. Volume increases of the canal were calculated by subtracting the volumes of the non-instrumented canals from the instrumented canals.\n\n For measurement of diameter increases, canal transportations and centring ratios, each canal was resliced along the central axis of the canal to obtain pre-and postinstrumentation images (Figure 2) . 15 The parameters at 11 levels from the apex at 1-mm intervals were calculated using the following formulae: 16 Diameter increase \u00bc \u00f0X2-X1\u00de \u00fe \u00f0Y2-Y1\u00de Canal transportation \u00bc \u00f0X2-X1\u00de-\u00f0Y2-Y1\u00de\n\n Centring ratio \u00bc \u00f0X2-X1\u00de=\u00f0Y2-Y1\u00de if \u00f0Y2-Y1\u00de > \u00f0X2-X1\u00de or centring ratio \u00bc \u00f0Y2-Y1\u00de=\u00f0X2-X1\u00de if \u00f0X2-X1\u00de > \u00f0Y2-Y1\u00de\n\n Canal transportation toward the inner side of the curvature of the canal was denoted a positive number and a negative number was used if it was toward the outer side.", "qa": [["30_52914150_1_1", "What is the purpose of using a microCT scanner in the described procedure?\n", "The microCT scanner is used to prescan the specimens at an isotropic resolution of 20 lm. This allows for the reconstruction of slice images and the creation of 3D models of the canals, which are used to measure volume increases, diameter increases, canal transportations, and centring ratios."], ["30_52914150_1_2", "How is the working length (WL) determined in the described procedure?\n", "The working length (WL) is determined by inserting a no. 10 K-file until it is just visible at the apical foramen using 2\u00c2 magnification loupes. This step verifies the patency and creates a glide path for the instruments used in the procedure."], ["30_52914150_1_3", "What are the different groups and instruments used in the described procedure?\n", "The procedure involves three groups: PTN, HFCM, and HFEDM. In the PTN group, PTN X1 and X2 files are used at a specific speed and torque. In the HFCM group, the 25/08 file is used to prepare the coronal third of the canal, followed by the use of 20/06 and 25/06 files. In the HFEDM group, the 25/08 file is used to prepare the canal orifice, followed by the use of 10/05 and 25/08 files."]]}, {"passage_id": "61_6995493_2", "passage": "Figure 1 presents a diagram of how the magnitude and phase images are processed (please note that, due to averaging, the time resolution of a LV is 4 X TR while U is resolved with TR). Isovolumetric phases of tension, \u03c4 A , and elasticity relaxation, \u03c4 B , were deduced by means of superposed graphs of a LV (t) and U(t). The time shift \u03c4 A was determined by manual selection of the delay between the decaying branches of a LV (t) and U(t). Correspondingly, \u03c4 B was determined from the delay between the ascending branches of both physiological functions. More specifically, the time points t 1 and t 2 were selected at 50% of the maximum amplitude of the curve U(t), i.e. \n\n In all experiments, wave amplitudes decreased due to myocardial contraction. The change in wave amplitude occurred before that in the cross-sectional area of the left ventricle.\n\n Volunteers Figure 3 demonstrates superposed graphs of U(t) and a LV (t) of a healthy volunteer and illustrates the determined \u03c4 A -and \u03c4 B -intervals. In 35 volunteers, the time of isovolumetric tension was \u03c4 A = 136 \u00b1 36 ms. This time was found to be significantly longer than the time of isovolumetric elasticity relaxation, \u03c4 B = 75 \u00b1 31 ms (P < 0.01). Similar results were obtained when accounting for normalized isovolumetric times given by \u03c4 A and \u03c4 B divided by the square root of the RR-interval [2] . Also, these normalized time intervals, \u03c4 A ' = 138 \u00b1 37 ms and \u03c4 B ' = 76 \u00b1 30 ms, were significantly different (P < 0.01). The shortening fraction in volunteers was 56.0% \u00b1 5.6%. Neither \u03c4 A nor \u03c4 B showed any significant correlation with the shortening fraction, as demonstrated by low correlation coefficients, R = 0.34 and 0.02, respectively. The mean contractility ratio \u03c4 A /\u03c4 sys [17] of volunteers was 0.25 \u00b1 0.06. Figure 4 shows U(t) and a LV (t) of a patient with mild diastolic dysfunction. The mean isovolumetric tension time in patients (\u03c4 A = 161 \u00b1 45 ms) was similar to \u03c4 A measured in volunteers (P = 0.039). In contrast, isovolumetric elasticity relaxation, \u03c4 B = 133 \u00b1 57 ms, was significantly prolonged in patients (P < 0.001). Normalized time intervals were \u03c4 A ' = 142 \u00b1 68 ms and \u03c4 B ' = 173 \u00b1 50 ms. The shortening fraction in the patient group was 44.9 \u00b1 12.6%. Similar to volunteers, time intervals \u03c4 A and \u03c4 B showed no correlation with the shortening fraction (R = 0.07 and R = 0.29, respectively). The mean contractility ratio of patients (\u03c4 A /\u03c4 sys = 0.36 \u00b1 0.13) was higher than in volunteers. All data are summarized in Table 1 . Figure 5 illustrates \u03c4 A and \u03c4 B as functions of the two groups studied.\n\n \n\n Cardiac time intervals give insight into the hemodynamics and mechanical function of the heart and are thus valuable indicators of cardiac health. This study for the first time analyzed cardiac intervals based on changes in elastic wave amplitudes in volunteers and in patients with mild diastolic dysfunction. The evaluation of the complex signal in MRE enabled us to derive the timing of elasticity variation in relation to that of changes in LV geometry from a single MRE examination. This paves the way for the development of software capable of automatically combining magnitudebased and phase-based information from cardiac MRE. On-site implementation would provide a new modality of cardiac MRI for the instantaneous assessment of isovolumetric times based on changes in myocardial elasticity. In this study, externally induced elastic waves were used for probing the inherent cohesion of the mechanical matrix of myocardium, which determines its macroscopic shear elasticity. Changes in shear elasticity of biological tissue are known to be sensitive to disease progression [18] [19] [20] . In particular, cardiac function is synonymous with variation in myocardial elasticity (i.e. shear modulus). A variety of heart diseases are associated with dysfunctional elasticity variations of the myocardium. An important cardiac disease pattern is diastolic dysfunction, which is characterized by an elevated elastic modulus of the ventricular wall muscle at relaxation [21] [22] [23] .", "qa": [["61_6995493_2_1", "How can cardiac time intervals be used as indicators of cardiac health?\n", "Cardiac time intervals provide insight into the hemodynamics and mechanical function of the heart, making them valuable indicators of cardiac health. Changes in these intervals can reveal abnormalities or dysfunction in the heart, such as elevated elastic modulus of the ventricular wall muscle during relaxation, which is characteristic of diastolic dysfunction."], ["61_6995493_2_2", "What is the significance of changes in shear elasticity of biological tissue?\n", "Changes in shear elasticity of biological tissue, such as the myocardium, are known to be sensitive to disease progression. In the case of the heart, variations in myocardial elasticity (shear modulus) are synonymous with cardiac function. Dysfunctional elasticity variations of the myocardium are associated with a variety of heart diseases, including diastolic dysfunction."], ["61_6995493_2_3", "How can magnitude-based and phase-based information from cardiac MRE be combined to assess isovolumetric times?\n", "The evaluation of the complex signal in Magnetic Resonance Elastography (MRE) enables the derivation of the timing of elasticity variation in relation to changes in left ventricle (LV) geometry. By combining magnitude-based and phase-based information from cardiac MRE, software can be developed to automatically assess isovolumetric times based on changes in myocardial elasticity. This would provide a new modality of cardiac MRI for the instantaneous assessment of cardiac health."]]}, {"passage_id": "58_5325041_0", "passage": "In March 1998 the Office of Bioethics Education and Research at Dalhousie University hosted a multidisciplinary workshop, \"Assent and Dissent in Research Involving Children.\" 1 The overall objectives for the meeting were:\n\n first, to promote and protect the health and well-being of children 2 by ensuring that potentially beneficial biomedical and health research involving children was not inappropriately precluded; and second, that when such research did proceed, it was appropriately respectful of children. For the purposes of the discussion, appropriate respect included both protection from harm and promotion of the capacity for independent decision-making. Particular attention was focused on the roles and responsibilities of children and their parent(s) or legal guardian(s) in decisionmaking about research participation.\n\n At the outset, it was hoped that workshop participants would generate a consensus statement clearly identifying substantive points of agreement and disagreement. In retrospect, that goal was overly ambitious. A more modest goal informs this discussion document-namely, to provide a public record of important contributions made to the debate about research involving children in an attempt to advance the discourse on this important issue.\n\n Research guidelines the world over recognize the importance of respect for persons. 3 This principle requires that research involving humans not proceed without appropriate authorization. For persons with decisionmaking capacity, 4 this authorization is their informed consent to research participation. For persons without decisional capacity (and this typically includes most children), this authorization is the permission to proceed granted by a legally recognized surrogate decisionmaker. For children, the legal surrogate decisionmaker is most often their parents.\n\n In recent years it has been thought, in some jurisdictions, that authorization by a surrogate decisionmaker is insufficient in some instances, and that persons with developing decisional capacity must also be involved in the decisionmaking process. In the United States, as early as 1977 the National Commission for the Protection of Human Subjects of Biomedical and Behavioural Research held that:\n\n Parental permission normally will be required for the participation of children in research. In addition, assent of the children should be required when they are seven years of age or older. The Commission uses the term \"assent\" rather than \"consent\" in this context, to distinguish a child's agreement from a legally valid choice. A concept has developed that a child incapable of giving legally and ethically acceptable consent may give an \"assent\" which is significant in respecting a level of autonomy. Related to this concept is the recognition that a child, whose consent or assent to participate in research is questionable, may nevertheless have the power to decline invasive involvement with conclusive effect. Parental consent may be a necessary condition of engaging the child in research, but it is not necessarily sufficient; the child's negative preferences in such cases should be respected. 6 In the fall of 1998 these research guidelines were replaced with the Where free and informed consent has been obtained from an authorized third party, and in those circumstances where the legally incompetent individual understands the nature and consequences of the research, the researcher shall seek to ascertain the wishes of the individual concerning participation. The potential subject's dissent will preclude his or her participation. 7 Persons capable of dissent include \"those whose competence is in the process of development, such as children whose capacity for judgement and self-direction is maturing.\" 8 As suggested by the excerpts above, it is widely accepted that in principle children's expressed wishes regarding research participation should be respected. 9 In practice, however, the involvement of children in decisions about research participation is neither routine nor standardized. One reason for this difference between principle and practice is a general reluctance on the part of the research community and parents to recognize mature children and adolescents as persons with decisionmaking capacity who are fully capable of providing an independent consent or refusal to research participation. The Declaration of Helsinki illustrates the problem. It is stipulated therein that,\n\n In case of legal incompetence, informed consent should be obtained from the legal guardian in accordance with national legislation.... Whenever the minor child is in fact able to give consent, the minor's consent must be obtained in addition to the consent of the minor's legal guardian. 10 Note, the requirement here is that the child's consent be obtained in addition to, not in lieu of, the consent of the legal guardian.\n\n A second reason for the difference is concern about liability. The problem here is the lack of clarity in the law about decisionmaking authority for mature minors with respect to research, as well as the lack of clarity in its interpretation, particularly by investigators and research review committees ( To allow a child's dissent to function as the moral equivalent of a refusal by a person with decisionmaking capacity, without first having determined that the child has decisional capacity, is to seriously undermine parental responsibility for promoting children's interests. Parents who are not neglectful, exploitive, or abusive of their children are generally deemed responsible for making decisions on behalf of their children as they mature.", "qa": [["58_5325041_0_1", "How do research guidelines address the involvement of children in decision-making about research participation?\n", "Research guidelines recognize the importance of respect for persons and require appropriate authorization for research involving humans. For individuals with decision-making capacity, this authorization is informed consent. However, for individuals without decisional capacity, such as most children, authorization is granted by a legally recognized surrogate decision-maker, usually their parents. In some jurisdictions, it has been suggested that involving persons with developing decisional capacity in the decision-making process is necessary. The involvement of children in decisions about research participation is not standardized, and there is a reluctance to recognize mature children and adolescents as individuals capable of providing independent consent or refusal to research participation."], ["58_5325041_0_2", "What are the objectives of the workshop on \"Assent and Dissent in Research Involving Children\"?\n", "The objectives of the workshop were twofold. First, to promote and protect the health and well-being of children by ensuring that potentially beneficial biomedical and health research involving children was not inappropriately precluded. Second, to ensure that when such research did proceed, it was appropriately respectful of children. Appropriate respect included protection from harm and promotion of the capacity for independent decision-making. The workshop focused on the roles and responsibilities of children and their parents or legal guardians in decision-making about research participation."], ["58_5325041_0_3", "Why is there a difference between the principle and practice of involving children in decisions about research participation?\n", "While it is widely accepted in principle that children's expressed wishes regarding research participation should be respected, there is a difference between principle and practice. One reason for this difference is a general reluctance on the part of the research community and parents to recognize mature children and adolescents as individuals with decision-making capacity who can provide independent consent or refusal to research participation. Another reason is concern about liability, as there is a lack of clarity in the law and its interpretation regarding decision-making authority for mature minors with respect to research. This lack of clarity undermines parental responsibility for promoting children's interests."]]}, {"passage_id": "63_199436389_0", "passage": "The 2016 publication of the Centers for Disease Control and Prevention (CDC) guidelines for primary care physicians prescribing opioid therapy to treat noncancer pain in chronic pain patients advised prescribers to limit the daily dose of opioids in chronic therapy to under 90 morphine mg equivalents (MME) per day and to discontinue opioid therapy in patients who are no longer effectively treated with this regimen [1] . Recent changes to these guidelines have caused further confusion among clinicians as the CDC ''walked back'' some of their initial recommendations [2] . Unfortunately, the CDC has not offered guidance or insight into the often-challenging series of steps on how to reduce or even discontinue long-term opioid therapy. The discontinuation or reduction of opioid therapy after long-term exposure can be a long, slow, and difficult process for both patient and clinician. The abrupt discontinuation or even a sudden substantial dose reduction in opioid therapy for a patient on long-term (or even mid-term) opioid treatment can precipitate distressing and severe withdrawal symptoms and often causes an intense, seemingly irresistible craving for the drug that can drive patients to seek illicit or at least inappropriate sources for more opioids. Some chronic pain patients on a long-term opioid regimen have found themselves ''abandoned'' by physicians who refuse to prescribe opioid analgesics or who abruptly reduce their doses, leaving them with little explanation, no game plan, and inadequate analgesia. The consequences of inadequate pain control are far from trivial and may be difficult for those without chronic pain to fully comprehend. Patients with uncontrolled pain may experience limited function, social isolation, an inability to work or go about their activities of everyday living, disrupted sleep, diminished libido, and reduced quality of life.\n\n Pain control is more than just a regulation of nociception resulting in a decrease in pain intensity. A patient in chronic pain who experiences analgesia also experiences mitigation of a number of experiential symptoms-the profound emotional relief from depression, the alleviation of fear and anxiety, soothing of sensations of suffering, the restoration of emotional equilibrium, the confidence of regained function, and the like. This relief provides the patient with a significant neurological ''reward.'' It should be noted that the neural pathways associated with reward are distinct from pain pathways [3] [4] [5] and opioids can affect both. This importance of separate pain and reward pathways may explain why many individuals derive pain relief from medical marijuana although it offers no appreciable direct effect on pain pathways-but it does trigger reward pathways and in that way may provide relief from the emotional component of pain. Opioid analgesia not only provides pain relief, it directly affects the reward pathways and relieves the negative aspects associated with the pain experience [6] . These different neural pathways for reward versus analgesia may explain why some individuals are at higher risk for opioid use disorder than others. It goes beyond the scope of this article to more fully address the complex biopsychosocial phenomenon of opioid use disorder as a clinical condition unto itself.\n\n There is likewise a strong link between untreated chronic pain and depression that involves the neural networks of the brain [7] as much as the sense of despair from living a life reduced by physical suffering [8] . So vital is analgesia for the well-being of many chronic pain patients that any disruption to their pain control medications can provoke anxiety and fearfulness that they may hide from healthcare professionals to avoid being labeled as ''drug seekers.'' Unrelieved chronic pain has been associated with suicidal ideation [9] , and people in chronic pain are 40% more likely to have suicidal ideation and/or attempt suicide than those without chronic pain [10] . Patients taking opioids may report anxiety, depression, sleep disorders, and other conditions, but these are likely to be chronic pathologies of different etiology and should be diagnosed and treated separately.\n\n For patients on chronic opioid therapy who are appropriate candidates for a decrease in or discontinuation of therapy, it is important for clinicians to approach this situation both carefully and systematically. Patient education, professional compassion, and clinical patience are vitally important for safe, effective decreases in opioid therapy. It is our goal in this commentary to describe a methodology for decreasing or discontinuing opioid therapy. Abruptly stopping the drug and failing to communicate with the patient about the process are not options. We propose a series of steps that are necessary in effective opioid discontinuation: shared decision-making, tapering, clinical monitoring, managing withdrawal symptoms (if needed), pain control, and addiction therapy (if appropriate).\n\n This article is based on previously conducted studies and does not contain any studies with human participants or animals performed by any of the authors.\n\n Shared decision-making is increasingly upheld to be an important aspect of modern patientcentric healthcare but there is a dearth of training options for clinicians who seek to effectively involve their patients in making informed choices about their own care [11] .", "qa": [["63_199436389_0_1", "What are the potential consequences of inadequate pain control in chronic pain patients?\n", "Inadequate pain control in chronic pain patients can lead to limited function, social isolation, disruption of daily activities, disrupted sleep, diminished libido, and reduced quality of life. It can also contribute to emotional symptoms such as depression, fear, anxiety, and a sense of suffering. Patients may experience a decrease in their ability to work and engage in everyday activities, leading to a significant negative impact on their overall well-being."], ["63_199436389_0_2", "How does opioid analgesia provide relief for chronic pain patients beyond just decreasing pain intensity?\n", "Opioid analgesia not only decreases pain intensity but also affects the reward pathways in the brain, providing relief from the emotional aspects of pain. Chronic pain patients who experience analgesia from opioids also experience emotional relief from depression, alleviation of fear and anxiety, restoration of emotional equilibrium, regained function, and other experiential symptoms. Opioids can affect both pain pathways and reward pathways in the brain, providing a significant neurological reward for patients."], ["63_199436389_0_3", "What are the potential risks and challenges associated with decreasing or discontinuing long-term opioid therapy in chronic pain patients?\n", "Decreasing or discontinuing long-term opioid therapy can be a long, slow, and difficult process for both patients and clinicians. Abrupt discontinuation or sudden substantial dose reduction can lead to distressing and severe withdrawal symptoms, as well as intense cravings for the drug. Patients may seek illicit or inappropriate sources for more opioids. Additionally, patients may feel abandoned by physicians who refuse to prescribe opioid analgesics or abruptly reduce their doses, leaving them with inadequate pain control and little explanation or game plan. It is important for clinicians to approach the situation carefully and systematically, with patient education, compassion, and clinical patience."]]}]